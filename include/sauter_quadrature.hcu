/**
 * @file sauter_quadrature.h
 * @brief Introduction of sauter_quadrature.h
 *
 * @date 2022-03-02
 * @author Jihuan Tian
 */
#ifndef INCLUDE_SAUTER_QUADRATURE_HCU_
#define INCLUDE_SAUTER_QUADRATURE_HCU_

#include <deal.II/base/logstream.h>
#include <deal.II/base/utilities.h>

#include <deal.II/dofs/dof_accessor.h>
#include <deal.II/dofs/dof_handler.h>

#include <deal.II/fe/fe.h>
#include <deal.II/fe/fe_data.h>

#include <deal.II/grid/tria_accessor.h>

#include <deal.II/lac/full_matrix.h>

#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

#include <algorithm>
#include <atomic>
#include <iostream>
#include <iterator>
#include <mutex>
#include <vector>

#include "bem_kernels.hcu"
#include "bem_values.h"
#include "config.h"
#include "cu_bem_values.hcu"
#include "cu_profile.hcu"
#include "cu_qgauss.hcu"
#include "debug_tools.hcu"
#include "hmatrix.h"
#include "mapping_q_generic_ext.h"
#include "sauter_quadrature_tools.h"

namespace IdeoBEM
{
  using namespace dealii;

  template <int dim, int spacedim>
  class DofToCellTopology
  {
  public:
    /**
     * The core structure describing the DoF-to-cell topology.
     */
    std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
      topology;

    /**
     * Maximum number of cells associated with a DoF.
     */
    unsigned int max_cells_per_dof;
  };

  /**
   * Build the topology for "DoF support point-to-cell" relation.
   *
   * \mynote{2022-06-06 This topology is needed when the continuous finite
   * element such as @p FE_Q is adopted. For the discontinuous finite element
   * such as @p FE_DGQ, the DoFs in a cell are separated from those in other
   * cells. Hence, such point-to-cell topology is not necessary.}
   *
   * @param dof_to_cell_topo
   * @param dof_handler
   * @param fe_index
   */
  template <int dim, int spacedim>
  void
  build_dof_to_cell_topology(
    std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
      &dof_to_cell_topo,
    const std::vector<typename DoFHandler<dim, spacedim>::cell_iterator>
                                    &cell_iterators_in_dof_handler,
    const DoFHandler<dim, spacedim> &dof_handler,
    const unsigned int               fe_index = 0)
  {
    const types::global_dof_index        n_dofs = dof_handler.n_dofs();
    const FiniteElement<dim, spacedim>  &fe     = dof_handler.get_fe(fe_index);
    const unsigned int                   dofs_per_cell = fe.dofs_per_cell;
    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);

    dof_to_cell_topo.resize(n_dofs);

    for (const auto &cell : cell_iterators_in_dof_handler)
      {
        cell->get_dof_indices(local_dof_indices);
        for (auto dof_index : local_dof_indices)
          {
            dof_to_cell_topo[dof_index].push_back(&cell);
          }
      }
  }


  template <int dim, int spacedim>
  void
  build_dof_to_cell_topology(
    DofToCellTopology<dim, spacedim> &dof_to_cell_topo,
    const std::vector<typename DoFHandler<dim, spacedim>::cell_iterator>
                                    &cell_iterators_in_dof_handler,
    const DoFHandler<dim, spacedim> &dof_handler,
    const unsigned int               fe_index = 0)
  {
    const types::global_dof_index        n_dofs = dof_handler.n_dofs();
    const FiniteElement<dim, spacedim>  &fe     = dof_handler.get_fe(fe_index);
    const unsigned int                   dofs_per_cell = fe.dofs_per_cell;
    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);

    dof_to_cell_topo.topology.resize(n_dofs);
    dof_to_cell_topo.max_cells_per_dof = 0;

    for (const auto &cell : cell_iterators_in_dof_handler)
      {
        cell->get_dof_indices(local_dof_indices);
        for (auto dof_index : local_dof_indices)
          {
            dof_to_cell_topo.topology[dof_index].push_back(&cell);
          }
      }

    for (const auto &dof_to_cells : dof_to_cell_topo.topology)
      {
        if (dof_to_cells.size() > dof_to_cell_topo.max_cells_per_dof)
          {
            dof_to_cell_topo.max_cells_per_dof = dof_to_cells.size();
          }
      }
  }


  /**
   * Build the topology for "DoF support point-to-cell" relation.
   *
   * This version handles the case when a subset of the complete DoFs contained
   * in a DoF handler is selected. This case is met in the DoF handlers for
   * Dirichlet space in the mixed boundary value problem. Therefore, a map from
   * local to full DoF indices is passed.
   *
   * \mynote{2022-06-06 This topology is needed when the continuous finite
   * element such as @p FE_Q is adopted. For the discontinuous finite element
   * such as @p FE_DGQ, the DoFs in a cell are separated from those in other
   * cells. Hence, such point-to-cell topology is not necessary.}
   *
   * @param dof_to_cell_topo
   * @param dof_handler
   * @param map_from_local_to_full_dof_indices
   * @param fe_index
   */
  //  template <int dim, int spacedim>
  //  void
  //  build_dof_to_cell_topology(
  //    std::vector<std::vector<unsigned int>> &dof_to_cell_topo,
  //    const DoFHandler<dim, spacedim> &       dof_handler,
  //    const std::vector<types::global_dof_index>
  //      &                map_from_local_to_full_dof_indices,
  //    const unsigned int fe_index = 0)
  //  {
  //    const types::global_dof_index n_dofs =
  //      map_from_local_to_full_dof_indices.size();
  //    const FiniteElement<dim, spacedim> & fe = dof_handler.get_fe(fe_index);
  //    const unsigned int                   dofs_per_cell = fe.dofs_per_cell;
  //    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);
  //
  //    dof_to_cell_topo.resize(n_dofs);
  //
  //    /**
  //     * Iterate over each active cell in the triangulation and extract the
  //     DoF
  //     * indices.
  //     */
  //    dof_handler.
  //    for (const typename DoFHandler<dim, spacedim>::active_cell_iterator
  //    &cell :
  //         dof_handler.active_cell_iterators())
  //      {
  //        cell->get_dof_indices(local_dof_indices);
  //        for (auto dof_index : local_dof_indices)
  //          {
  //            dof_to_cell_topo[dof_index].push_back(cell->active_cell_index());
  //          }
  //      }
  //  }


  /**
   * Get the DoF indices associated with the cell vertices from a list of DoF
   * indices which have been arranged in either the forward or backward
   * lexicographic order. In this overloaded version, the results are returned
   * in an array as the function's return value.
   *
   * \mynote{There are <code>GeometryInfo<dim>::vertices_per_cell</code>
   * vertices in the returned array, among which the last two vertex DoF indices
   * have been swapped in this function so that the whole list of vertex DoF
   * indices in the returned array are arranged in either the clockwise or
   * counter clockwise order instead of the original lexicographic(zigzag)
   * order.}
   *
   * @param fe
   * @param dof_indices List of DoF indices in either the forward or backward
   * lexicographic order.
   * @return List of DoF indices for the cell vertices or corners with the last
   * two swapped.
   */
  template <int dim, int spacedim>
  std::array<types::global_dof_index, GeometryInfo<dim>::vertices_per_cell>
  get_vertex_dof_indices_swapped(
    const FiniteElement<dim, spacedim>         &fe,
    const std::vector<types::global_dof_index> &dof_indices)
  {
    Assert(dim == 2, ExcNotImplemented());

    std::array<types::global_dof_index, GeometryInfo<dim>::vertices_per_cell>
      vertex_dof_indices;

    /**
     * When the finite element is L2, such as @p FE_DGQ, its member
     * @p dofs_per_face is 0. Therefore, here we manually calculate the number
     * of DoFs per face.
     */
    const unsigned int dofs_per_face =
      fe.dofs_per_face > 0 ?
        fe.dofs_per_face :
        static_cast<unsigned int>(
          dealii::Utilities::fixed_power<dim - 1>(fe.degree + 1));

    vertex_dof_indices[0] = dof_indices[0];
    vertex_dof_indices[1] = dof_indices[dofs_per_face - 1];
    vertex_dof_indices[2] = dof_indices[dof_indices.size() - 1];
    vertex_dof_indices[3] =
      dof_indices[dof_indices.size() - 1 - (dofs_per_face - 1)];

    return vertex_dof_indices;
  }


  /**
   * Get the DoF indices associated with the cell vertices from a list of DoF
   * indices which have been arranged in either the forward or backward
   * lexicographic order. In this overloaded version, the results are returned
   * in an array as the last argument of this function.
   *
   * \mynote{There are <code>GeometryInfo<dim>::vertices_per_cell</code>
   * vertices in the returned array, among which the last two vertex DoF indices
   * have been swapped in this function so that the whole list of vertex DoF
   * indices in the returned array are arranged in either the clockwise or
   * counter clockwise order instead of the original lexicographic(zigzag)
   * order.}
   *
   * @param fe
   * @param dof_indices List of DoF indices in either the forward or backward
   * lexicographic order.
   * @param vertex_dof_indices [out] List of DoF indices for the cell vertices
   * or corners with the last two swapped.
   */
  template <int dim, int spacedim>
  void
  get_vertex_dof_indices_swapped(
    const FiniteElement<dim, spacedim>         &fe,
    const std::vector<types::global_dof_index> &dof_indices,
    std::array<types::global_dof_index, GeometryInfo<dim>::vertices_per_cell>
      &vertex_dof_indices)
  {
    Assert(dim == 2, ExcNotImplemented());

    /**
     * When the finite element is L2, such as @p FE_DGQ, its member
     * @p dofs_per_face is 0. Therefore, here we manually calculate the number
     * of DoFs per face.
     */
    const unsigned int dofs_per_face =
      fe.dofs_per_face > 0 ?
        fe.dofs_per_face :
        static_cast<unsigned int>(
          dealii::Utilities::fixed_power<dim - 1>(fe.degree + 1));

    vertex_dof_indices[0] = dof_indices[0];
    vertex_dof_indices[1] = dof_indices[dofs_per_face - 1];
    vertex_dof_indices[2] = dof_indices[dof_indices.size() - 1];
    vertex_dof_indices[3] =
      dof_indices[dof_indices.size() - 1 - (dofs_per_face - 1)];
  }


  /**
   * Get the local index for the starting vertex in the cell by checking
   * the list of numbers assigned to cell vertices with the last two swapped.
   *
   * \mynote{There are two cases to be processed here, common edge and common
   * vertex.
   * 1. In the common edge case, there are two DoF indices in the vector
   * <code>vertex_dof_index_intersection</code>. Then their array indices wrt.
   * the vector <code>local_vertex_dof_indices_swapped</code> will be
   * searched. By considering this vector as a closed loop list, the two DoF
   * indices in this vector are successively located and the first one of which
   * is the vertex to start subsequent DoF traversing.
   * 2. In the common vertex case, since there is only one DoF index in the
   * vector @p vertex_dof_index_intersection, this vertex is the starting point.}
   *
   * @param common_vertex_dof_indices The vector storing the pairs of vertex
   * DoF indices in \f$K_x\f$ and \f$K_y\f$, which share common vertices.
   * @param local_vertex_dof_indices_swapped Vertex DoF indices with the last
   * two swapped, which have been obtained from the function
   * @p get_vertex_dof_indices_swapped.
   * @param is_first_cell If the common vertex DoF indices in the first cell or
   * the second cell are to be extracted.
   * @return The array index for the starting vertex, wrt. the original list of
   * vertex DoF indices, i.e. the last two elements of which are not swapped.
   */
  template <int vertices_per_cell>
  unsigned int
  get_start_vertex_local_index_in_cell_from_vertex_numbers(
    const std::vector<std::pair<unsigned int, unsigned int>>
      &common_vertex_pair_local_indices,
    const std::array<unsigned int, vertices_per_cell>
              &vertex_local_indices_in_cell_with_last_two_swapped,
    const bool is_first_cell)
  {
    /**
     * The local index of the starting vertex should be in the range [0,
     * vertices_per_cell). Therefore, we use @p vertices_per_cell as its
     * initial invalid value.
     */
    unsigned int starting_vertex_local_index = vertices_per_cell;

    switch (common_vertex_pair_local_indices.size())
      {
          case 2: // Common edge case
          {
            unsigned int first_vertex_local_index;
            unsigned int second_vertex_local_index;

            if (is_first_cell)
              {
                first_vertex_local_index =
                  common_vertex_pair_local_indices[0].first;
                second_vertex_local_index =
                  common_vertex_pair_local_indices[1].first;
              }
            else
              {
                first_vertex_local_index =
                  common_vertex_pair_local_indices[0].second;
                second_vertex_local_index =
                  common_vertex_pair_local_indices[1].second;
              }

            typename std::array<unsigned int, vertices_per_cell>::const_iterator
              first_common_vertex_iterator = std::find(
                vertex_local_indices_in_cell_with_last_two_swapped.cbegin(),
                vertex_local_indices_in_cell_with_last_two_swapped.cend(),
                first_vertex_local_index);
            Assert(first_common_vertex_iterator !=
                     vertex_local_indices_in_cell_with_last_two_swapped.cend(),
                   ExcInternalError());

            if ((first_common_vertex_iterator + 1) !=
                vertex_local_indices_in_cell_with_last_two_swapped.cend())
              {
                if (*(first_common_vertex_iterator + 1) ==
                    second_vertex_local_index)
                  {
                    starting_vertex_local_index = first_vertex_local_index;
                  }
                else
                  {
                    starting_vertex_local_index = second_vertex_local_index;
                  }
              }
            else
              {
                if ((*vertex_local_indices_in_cell_with_last_two_swapped
                        .cbegin()) == second_vertex_local_index)
                  {
                    starting_vertex_local_index = first_vertex_local_index;
                  }
                else
                  {
                    starting_vertex_local_index = second_vertex_local_index;
                  }
              }

            break;
          }
          case 1: // Common vertex case
          {
            starting_vertex_local_index =
              is_first_cell ? common_vertex_pair_local_indices[0].first :
                              common_vertex_pair_local_indices[0].second;

            break;
          }
        default:
          Assert(false, ExcInternalError());
          break;
      }

    return starting_vertex_local_index;
  }


  /**
   * Permute DoFs support points in real cells and their associated global
   * DoF indices for Sauter quadrature, the behavior of which depends on the
   * detected cell neighboring types.
   *
   * \mynote{This version involves @p PairCellWiseScratchData and
   * @p PairCellWisePerTaskData.
   *
   * @param scratch
   * @param data
   * @param kx_cell_iter
   * @param ky_cell_iter
   * @param kx_mapping
   * @param ky_mapping
   */
  template <int dim, int spacedim, typename RangeNumberType = double>
  void
  permute_dofs_and_mapping_support_points_for_sauter_quad(
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &data,
    const CellNeighboringType cell_neighboring_type,
    const typename DoFHandler<dim, spacedim>::cell_iterator &kx_cell_iter,
    const typename DoFHandler<dim, spacedim>::cell_iterator &ky_cell_iter,
    const MappingQGenericExt<dim, spacedim>                 &kx_mapping,
    const MappingQGenericExt<dim, spacedim>                 &ky_mapping)
  {
    // Geometry information.
    const unsigned int vertices_per_cell = GeometryInfo<dim>::vertices_per_cell;

    const FiniteElement<dim, spacedim> &kx_fe = kx_cell_iter->get_fe();
    const FiniteElement<dim, spacedim> &ky_fe = ky_cell_iter->get_fe();

    switch (cell_neighboring_type)
      {
          case SamePanel: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     vertices_per_cell,
                   ExcInternalError());

            /**
             * Permute mapping support points into lexicographic order.
             */
            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_poly_space_numbering_inverse,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_poly_space_numbering_inverse,
                           scratch.ky_mapping_support_points_permuted);

            /**
             * Permute DoF indices in the finite elements.
             */
            if (kx_fe.dofs_per_cell > 1)
              {
                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_fe_poly_space_numbering_inverse,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ(0). Permutation is not needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                /**
                 * Get DoF indices in the lexicographic
                 * order.
                 */
                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_fe_poly_space_numbering_inverse,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          case CommonEdge: {
            /**
             * This part handles the common edge case of Sauter's quadrature
             * rule.
             * 1. Get the DoF indices in the lexicographic order for \f$K_x\f$.
             * 2. Get the DoF indices in the reversed lexicographic order for
             * \f$K_y\f$.
             * 3. Extract only those DoF indices which are located at cell
             * vertices in \f$K_x\f$ and \f$K_y\f$. N.B. The DoF indices for the
             * last two vertices are swapped, such that the four vertices are in
             * either clockwise or counter clockwise order.
             * 4. Determine the starting vertex for \f$K_x\f$ and regenerate the
             * permutation numbering for traversing in the forward lexicographic
             * order by starting from this vertex.
             * 5. Determine the starting vertex for \f$K_y\f$ and regenerate the
             * permutation numbering for traversing in the backward
             * lexicographic order by starting from this vertex.
             * 6. Apply the newly generated permutation numbering scheme to
             * support points and DoF indices in the original default DoF order.
             */

            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<dim>::vertices_per_face,
                   ExcInternalError());

            // Determine the starting vertex index in \f$K_x\f$.
            unsigned int kx_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 1, 3, 2}},
                                   true);
            AssertIndexRange(kx_starting_vertex_local_index, vertices_per_cell);

            // Determine the starting vertex index in \f$K_y\f$.
            unsigned int ky_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 2, 3, 1}},
                                   false);
            AssertIndexRange(ky_starting_vertex_local_index, vertices_per_cell);

            // Generate the permutation of support points for the mappings.
            generate_forward_mapping_support_point_permutation(
              kx_mapping,
              kx_starting_vertex_local_index,
              scratch.kx_mapping_support_point_permutation);
            generate_backward_mapping_support_point_permutation(
              ky_mapping,
              ky_starting_vertex_local_index,
              scratch.ky_mapping_support_point_permutation);

            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_support_point_permutation,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_support_point_permutation,
                           scratch.ky_mapping_support_points_permuted);

            if (kx_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_x\f$ by starting
                // from <code>kx_starting_vertex_index</code>.
                generate_forward_dof_permutation(
                  kx_fe,
                  kx_starting_vertex_local_index,
                  scratch.kx_local_dof_permutation);

                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_local_dof_permutation,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_y\f$ by starting
                // from <code>ky_starting_vertex_index</code>.
                generate_backward_dof_permutation(
                  ky_fe,
                  ky_starting_vertex_local_index,
                  scratch.ky_local_dof_permutation);

                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_local_dof_permutation,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          case CommonVertex: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 1,
                   ExcInternalError());

            // Determine the starting vertex index in \f$K_x\f$.
            unsigned int kx_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 1, 3, 2}},
                                   true);
            AssertIndexRange(kx_starting_vertex_local_index, vertices_per_cell);

            // Determine the starting vertex index in \f$K_y\f$.
            unsigned int ky_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 2, 3, 1}},
                                   false);
            AssertIndexRange(ky_starting_vertex_local_index, vertices_per_cell);

            // Generate the permutation of support points for the mappings.
            generate_forward_mapping_support_point_permutation(
              kx_mapping,
              kx_starting_vertex_local_index,
              scratch.kx_mapping_support_point_permutation);
            generate_forward_mapping_support_point_permutation(
              ky_mapping,
              ky_starting_vertex_local_index,
              scratch.ky_mapping_support_point_permutation);

            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_support_point_permutation,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_support_point_permutation,
                           scratch.ky_mapping_support_points_permuted);

            if (kx_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_x\f$ by starting
                // from <code>kx_starting_vertex_index</code>.
                generate_forward_dof_permutation(
                  kx_fe,
                  kx_starting_vertex_local_index,
                  scratch.kx_local_dof_permutation);

                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_local_dof_permutation,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_y\f$ by starting
                // from <code>ky_starting_vertex_index</code>.
                generate_forward_dof_permutation(
                  ky_fe,
                  ky_starting_vertex_local_index,
                  scratch.ky_local_dof_permutation);

                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_local_dof_permutation,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          case Regular: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 0,
                   ExcInternalError());

            /**
             * Permute mapping support points into lexicographic order.
             */
            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_poly_space_numbering_inverse,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_poly_space_numbering_inverse,
                           scratch.ky_mapping_support_points_permuted);

            if (kx_fe.dofs_per_cell > 1)
              {
                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_fe_poly_space_numbering_inverse,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_fe_poly_space_numbering_inverse,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          default: {
            Assert(false, ExcInternalError());
          }
      }

    /**
     * @internal Extract two components from the permuted mapping support
     * points.
     */
    for (unsigned int i = 0;
         i < scratch.kx_mapping_support_points_permuted.size();
         i++)
      {
        scratch.kx_mapping_support_points_permuted_xy_components[i](0) =
          scratch.kx_mapping_support_points_permuted[i](0);
        scratch.kx_mapping_support_points_permuted_xy_components[i](1) =
          scratch.kx_mapping_support_points_permuted[i](1);

        scratch.kx_mapping_support_points_permuted_yz_components[i](0) =
          scratch.kx_mapping_support_points_permuted[i](1);
        scratch.kx_mapping_support_points_permuted_yz_components[i](1) =
          scratch.kx_mapping_support_points_permuted[i](2);

        scratch.kx_mapping_support_points_permuted_zx_components[i](0) =
          scratch.kx_mapping_support_points_permuted[i](2);
        scratch.kx_mapping_support_points_permuted_zx_components[i](1) =
          scratch.kx_mapping_support_points_permuted[i](0);
      }

    for (unsigned int i = 0;
         i < scratch.ky_mapping_support_points_permuted.size();
         i++)
      {
        scratch.ky_mapping_support_points_permuted_xy_components[i](0) =
          scratch.ky_mapping_support_points_permuted[i](0);
        scratch.ky_mapping_support_points_permuted_xy_components[i](1) =
          scratch.ky_mapping_support_points_permuted[i](1);

        scratch.ky_mapping_support_points_permuted_yz_components[i](0) =
          scratch.ky_mapping_support_points_permuted[i](1);
        scratch.ky_mapping_support_points_permuted_yz_components[i](1) =
          scratch.ky_mapping_support_points_permuted[i](2);

        scratch.ky_mapping_support_points_permuted_zx_components[i](0) =
          scratch.ky_mapping_support_points_permuted[i](2);
        scratch.ky_mapping_support_points_permuted_zx_components[i](1) =
          scratch.ky_mapping_support_points_permuted[i](0);
      }
  }


  namespace CUDAWrappers
  {
    extern cudaDeviceProp device_properties;

    /**
     * Determine the dimensions of block rectangle and thread rectangle for CUDA
     * parallelization over Sauter quadrature points.
     *
     * N.B. The allocation of thread blocks are enough to cover all quadrature
     * points. Therefore, there is no for-loop inside related kernel functions.
     *
     * @param cell_neighboring_type
     * @param blocks_rect
     * @param threads_rect
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    void
    configure_thread_blocks_for_sauter_quadrature_points(
      const CellNeighboringType                        cell_neighboring_type,
      const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
      dim3                                            &blocks_rect,
      dim3                                            &threads_rect)
    {
      switch (cell_neighboring_type)
        {
            case SamePanel: {
              threads_rect.x = 32;
              threads_rect.y = 8;

              blocks_rect.x = (bem_values.quad_rule_for_same_panel.size() +
                               threads_rect.x - 1) /
                              threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            case CommonEdge: {
              threads_rect.x = 32;
              threads_rect.y = 6;

              blocks_rect.x = (bem_values.quad_rule_for_common_edge.size() +
                               threads_rect.x - 1) /
                              threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            case CommonVertex: {
              threads_rect.x = 32;
              threads_rect.y = 4;

              blocks_rect.x = (bem_values.quad_rule_for_common_vertex.size() +
                               threads_rect.x - 1) /
                              threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            case Regular: {
              threads_rect.x = 32;
              threads_rect.y = 1;

              blocks_rect.x =
                (bem_values.quad_rule_for_regular.size() + threads_rect.x - 1) /
                threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            default: {
              threads_rect.x = 32;
              threads_rect.y = 1;

              blocks_rect.x = 1;
              blocks_rect.y = 1;

              break;
            }
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and quadrature points in
     * the real cell to be used in the Sauter quadrature (for same panel cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_same_panel(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        gridDim.x * blockDim.x * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);

      if (current_quad_point_index < bem_values.quad_rule_for_same_panel.size())
        {
          scratch_data.kx_jacobians_same_panel(threadIdx.y,
                                               current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              current_quad_point_index,
              bem_values.kx_mapping_shape_grad_matrix_table_for_same_panel,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_same_panel(threadIdx.y,
                                                 current_quad_point_index));

          scratch_data.ky_jacobians_same_panel(threadIdx.y,
                                               current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              current_quad_point_index,
              bem_values.ky_mapping_shape_grad_matrix_table_for_same_panel,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_same_panel(threadIdx.y,
                                                 current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            current_quad_point_index,
            bem_values.kx_mapping_shape_value_table_for_same_panel,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_same_panel(threadIdx.y,
                                                   current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            current_quad_point_index,
            bem_values.ky_mapping_shape_value_table_for_same_panel,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_same_panel(threadIdx.y,
                                                   current_quad_point_index));
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and qudrature points in
     * the real cell to be used in the Sauter quadrature (for common edge cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_common_edge(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        gridDim.x * blockDim.x * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);

      if (current_quad_point_index <
          bem_values.quad_rule_for_common_edge.size())
        {
          scratch_data.kx_jacobians_common_edge(threadIdx.y,
                                                current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              current_quad_point_index,
              bem_values.kx_mapping_shape_grad_matrix_table_for_common_edge,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_common_edge(threadIdx.y,
                                                  current_quad_point_index));

          scratch_data.ky_jacobians_common_edge(threadIdx.y,
                                                current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              current_quad_point_index,
              bem_values.ky_mapping_shape_grad_matrix_table_for_common_edge,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_common_edge(threadIdx.y,
                                                  current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            current_quad_point_index,
            bem_values.kx_mapping_shape_value_table_for_common_edge,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_common_edge(threadIdx.y,
                                                    current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            current_quad_point_index,
            bem_values.ky_mapping_shape_value_table_for_common_edge,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_common_edge(threadIdx.y,
                                                    current_quad_point_index));
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and quadrature points in
     * the real cell to be used in the Sauter quadrature (for common vertex cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_common_vertex(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        gridDim.x * blockDim.x * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);

      if (current_quad_point_index <
          bem_values.quad_rule_for_common_vertex.size())
        {
          scratch_data.kx_jacobians_common_vertex(threadIdx.y,
                                                  current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              current_quad_point_index,
              bem_values.kx_mapping_shape_grad_matrix_table_for_common_vertex,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_common_vertex(threadIdx.y,
                                                    current_quad_point_index));

          scratch_data.ky_jacobians_common_vertex(threadIdx.y,
                                                  current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              current_quad_point_index,
              bem_values.ky_mapping_shape_grad_matrix_table_for_common_vertex,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_common_vertex(threadIdx.y,
                                                    current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            current_quad_point_index,
            bem_values.kx_mapping_shape_value_table_for_common_vertex,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_common_vertex(
              threadIdx.y, current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            current_quad_point_index,
            bem_values.ky_mapping_shape_value_table_for_common_vertex,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_common_vertex(
              threadIdx.y, current_quad_point_index));
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and quadrature points in
     * the real cell to be used in the Sauter quadrature (for regular cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_regular(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        blockIdx.x * blockDim.x + threadIdx.x;

      if (current_quad_point_index < bem_values.quad_rule_for_regular.size())
        {
          scratch_data.kx_jacobians_regular(0, current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              0,
              current_quad_point_index,
              bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_regular(0, current_quad_point_index));

          scratch_data.ky_jacobians_regular(0, current_quad_point_index) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              0,
              current_quad_point_index,
              bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_regular(0, current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            0,
            current_quad_point_index,
            bem_values.kx_mapping_shape_value_table_for_regular,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_regular(0, current_quad_point_index));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            0,
            current_quad_point_index,
            bem_values.ky_mapping_shape_value_table_for_regular,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_regular(0, current_quad_point_index));
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_same_panel(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        gridDim.x * blockDim.x * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);

      if (current_quad_point_index < bem_values.quad_rule_for_same_panel.size())
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_same_panel(
              threadIdx.y, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            current_quad_point_index,
            bem_values.kx_mapping_shape_grad_matrix_table_for_same_panel,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_same_panel(
              threadIdx.y, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            current_quad_point_index,
            bem_values.ky_mapping_shape_grad_matrix_table_for_same_panel,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_common_edge(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        gridDim.x * blockDim.x * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);

      if (current_quad_point_index <
          bem_values.quad_rule_for_common_edge.size())
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_common_edge(
              threadIdx.y, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            current_quad_point_index,
            bem_values.kx_mapping_shape_grad_matrix_table_for_common_edge,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_common_edge(
              threadIdx.y, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            current_quad_point_index,
            bem_values.ky_mapping_shape_grad_matrix_table_for_common_edge,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_common_vertex(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        gridDim.x * blockDim.x * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);

      if (current_quad_point_index <
          bem_values.quad_rule_for_common_vertex.size())
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_common_vertex(
              threadIdx.y, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            current_quad_point_index,
            bem_values.kx_mapping_shape_grad_matrix_table_for_common_vertex,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_common_vertex(
              threadIdx.y, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            current_quad_point_index,
            bem_values.ky_mapping_shape_grad_matrix_table_for_common_vertex,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_regular(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        blockIdx.x * blockDim.x + threadIdx.x;

      if (current_quad_point_index < bem_values.quad_rule_for_regular.size())
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_regular(
              0, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            0,
            current_quad_point_index,
            bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_regular(
              0, current_quad_point_index, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            0,
            current_quad_point_index,
            bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    /**
     * Sum using warp reduce.
     *
     * @param data
     * @param tid
     */
    template <typename RangeNumberType = double>
    __device__ void
    warpReduce(volatile RangeNumberType *data, const unsigned int tid)
    {
      assert(warpSize == 32);

      RangeNumberType temp;
      temp = data[tid + 16];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 8];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 4];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 2];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 1];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the same
     * panel case.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesSamePanel(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = blockIdx.x * blockDim.x + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_same_panel.size())
        {
          const CUDATable<2, double> &quad_points =
            bem_values.quad_rule_for_same_panel.get_points();

          /**
           * @internal Evaluate the kernel function at the current quadrature
           * point, which is an accumulation of all @p k3 terms.
           */
          RangeNumberType kernel_value = 0.;

#pragma unroll
          for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
            {
              if (kernel_function.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current shape
                   * function at the current quadrature point in the unit cell
                   * for \f$K_x\f$ as well as \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

#pragma unroll
                  for (unsigned int i = 0; i < dim; i++)
                    {
                      kx_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_same_panel(
                          k3_index, quad_no, i, kx_dof_index);
                      ky_shape_grad_in_unit_cell[i] =
                        bem_values.ky_shape_grad_matrix_table_for_same_panel(
                          k3_index, quad_no, i, ky_dof_index);
                    }

                  /**
                   * Apply covariant transformation to the gradient tensors in
                   * the unit cell.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.kx_covariants_same_panel(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);
                  CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.ky_covariants_same_panel(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector and
                   * the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.kx_normals_same_panel(k3_index, quad_no),
                      kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.ky_normals_same_panel(k3_index, quad_no),
                      ky_shape_grad_in_real_cell_vector);

                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.ky_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.kx_normals_same_panel(k3_index, quad_no),
                      scratch_data.ky_normals_same_panel(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_same_panel(k3_index, quad_no) *
                    scratch_data.ky_jacobians_same_panel(k3_index, quad_no) *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl);
                }
              else
                {
                  /**
                   * @internal Evaluate the kernel function at the specified
                   * pair of points in the real cells with their normal vectors,
                   * the result of which is then multiplied by the Jacobians
                   * from unit cell to real cell and shape function values.
                   */
                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.ky_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.kx_normals_same_panel(k3_index, quad_no),
                      scratch_data.ky_normals_same_panel(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_same_panel(k3_index, quad_no) *
                    scratch_data.ky_jacobians_same_panel(k3_index, quad_no) *
                    bem_values.kx_shape_value_table_for_same_panel(kx_dof_index,
                                                                   k3_index,
                                                                   quad_no) *
                    bem_values.ky_shape_value_table_for_same_panel(ky_dof_index,
                                                                   k3_index,
                                                                   quad_no);
                }
            }

          /**
           * @internal Multiply the kernel function value with the Jacobian from
           * the Sauter 4D parametric space to the unit cell, then with the
           * quadrature weights. The result is stored into the shared memory.
           */
          quad_values_in_thread_block[tid] =
            kernel_value * quad_points(quad_no, 0) *
            (1 - quad_points(quad_no, 0)) *
            (1 - quad_points(quad_no, 0) * quad_points(quad_no, 1)) *
            bem_values.quad_rule_for_same_panel.get_weights()[quad_no];
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result (scaled by the factor) for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the common
     * edge case.
     *
     * \myalert{During the computation, the normal vector of \f$K_y\f$} should
     * be negated.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesCommonEdge(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = blockIdx.x * blockDim.x + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_common_edge.size())
        {
          const CUDATable<2, double> &quad_points =
            bem_values.quad_rule_for_common_edge.get_points();
          RangeNumberType jacobian_det1 = quad_points(quad_no, 0) *
                                          quad_points(quad_no, 0) *
                                          (1 - quad_points(quad_no, 0));
          RangeNumberType jacobian_det2 =
            quad_points(quad_no, 0) * quad_points(quad_no, 0) *
            (1 - quad_points(quad_no, 0) * quad_points(quad_no, 1));

          /**
           * @internal Evaluate the kernel function at the current quadrature
           * point, which is an accumulation of all @p k3 terms.
           */
          RangeNumberType kernel_value = 0.;

#pragma unroll
          for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
            {
              if (kernel_function.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current shape
                   * function at the current quadrature point in the unit cell
                   * for \f$K_x\f$ as well as \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

#pragma unroll
                  for (unsigned int i = 0; i < dim; i++)
                    {
                      kx_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_common_edge(
                          k3_index, quad_no, i, kx_dof_index);
                      ky_shape_grad_in_unit_cell[i] =
                        bem_values.ky_shape_grad_matrix_table_for_common_edge(
                          k3_index, quad_no, i, ky_dof_index);
                    }

                  /**
                   * Apply covariant transformation to the gradient tensors in
                   * the unit cell.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.kx_covariants_common_edge(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);
                  CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.ky_covariants_common_edge(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector and
                   * the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors to be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.kx_normals_common_edge(k3_index, quad_no),
                      kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(
                      -scratch_data.ky_normals_common_edge(k3_index, quad_no),
                      ky_shape_grad_in_real_cell_vector);

                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.ky_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.kx_normals_common_edge(k3_index, quad_no),
                      -scratch_data.ky_normals_common_edge(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_edge(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_edge(k3_index, quad_no) *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl) *
                    (k3_index <= 1 ? jacobian_det1 : jacobian_det2);
                }
              else
                {
                  /**
                   * @internal Evaluate the kernel function at the specified
                   * pair of points in the real cells with their normal vectors,
                   * the result of which is then multiplied by the Jacobians
                   * from unit cell to real cell, Jacobian from Sauter 4D space
                   * to unit cell, and shape function values.
                   */
                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.ky_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.kx_normals_common_edge(k3_index, quad_no),
                      -scratch_data.ky_normals_common_edge(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_edge(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_edge(k3_index, quad_no) *
                    bem_values.kx_shape_value_table_for_common_edge(
                      kx_dof_index, k3_index, quad_no) *
                    bem_values.ky_shape_value_table_for_common_edge(
                      ky_dof_index, k3_index, quad_no) *
                    (k3_index <= 1 ? jacobian_det1 : jacobian_det2);
                }
            }

          /**
           * @internal Multiply the kernel function value with the quadrature
           * weights. The result is stored into the shared memory.
           */
          quad_values_in_thread_block[tid] =
            kernel_value *
            bem_values.quad_rule_for_common_edge.get_weights()[quad_no];
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the common
     * vertex case.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesCommonVertex(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = blockIdx.x * blockDim.x + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_common_vertex.size())
        {
          const CUDATable<2, double> &quad_points =
            bem_values.quad_rule_for_common_vertex.get_points();

          /**
           * @internal Evaluate the kernel function at the current quadrature
           * point, which is an accumulation of all @p k3 terms.
           */
          RangeNumberType kernel_value = 0.;

#pragma unroll
          for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
            {
              if (kernel_function.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current shape
                   * function at the current quadrature point in the unit cell
                   * for \f$K_x\f$ as well as \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

#pragma unroll
                  for (unsigned int i = 0; i < dim; i++)
                    {
                      kx_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_common_vertex(
                          k3_index, quad_no, i, kx_dof_index);
                      ky_shape_grad_in_unit_cell[i] =
                        bem_values.ky_shape_grad_matrix_table_for_common_vertex(
                          k3_index, quad_no, i, ky_dof_index);
                    }

                  /**
                   * Apply covariant transformation to the gradient tensors in
                   * the unit cell.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.kx_covariants_common_vertex(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);
                  CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.ky_covariants_common_vertex(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector and
                   * the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors to be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.kx_normals_common_vertex(k3_index, quad_no),
                      kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.ky_normals_common_vertex(k3_index, quad_no),
                      ky_shape_grad_in_real_cell_vector);

                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.ky_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.kx_normals_common_vertex(k3_index, quad_no),
                      scratch_data.ky_normals_common_vertex(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_vertex(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_vertex(k3_index, quad_no) *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl);
                }
              else
                {
                  /**
                   * @internal Evaluate the kernel function at the specified
                   * pair of points in the real cells with their normal vectors,
                   * the result of which is then multiplied by the Jacobians
                   * from unit cell to real cell and shape function values.
                   */
                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.ky_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.kx_normals_common_vertex(k3_index, quad_no),
                      scratch_data.ky_normals_common_vertex(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_vertex(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_vertex(k3_index, quad_no) *
                    bem_values.kx_shape_value_table_for_common_vertex(
                      kx_dof_index, k3_index, quad_no) *
                    bem_values.ky_shape_value_table_for_common_vertex(
                      ky_dof_index, k3_index, quad_no);
                }
            }

          /**
           * @internal Multiply the kernel function value with the Jacobian from
           * the Sauter 4D parametric space to the unit cell, then with the
           * quadrature weights. The result is stored into the shared memory.
           */
          quad_values_in_thread_block[tid] =
            kernel_value *
            IdeoBEM::Utilities::CUDAWrappers::fixed_power<3>(
              quad_points(quad_no, 0)) *
            bem_values.quad_rule_for_common_vertex.get_weights()[quad_no];
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result (scaled by the factor) for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the regular
     * case.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesRegular(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = blockIdx.x * blockDim.x + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_regular.size())
        {
          if (kernel_function.kernel_type == HyperSingularRegular)
            {
              /**
               * @internal Extract the gradient values of the current shape
               * function at the current quadrature point in the unit cell
               * for \f$K_x\f$ as well as \f$K_y\f$.
               */
              RangeNumberType             kx_shape_grad_in_unit_cell[dim];
              RangeNumberType             ky_shape_grad_in_unit_cell[dim];
              CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                kx_shape_grad_in_unit_cell, dim);
              CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                ky_shape_grad_in_unit_cell, dim);

#pragma unroll
              for (unsigned int i = 0; i < dim; i++)
                {
                  kx_shape_grad_in_unit_cell[i] =
                    bem_values.kx_shape_grad_matrix_table_for_regular(
                      0, quad_no, i, kx_dof_index);
                  ky_shape_grad_in_unit_cell[i] =
                    bem_values.ky_shape_grad_matrix_table_for_regular(
                      0, quad_no, i, ky_dof_index);
                }

              /**
               * Apply covariant transformation to the gradient tensors in
               * the unit cell.
               */
              RangeNumberType             kx_shape_grad_in_real_cell[spacedim];
              RangeNumberType             ky_shape_grad_in_real_cell[spacedim];
              CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                kx_shape_grad_in_real_cell, spacedim);
              CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                ky_shape_grad_in_real_cell, spacedim);

              CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                const_cast<RangeNumberType *>(
                  &(scratch_data.kx_covariants_regular(0, quad_no, 0, 0))),
                spacedim,
                dim);
              CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                const_cast<RangeNumberType *>(
                  &(scratch_data.ky_covariants_regular(0, quad_no, 0, 0))),
                spacedim,
                dim);

              kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                        kx_shape_grad_in_unit_cell_vector);
              ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                        ky_shape_grad_in_unit_cell_vector);

              /**
               * Calculate the surface gradient tensor of the shape
               * functions, which is the cross product of normal vector and
               * the volume gradient vector.
               *
               * \mynote{The cross product operation requires the input
               * vectors be transformed to tensors.}
               */
              Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                cross_product_3d(scratch_data.kx_normals_regular(0, quad_no),
                                 kx_shape_grad_in_real_cell_vector);
              Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                cross_product_3d(scratch_data.ky_normals_regular(0, quad_no),
                                 ky_shape_grad_in_real_cell_vector);

              quad_values_in_thread_block[tid] =
                kernel_function.value(
                  scratch_data.kx_quad_points_regular(0, quad_no),
                  scratch_data.ky_quad_points_regular(0, quad_no),
                  scratch_data.kx_normals_regular(0, quad_no),
                  scratch_data.ky_normals_regular(0, quad_no),
                  component) *
                scratch_data.kx_jacobians_regular(0, quad_no) *
                scratch_data.ky_jacobians_regular(0, quad_no) *
                scalar_product(kx_shape_surface_curl, ky_shape_surface_curl) *
                bem_values.quad_rule_for_regular.get_weights()[quad_no];
            }
          else
            {
              /**
               * @internal Evaluate the kernel function at the specified
               * pair of points in the real cells with their normal vectors,
               * the result of which is then multiplied by the Jacobians
               * from unit cell to real cell, shape function values and
               * quadrature weights.
               */
              quad_values_in_thread_block[tid] =
                kernel_function.value(
                  scratch_data.kx_quad_points_regular(0, quad_no),
                  scratch_data.ky_quad_points_regular(0, quad_no),
                  scratch_data.kx_normals_regular(0, quad_no),
                  scratch_data.ky_normals_regular(0, quad_no),
                  component) *
                scratch_data.kx_jacobians_regular(0, quad_no) *
                scratch_data.ky_jacobians_regular(0, quad_no) *
                bem_values.kx_shape_value_table_for_regular(kx_dof_index,
                                                            0,
                                                            quad_no) *
                bem_values.ky_shape_value_table_for_regular(ky_dof_index,
                                                            0,
                                                            quad_no) *
                bem_values.quad_rule_for_regular.get_weights()[quad_no];
            }
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result (scaled by the factor) for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    enum SauterQuadratureTaskStatus
    {
      before_creation,
      during_creation,
      created,
      during_processing
    };


    /**
     * Ring buffer for holding Sauter quadrature tasks, which is used in the
     * producer-consumer model for building near field \hmatrices.
     *
     * @tparam capacity
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     */
    template <int capacity,
              int dim,
              int spacedim,
              typename RangeNumberType = double>
    class SauterQuadratureTaskRingBuffer
    {
    public:
      /**
       * Allocate memory for the ring buffer on the host and device.
       *
       * @param cell_neighboring_type Cell neighboring type to be handled in
       * this queue
       * @param num_to_fetch The number of quadrature tasks, which are treated
       * as a batch for being processed by a consumer thread.
       * @param bem_values @p BEMValues on the host, which is used to pass
       * necessary data sizes, such as the number of shape functions used by the
       * mapping objects, the number of quadrature points, etc.
       */
      __host__ void
      allocate(const CellNeighboringType       neighboring_type,
               const unsigned int              num_to_fetch,
               const BEMValues<dim, spacedim> &bem_values);

      /**
       * Release the memory on the host and device.
       */
      __host__ void
      release();

      __host__ unsigned int
      task_num_during_creation() const;

      __host__ unsigned int
      task_num_ready_for_processing() const;

      __host__ unsigned int
      task_num_during_processing() const;

      __host__ unsigned
      task_num_total() const;

      __host__ bool
      is_full() const;

      __host__ bool
      is_not_full() const;

      __host__ bool
      is_empty() const;

      /**
       * Add a Sauter quadrature task to the ring buffer, which is called by a
       * producer thread.
       *
       * \mynote{About the @p scratch_data argument
       * 1. It data fields for permuted mapping support points in \f$K_x\f$ and
       * \f$K_y\f$ should have been computed for a pair of DoFs on the CPU.
       * 2. It is created with the creation of the associated producer thread.
       * During its initialization, a CUDA stream is created for this producer
       * thread.
       * 3. When adding a Sauter quadrature task, its data fields for permuted
       * mapping support points in \f$K_x\f$ and \f$K_y\f$ will be copied to the
       * related data fields in the ring buffer.
       * 4. For other quadrature related values, namely Jacobians, normal
       * vectors, covariant matrices and quadrature point coordinates in real
       * cells, they will be computed on the device when the quadrature task is
       * fetched from the ring buffer. The results will be stored back into the
       * ring buffer.}
       *
       * @param fullmat Full matrix within a near field \hmatrix node
       * @param fullmat_row_index Row index of the entry in the full matrix
       * @param fullmat_col_index Column index of the entry in the full matrix
       * @param i_local_index
       * @param j_local_index
       * @param mass_matrix_entry Value of the mass matrix entry, if there is any.
       * @param scratch_data
       */
      __host__ void
      add_task(LAPACKFullMatrixExt<RangeNumberType> *fullmat,
               const unsigned int                    fullmat_row_index,
               const unsigned int                    fullmat_col_index,
               const unsigned int                    i_local_index,
               const unsigned int                    j_local_index,
               const RangeNumberType                 mass_matrix_entry,
               const PairCellWiseScratchData<dim, spacedim, RangeNumberType>
                 &scratch_data);

      /**
       * @param result_lock This should be a mutex for a same set of ring
       * buffers for the four types of cell neighboring types. Since for a same
       * pair of global DoFs, it may be from pairs of cells having different
       * neighboring types.
       */
      template <template <int, typename> typename KernelFunctionType>
      __host__ bool
      fetch_and_process_task_batch(
        const KernelFunctionType<spacedim, RangeNumberType> &kernel,
        const RangeNumberType                                kernel_factor,
        const unsigned int                                   block_size,
        const CUDABEMValues<dim, spacedim, RangeNumberType> &bem_values,
        std::mutex                                          &result_lock,
        const cudaStream_t cuda_stream_handle);

      /**
       * Print the current values of all ring buffer pointers. For debugging
       * purpose only.
       *
       * @param log_stream
       */
      __host__ void
      print_pointers(LogStream &log_stream) const;

      /**
       * The cell neighboring type for the quadrature tasks in the current ring
       * buffer.
       *
       * \mynote{Within a ring buffer, all quadrature tasks have a same cell
       * neighboring type, so that the CUDA threads can run in lockstep without
       * branching.}
       */
      CellNeighboringType cell_neighboring_type;

      /**
       * Mutex for protecting multi-threaded accesses to the ring buffer
       * properties, but not the actual task data.
       */
      std::mutex buffer_lock;

      /**
       * Condition variable indicating the ring buffer not being full, i.e. a
       * new task is allowed to be added from a producer thread.
       */
      std::condition_variable buffer_not_full;

      /**
       * Condition variable indicating the ring buffer not being empty and
       * having enough number of tasks to be fetched and processed as a batch by
       * a consumer thread. Then a consumer thread can start to wait for
       * processing a batch of contiguous tasks
       * (starting from the task pointed by @p head_pending) having been created.
       */
      std::condition_variable batch_task_ready;

      /**
       * Array of atomic variables storing the task status in the ring buffer.
       */
      std::atomic<SauterQuadratureTaskStatus> task_status_ring_buffer[capacity];

      /**
       * List of permuted mapping support points in \f$K_x\f$, which are stored
       * on the host.
       *
       * 1. First dimension: task index in the ring buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_x\f$.
       */
      Table<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_ring_buffer_cpu;

      /**
       * List of permuted mapping support points in \f$K_y\f$, which are stored
       * on the host.
       *
       * 1. First dimension: task index in the ring buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_y\f$.
       */
      Table<2, Point<spacedim, RangeNumberType>>
        ky_mapping_support_points_permuted_ring_buffer_cpu;

      /**
       * List of permuted mapping support points in \f$K_x\f$, which are stored
       * on the device.
       *
       * 1. First dimension: task index in the ring buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_x\f$.
       *
       * \mynote{Because @p CUDATable class is adopted for storing the list of
       * permuted mapping support points, the size of its second dimension is
       * constant, which prescribes that all cells in the mesh adopt a same
       * mapping object.
       *
       * TODO When different mapping objects are adopted for different
       * subdomains, this should be modified accordingly.}
       */
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_ring_buffer_gpu;

      /**
       * List of permuted mapping support points in \f$K_y\f$, which are stored
       * on the device.
       *
       * 1. First dimension: task index in the ring buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_y\f$.
       *
       * \mynote{Because @p CUDATable class is adopted for storing the list of
       * permuted mapping support points, the size of its second dimension is
       * constant, which prescribes that all cells in the mesh adopt a same
       * mapping object.
       *
       * TODO When different mapping objects are adopted for different
       * subdomains, this should be modified accordingly.}
       */
      CUDATable<2, Point<spacedim, RangeNumberType>>
        ky_mapping_support_points_permuted_ring_buffer_gpu;

      /**
       * Array of pointers to the full matrices held within the near field
       * \hmatrix leaf nodes.
       *
       * \mynote{This array is located on the host, since it is only related to
       * quadrature result assembly.}
       */
      LAPACKFullMatrixExt<RangeNumberType> *fullmat_ring_buffer_cpu[capacity];

      /**
       * Array of row indices for the entries in the corresponding full matrix.
       *
       * \mynote{This array is located on the host, since it is only related to
       * quadrature result assembly.}
       */
      unsigned int fullmat_row_index_ring_buffer_cpu[capacity];

      /**
       * Array of column indices for the entries in the corresponding full
       * matrix.
       *
       * \mynote{This array is located on the host, since it is only related to
       * quadrature result assembly.}
       */
      unsigned int fullmat_col_index_ring_buffer_cpu[capacity];

      /**
       * Array of local DoF indices in \f$K_x\f$ corresponding to the global DoF
       * index \f$i\f$, which are stored on the host.
       */
      unsigned int *i_local_index_ring_buffer_cpu;

      /**
       * Array of local DoF indices in \f$K_y\f$ corresponding to the global DoF
       * index \f$j\f$, which are stored on the host.
       */
      unsigned int *j_local_index_ring_buffer_cpu;

      /**
       * Array of local DoF indices in \f$K_x\f$ corresponding to the global DoF
       * index \f$i\f$.
       *
       * \mynote{Because the local index associated with the global DoF @p i
       * will be used during the evaluation Sauter quadrature, this array is
       * allocated on the device.}
       */
      unsigned int *i_local_index_ring_buffer_gpu;

      /**
       * Array of local DoF indices in \f$K_y\f$ corresponding to the global DoF
       * index \f$j\f$.
       *
       * \mynote{Because the local index associated with the global DoF @p j
       * will be used during the evaluation Sauter quadrature, this array is
       * allocated on the device.}
       */
      unsigned int *j_local_index_ring_buffer_gpu;

      /**
       * Mass matrix entries
       */
      RangeNumberType mass_matrix_entries_ring_buffer_cpu[capacity];

      /**
       * Array of quadrature values on the host.
       */
      RangeNumberType *quadrature_value_ring_buffer_cpu;

      /**
       * Array of quadrature values on the device.
       */
      RangeNumberType *quadrature_value_ring_buffer_gpu;

      /**
       * Flag indicating if all matrix entries in all near filed \hmatrices have
       * been added as tasks to the ring buffer. If it is true, even though the
       * number of tasks in the buffer (@p task_num) is less than the total
       * number of GPU threads specified to compute the Sauter quadrature, the
       * consumer threads will get all of them directly and perform the
       * computation.
       *
       * \mynote{This flag will be set to true when all producer threads have
       * exited. After that, the consumer threads will be notified to fetch and
       * process the remaining tasks in the buffer, the number of which may be
       * smaller than the batch size.}
       */
      bool if_all_near_field_hmat_entries_added;

      /**
       * Number of tasks in a batch for fetching.
       */
      unsigned int batch_size;

      /**
       * Index of the head position in the ring buffer, which points to the
       * first task being processed.
       */
      unsigned int head_committed;

      /**
       * Index of the head position in the ring buffer, from where a task can
       * be claimed to fetch.
       */
      unsigned int head_pending;

      /**
       * Index of the tail position in the ring buffer, which points to the
       * first task being created.
       */
      unsigned int tail_committed;

      /**
       * Index of the tail position in the ring buffer, at which a new task can
       * be claimed to add.
       */
      unsigned int tail_pending;
    };


    /**
     * Buffer class for the assembly of a row or column vector in an \hmat.
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    class SauterQuadratureTaskBufferForVector
    {
    public:
      /**
       * Constructor
       *
       * \mynote{The product of @p max_cells_per_dof_in_test_space and
       * @p max_cells_per_dof_in_ansatz_space is used to estimate the number of
       * pairs of cells contributing to a same pair of global DoFs.}
       *
       * @param v_vector_size
       * @param max_cells_per_dof_in_test_space
       * @param max_cells_per_dof_in_ansatz_space
       * @param bem_values
       * @param stream
       */
      SauterQuadratureTaskBufferForVector(
        const unsigned int              v_vector_size,
        const unsigned int              max_cells_per_dof_in_test_space,
        const unsigned int              max_cells_per_dof_in_ansatz_space,
        const BEMValues<dim, spacedim> &bem_values,
        const cudaStream_t              stream);

      /**
       * Reinitialize the buffer to be used for another vector having the same
       * size as before.
       */
      void
      reinit();

      /**
       * Destructor
       */
      ~SauterQuadratureTaskBufferForVector();

      /**
       * Add a Sauter quadrature task to the buffer, the data of which are still
       * on the host.
       *
       * @param i_local_index
       * @param j_local_index
       * @param scratch_data
       */
      void
      add_task(const unsigned int i_local_index,
               const unsigned int j_local_index,
               const PairCellWiseScratchData<dim, spacedim, RangeNumberType>
                 &scratch_data);

      /**
       * Copy data from host to device and process all Sauter quadrature tasks
       * in the buffer, which are used for build a whole row or column vector in
       * the far field \hmat.
       */
      template <template <int, typename> typename KernelFunctionType>
      void
      process_tasks(
        Vector<RangeNumberType>                             &hmat_vector,
        const KernelFunctionType<spacedim, RangeNumberType> &kernel,
        const RangeNumberType                                kernel_factor,
        const CUDABEMValues<dim, spacedim, RangeNumberType> &bem_values,
        const unsigned int                                   task_num_per_block,
        const unsigned int block_size_for_quad_result_accumulation,
        const cudaStream_t cuda_stream_handle);

      void
      compute_vector(Vector<RangeNumberType> &vec);

      /**
       * CUDA stream.
       *
       * \mynote{This CUDA stream handle is copied from that held within the
       * scratch data. This class is not responsible for the creation and
       * destroy of this handle.}
       */
      cudaStream_t cuda_stream_handle;

      /**
       * Number of elements in the row or column vector.
       */
      unsigned int vector_size;

      /**
       * Capacity of the task buffer, which is
       * @p vector_size*max_cells_per_dof_in_test_space*max_cells_per_dof_in_ansatz_space.
       */
      unsigned int capacity;

      /**
       * Number of quadrature points.
       *
       * \mynote{For building far field \hmats, the cell neighboring type can
       * only be @p regular, therefore the number of quadrature points is the
       * size of the quadrature rule for this regular case, which can be
       * acquired from @p BEMValues.}
       */
      unsigned int quad_point_num;

      /**
       * Actual number of Sauter quadrature tasks in the buffer.
       */
      unsigned int task_num;

      unsigned int *starting_task_indices_for_dof_pairs_cpu;
      unsigned int *starting_task_indices_for_dof_pairs_gpu;

      /**
       * List of permuted mapping support points in \f$K_x\f$, which are stored
       * on the host.
       *
       * 1. First dimension: task index in the buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_x\f$.
       */
      Table<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_buffer_cpu;

      /**
       * List of permuted mapping support points in \f$K_y\f$, which are stored
       * on the host.
       *
       * 1. First dimension: task index in the buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_y\f$.
       */
      Table<2, Point<spacedim, RangeNumberType>>
        ky_mapping_support_points_permuted_buffer_cpu;

      /**
       * List of permuted mapping support points in \f$K_x\f$, which are stored
       * on the device.
       *
       * 1. First dimension: task index in the buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_x\f$.
       *
       * \mynote{Because @p CUDATable class is adopted for storing the list of
       * permuted mapping support points, the size of its second dimension is
       * constant, which prescribes that all cells in the mesh adopt a same
       * mapping object.
       *
       * TODO When different mapping objects are adopted for different
       * subdomains, this should be modified accordingly.}
       */
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_buffer_gpu;

      /**
       * List of permuted mapping support points in \f$K_y\f$, which are stored
       * on the device.
       *
       * 1. First dimension: task index in the buffer.
       * 2. Second dimension: point index in the permuted mapping support points
       * in \f$K_y\f$.
       *
       * \mynote{Because @p CUDATable class is adopted for storing the list of
       * permuted mapping support points, the size of its second dimension is
       * constant, which prescribes that all cells in the mesh adopt a same
       * mapping object.
       *
       * TODO When different mapping objects are adopted for different
       * subdomains, this should be modified accordingly.}
       */
      CUDATable<2, Point<spacedim, RangeNumberType>>
        ky_mapping_support_points_permuted_buffer_gpu;

      /**
       * Array of local indices for accessing global DoF indices in \f$K_x\f$,
       * which are stored on the host.
       */
      unsigned int *i_local_index_buffer_cpu;

      /**
       * Array of local indices for accessing global DoF indices in \f$K_y\f$,
       * which are stored on the host.
       */
      unsigned int *j_local_index_buffer_cpu;

      /**
       * Array of local DoF indices in \f$K_x\f$ corresponding to the global DoF
       * index \f$i\f$.
       *
       * \mynote{Because the local index associated with the global DoF @p i
       * will be used during the evaluation Sauter quadrature, this array is
       * allocated on the device.}
       */
      unsigned int *i_local_index_buffer_gpu;

      /**
       * Array of local DoF indices in \f$K_y\f$ corresponding to the global DoF
       * index \f$j\f$.
       *
       * \mynote{Because the local index associated with the global DoF @p j
       * will be used during the evaluation Sauter quadrature, this array is
       * allocated on the device.}
       */
      unsigned int *j_local_index_buffer_gpu;

      /**
       * Values at all quadrature points on the device, which need to be
       * accumulated into the array @p vector_element_values_gpu, then copied
       * into the row or column vector in the \hmat.
       *
       * The number of elements in this array is @p capacity*quad_point_num.
       */
      RangeNumberType *values_at_all_quad_points_gpu;

      /**
       * Vector element values on the device. The number of elements in this
       * array is @p vector_size.
       */
      RangeNumberType *vector_element_values_gpu;
    };


    /**
     * Perform Sauter quadrature for a pair of DoFs with associated pair of
     * cells \f$K_x\f$ and \f$K_y\f$, which have the same panel neighboring
     * type.
     *
     * \mynote{There can be several pairs of cells contributing to the full
     * matrix entries associated with the pair of DoFs, because the
     * support of each DoF usually contains more than one cells.}
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    process_sauter_task_on_one_pair_of_dofs_same_panel(
      const KernelFunctionType<spacedim, RangeNumberType> kernel,
      const RangeNumberType                               kernel_factor,
      const unsigned int                                  capacity,
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_ring_buffer,
      CUDATable<2, Point<spacedim, RangeNumberType>>
                         ky_mapping_support_points_permuted_ring_buffer,
      unsigned int      *i_local_index_ring_buffer,
      unsigned int      *j_local_index_ring_buffer,
      RangeNumberType   *quadrature_value_ring_buffer_gpu,
      const unsigned int start_task_index,
      const unsigned int fetched_task_num,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values)
    {
      /**
       * @internal For simplicity, we use 1D linear thread grid.
       */
      const unsigned int thread_idx = blockIdx.x * blockDim.x + threadIdx.x;

      RangeNumberType                      kx_jacobian, ky_jacobian;
      Tensor<1, spacedim, RangeNumberType> kx_normal, ky_normal;
      Point<spacedim, RangeNumberType>     kx_quad_point, ky_quad_point;
      /**
       * @internal Create a wrapper matrix for the covariant matrix
       * on \f$K_x\f$. N.B. Only when the kernel function is
       * @p HyperSingularRegular, this matrix will be evaluated.
       */
      RangeNumberType                 kx_covariant_matrix_data[spacedim * dim];
      RangeNumberType                 ky_covariant_matrix_data[spacedim * dim];
      CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
        kx_covariant_matrix_data, spacedim, dim);
      CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
        ky_covariant_matrix_data, spacedim, dim);

      /**
       * @internal Get the list of quadrature points in the Sauter
       * parametric space.
       */
      const CUDATable<2, double> &quad_points =
        bem_values.quad_rule_for_same_panel.get_points();

      /**
       * @internal The number of allocated CUDA threads for executing this
       * kernel function is always enough to hold the fetched tasks. Hence,
       * those threads with linear IDs less than @p fetched_task_num actually
       * perform the computation.
       */
      if (thread_idx < fetched_task_num)
        {
          /**
           * @internal Calculate the task index in the ring buffer.
           */
          const unsigned int task_index =
            (thread_idx + start_task_index) >= capacity ?
              (thread_idx + start_task_index - capacity) :
              (thread_idx + start_task_index);

          /**
           * @internal Initialize the quadrature value stored in the register,
           * which is efficient for accumulating values contributed from all
           * \f$k_3\f$ terms and quadrature points.
           */
          RangeNumberType quad_value = 0.;

          /**
           * @internal Iterate over each quadrature point.
           */
          for (unsigned int quad_no = 0;
               quad_no < bem_values.quad_rule_for_same_panel.size();
               quad_no++)
            {
              /**
               * @internal Initialize the register variable for storing the
               * kernel function value at the current quadrature point, which is
               * also multiplied by Jacobians from unit cells to real cells as
               * well as shape function values in \f$K_x\f$ and \f$K_y\f$.
               *
               * \mynote{When the kernel function is @p HyperSingularRegular,
               * the scalar product of surface curls in \f$K_x\f$ and \f$K_y\f$
               * will be multiplied to @p kernel_value instead of the shape
               * function values.}
               */
              RangeNumberType kernel_value = 0.;
              /**
               * @internal Iterate over each k3 term.
               */
              for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
                {
                  /**
                   * @internal Calculate Jacobians, normal vectors, covariant
                   * matrices and quadrature point coordinates in the real cell
                   * for the current quadrature point in \f$K_x\f$.
                   */
                  kx_jacobian = BEMTools::CUDAWrappers::
                    surface_jacobian_det_normal_vector_and_covariant(
                      k3_index,
                      quad_no,
                      bem_values
                        .kx_mapping_shape_grad_matrix_table_for_same_panel,
                      &kx_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      kx_normal,
                      kx_covariant_matrix,
                      false,
                      kernel.kernel_type == KernelType::HyperSingularRegular);

                  BEMTools::CUDAWrappers::
                    transform_quad_point_from_unit_to_permuted_real_cell(
                      k3_index,
                      quad_no,
                      bem_values.kx_mapping_shape_value_table_for_same_panel,
                      &kx_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      kx_quad_point);

                  /**
                   * @internal Calculate Jacobians, normal vectors, covariant
                   * matrices and quadrature point coordinates in the real cell
                   * for the current quadrature point in \f$K_y\f$.
                   */
                  ky_jacobian = BEMTools::CUDAWrappers::
                    surface_jacobian_det_normal_vector_and_covariant(
                      k3_index,
                      quad_no,
                      bem_values
                        .ky_mapping_shape_grad_matrix_table_for_same_panel,
                      &ky_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      ky_normal,
                      ky_covariant_matrix,
                      false,
                      kernel.kernel_type == KernelType::HyperSingularRegular);

                  BEMTools::CUDAWrappers::
                    transform_quad_point_from_unit_to_permuted_real_cell(
                      k3_index,
                      quad_no,
                      bem_values.ky_mapping_shape_value_table_for_same_panel,
                      &ky_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      ky_quad_point);

                  if (kernel.kernel_type == HyperSingularRegular)
                    {
                      /**
                       * @internal Extract the gradient values of the current
                       * shape function at the current quadrature point in the
                       * unit cells for \f$K_x\f$ and \f$K_y\f$.
                       */
                      RangeNumberType kx_shape_grad_in_unit_cell[dim];
                      RangeNumberType ky_shape_grad_in_unit_cell[dim];
                      CUDAVector<RangeNumberType>
                        kx_shape_grad_in_unit_cell_vector(
                          kx_shape_grad_in_unit_cell, dim);
                      CUDAVector<RangeNumberType>
                        ky_shape_grad_in_unit_cell_vector(
                          ky_shape_grad_in_unit_cell, dim);

                      for (unsigned int d = 0; d < dim; d++)
                        {
                          kx_shape_grad_in_unit_cell[d] =
                            bem_values
                              .kx_shape_grad_matrix_table_for_same_panel(
                                k3_index,
                                quad_no,
                                d,
                                i_local_index_ring_buffer[task_index]);
                          ky_shape_grad_in_unit_cell[d] =
                            bem_values
                              .ky_shape_grad_matrix_table_for_same_panel(
                                k3_index,
                                quad_no,
                                d,
                                j_local_index_ring_buffer[task_index]);
                        }

                      /**
                       * Apply covariant transformation \f$JG^{-1}\f$ to the
                       * gradient tensors in the unit cells, then we get the
                       * gradient tensors in the real cells.
                       */
                      RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                      RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                      CUDAVector<RangeNumberType>
                        kx_shape_grad_in_real_cell_vector(
                          kx_shape_grad_in_real_cell, spacedim);
                      CUDAVector<RangeNumberType>
                        ky_shape_grad_in_real_cell_vector(
                          ky_shape_grad_in_real_cell, spacedim);

                      kx_covariant_matrix.vmult(
                        kx_shape_grad_in_real_cell_vector,
                        kx_shape_grad_in_unit_cell_vector);
                      ky_covariant_matrix.vmult(
                        ky_shape_grad_in_real_cell_vector,
                        ky_shape_grad_in_unit_cell_vector);

                      /**
                       * Calculate the surface gradient tensor of the shape
                       * functions, which is the cross product of normal vector
                       * and the volume gradient vector.
                       *
                       * \mynote{The cross product operation requires the input
                       * vectors to be transformed to tensors.}
                       */
                      Tensor<1, spacedim, RangeNumberType>
                        kx_shape_surface_curl =
                          cross_product_3d(kx_normal,
                                           kx_shape_grad_in_real_cell_vector);
                      Tensor<1, spacedim, RangeNumberType>
                        ky_shape_surface_curl =
                          cross_product_3d(ky_normal,
                                           ky_shape_grad_in_real_cell_vector);

                      kernel_value += kernel.value(kx_quad_point,
                                                   ky_quad_point,
                                                   kx_normal,
                                                   ky_normal) *
                                      kx_jacobian * ky_jacobian *
                                      scalar_product(kx_shape_surface_curl,
                                                     ky_shape_surface_curl);
                    }
                  else
                    {
                      kernel_value +=
                        kernel.value(kx_quad_point,
                                     ky_quad_point,
                                     kx_normal,
                                     ky_normal) *
                        kx_jacobian * ky_jacobian *
                        bem_values.kx_shape_value_table_for_same_panel(
                          i_local_index_ring_buffer[task_index],
                          k3_index,
                          quad_no) *
                        bem_values.ky_shape_value_table_for_same_panel(
                          j_local_index_ring_buffer[task_index],
                          k3_index,
                          quad_no);
                    }
                }

              quad_value +=
                kernel_value * quad_points(quad_no, 0) *
                (1 - quad_points(quad_no, 0)) *
                (1 - quad_points(quad_no, 0) * quad_points(quad_no, 1)) *
                bem_values.quad_rule_for_same_panel.get_weights()[quad_no];
            }

          /**
           * @internal Save the scaled quadrature value to the ring buffer.
           */
          quadrature_value_ring_buffer_gpu[task_index] =
            quad_value * kernel_factor;
        }
    }


    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    process_sauter_task_on_one_pair_of_dofs_common_edge(
      const KernelFunctionType<spacedim, RangeNumberType> kernel,
      const RangeNumberType                               kernel_factor,
      const unsigned int                                  capacity,
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_ring_buffer,
      CUDATable<2, Point<spacedim, RangeNumberType>>
                         ky_mapping_support_points_permuted_ring_buffer,
      unsigned int      *i_local_index_ring_buffer,
      unsigned int      *j_local_index_ring_buffer,
      RangeNumberType   *quadrature_value_ring_buffer_gpu,
      const unsigned int start_task_index,
      const unsigned int fetched_task_num,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values)
    {
      /**
       * @internal For simplicity, we use 1D linear thread grid.
       */
      const unsigned int thread_idx = blockIdx.x * blockDim.x + threadIdx.x;

      RangeNumberType                      kx_jacobian, ky_jacobian;
      Tensor<1, spacedim, RangeNumberType> kx_normal, ky_normal;
      Point<spacedim, RangeNumberType>     kx_quad_point, ky_quad_point;
      /**
       * @internal Create a wrapper matrix for the covariant matrix
       * on \f$K_x\f$. N.B. Only when the kernel function is
       * @p HyperSingularRegular, this matrix will be evaluated.
       */
      RangeNumberType                 kx_covariant_matrix_data[spacedim * dim];
      RangeNumberType                 ky_covariant_matrix_data[spacedim * dim];
      CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
        kx_covariant_matrix_data, spacedim, dim);
      CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
        ky_covariant_matrix_data, spacedim, dim);

      /**
       * @internal Get the list of quadrature points in the Sauter
       * parametric space.
       */
      const CUDATable<2, double> &quad_points =
        bem_values.quad_rule_for_common_edge.get_points();

      /**
       * @internal The number of allocated CUDA threads for executing this
       * kernel function is always enough to hold the fetched tasks. Hence,
       * those threads with linear IDs less than @p fetched_task_num actually
       * perform the computation.
       */
      if (thread_idx < fetched_task_num)
        {
          /**
           * @internal Calculate the task index in the ring buffer.
           */
          const unsigned int task_index =
            (thread_idx + start_task_index) >= capacity ?
              (thread_idx + start_task_index - capacity) :
              (thread_idx + start_task_index);

          /**
           * @internal Initialize the quadrature value stored in the register,
           * which is efficient for accumulating values contributed from all
           * \f$k_3\f$ terms and quadrature points.
           */
          RangeNumberType quad_value = 0.;

          /**
           * @internal Iterate over each quadrature point.
           */
          for (unsigned int quad_no = 0;
               quad_no < bem_values.quad_rule_for_common_edge.size();
               quad_no++)
            {
              RangeNumberType jacobian_det1 = quad_points(quad_no, 0) *
                                              quad_points(quad_no, 0) *
                                              (1 - quad_points(quad_no, 0));
              RangeNumberType jacobian_det2 =
                quad_points(quad_no, 0) * quad_points(quad_no, 0) *
                (1 - quad_points(quad_no, 0) * quad_points(quad_no, 1));

              /**
               * @internal Initialize the register variable for storing the
               * kernel function value at the current quadrature point, which is
               * also multiplied by Jacobians from unit cells to real cells as
               * well as shape function values in \f$K_x\f$ and \f$K_y\f$.
               *
               * \mynote{When the kernel function is @p HyperSingularRegular,
               * the scalar product of surface curls in \f$K_x\f$ and \f$K_y\f$
               * will be multiplied to @p kernel_value instead of the shape
               * function values.}
               */
              RangeNumberType kernel_value = 0.;
              /**
               * @internal Iterate over each k3 term.
               */
              for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
                {
                  /**
                   * @internal Calculate Jacobians, normal vectors, covariant
                   * matrices and quadrature point coordinates in the real cell
                   * for the current quadrature point in \f$K_x\f$.
                   */
                  kx_jacobian = BEMTools::CUDAWrappers::
                    surface_jacobian_det_normal_vector_and_covariant(
                      k3_index,
                      quad_no,
                      bem_values
                        .kx_mapping_shape_grad_matrix_table_for_common_edge,
                      &kx_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      kx_normal,
                      kx_covariant_matrix,
                      false,
                      kernel.kernel_type == KernelType::HyperSingularRegular);

                  BEMTools::CUDAWrappers::
                    transform_quad_point_from_unit_to_permuted_real_cell(
                      k3_index,
                      quad_no,
                      bem_values.kx_mapping_shape_value_table_for_common_edge,
                      &kx_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      kx_quad_point);

                  /**
                   * @internal Calculate Jacobians, normal vectors, covariant
                   * matrices and quadrature point coordinates in the real cell
                   * for the current quadrature point in \f$K_y\f$.
                   */
                  ky_jacobian = BEMTools::CUDAWrappers::
                    surface_jacobian_det_normal_vector_and_covariant(
                      k3_index,
                      quad_no,
                      bem_values
                        .ky_mapping_shape_grad_matrix_table_for_common_edge,
                      &ky_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      ky_normal,
                      ky_covariant_matrix,
                      false,
                      kernel.kernel_type == KernelType::HyperSingularRegular);

                  BEMTools::CUDAWrappers::
                    transform_quad_point_from_unit_to_permuted_real_cell(
                      k3_index,
                      quad_no,
                      bem_values.ky_mapping_shape_value_table_for_common_edge,
                      &ky_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      ky_quad_point);

                  if (kernel.kernel_type == HyperSingularRegular)
                    {
                      /**
                       * @internal Extract the gradient values of the current
                       * shape function at the current quadrature point in the
                       * unit cells for \f$K_x\f$ and \f$K_y\f$.
                       */
                      RangeNumberType kx_shape_grad_in_unit_cell[dim];
                      RangeNumberType ky_shape_grad_in_unit_cell[dim];
                      CUDAVector<RangeNumberType>
                        kx_shape_grad_in_unit_cell_vector(
                          kx_shape_grad_in_unit_cell, dim);
                      CUDAVector<RangeNumberType>
                        ky_shape_grad_in_unit_cell_vector(
                          ky_shape_grad_in_unit_cell, dim);

                      for (unsigned int d = 0; d < dim; d++)
                        {
                          kx_shape_grad_in_unit_cell[d] =
                            bem_values
                              .kx_shape_grad_matrix_table_for_common_edge(
                                k3_index,
                                quad_no,
                                d,
                                i_local_index_ring_buffer[task_index]);
                          ky_shape_grad_in_unit_cell[d] =
                            bem_values
                              .ky_shape_grad_matrix_table_for_common_edge(
                                k3_index,
                                quad_no,
                                d,
                                j_local_index_ring_buffer[task_index]);
                        }

                      /**
                       * Apply covariant transformation \f$JG^{-1}\f$ to the
                       * gradient tensors in the unit cells, then we get the
                       * gradient tensors in the real cells.
                       */
                      RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                      RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                      CUDAVector<RangeNumberType>
                        kx_shape_grad_in_real_cell_vector(
                          kx_shape_grad_in_real_cell, spacedim);
                      CUDAVector<RangeNumberType>
                        ky_shape_grad_in_real_cell_vector(
                          ky_shape_grad_in_real_cell, spacedim);

                      kx_covariant_matrix.vmult(
                        kx_shape_grad_in_real_cell_vector,
                        kx_shape_grad_in_unit_cell_vector);
                      ky_covariant_matrix.vmult(
                        ky_shape_grad_in_real_cell_vector,
                        ky_shape_grad_in_unit_cell_vector);

                      /**
                       * Calculate the surface gradient tensor of the shape
                       * functions, which is the cross product of normal vector
                       * and the volume gradient vector.
                       *
                       * \mynote{The cross product operation requires the input
                       * vectors to be transformed to tensors.
                       *
                       * When in common edge case, the normal vector in
                       * \f$K_y\f$ should be negated.}
                       */
                      Tensor<1, spacedim, RangeNumberType>
                        kx_shape_surface_curl =
                          cross_product_3d(kx_normal,
                                           kx_shape_grad_in_real_cell_vector);
                      Tensor<1, spacedim, RangeNumberType>
                        ky_shape_surface_curl =
                          cross_product_3d(-ky_normal,
                                           ky_shape_grad_in_real_cell_vector);

                      kernel_value +=
                        kernel.value(kx_quad_point,
                                     ky_quad_point,
                                     kx_normal,
                                     -ky_normal) *
                        kx_jacobian * ky_jacobian *
                        scalar_product(kx_shape_surface_curl,
                                       ky_shape_surface_curl) *
                        (k3_index <= 1 ? jacobian_det1 : jacobian_det2);
                    }
                  else
                    {
                      kernel_value +=
                        kernel.value(kx_quad_point,
                                     ky_quad_point,
                                     kx_normal,
                                     -ky_normal) *
                        kx_jacobian * ky_jacobian *
                        bem_values.kx_shape_value_table_for_common_edge(
                          i_local_index_ring_buffer[task_index],
                          k3_index,
                          quad_no) *
                        bem_values.ky_shape_value_table_for_common_edge(
                          j_local_index_ring_buffer[task_index],
                          k3_index,
                          quad_no) *
                        (k3_index <= 1 ? jacobian_det1 : jacobian_det2);
                    }
                }

              quad_value +=
                kernel_value *
                bem_values.quad_rule_for_common_edge.get_weights()[quad_no];
            }

          /**
           * @internal Save the scaled quadrature value to the ring buffer.
           */
          quadrature_value_ring_buffer_gpu[task_index] =
            quad_value * kernel_factor;
        }
    }


    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    process_sauter_task_on_one_pair_of_dofs_common_vertex(
      const KernelFunctionType<spacedim, RangeNumberType> kernel,
      const RangeNumberType                               kernel_factor,
      const unsigned int                                  capacity,
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_ring_buffer,
      CUDATable<2, Point<spacedim, RangeNumberType>>
                         ky_mapping_support_points_permuted_ring_buffer,
      unsigned int      *i_local_index_ring_buffer,
      unsigned int      *j_local_index_ring_buffer,
      RangeNumberType   *quadrature_value_ring_buffer_gpu,
      const unsigned int start_task_index,
      const unsigned int fetched_task_num,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values)
    {
      /**
       * @internal For simplicity, we use 1D linear thread grid.
       */
      const unsigned int thread_idx = blockIdx.x * blockDim.x + threadIdx.x;

      RangeNumberType                      kx_jacobian, ky_jacobian;
      Tensor<1, spacedim, RangeNumberType> kx_normal, ky_normal;
      Point<spacedim, RangeNumberType>     kx_quad_point, ky_quad_point;
      /**
       * @internal Create a wrapper matrix for the covariant matrix
       * on \f$K_x\f$. N.B. Only when the kernel function is
       * @p HyperSingularRegular, this matrix will be evaluated.
       */
      RangeNumberType                 kx_covariant_matrix_data[spacedim * dim];
      RangeNumberType                 ky_covariant_matrix_data[spacedim * dim];
      CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
        kx_covariant_matrix_data, spacedim, dim);
      CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
        ky_covariant_matrix_data, spacedim, dim);

      /**
       * @internal Get the list of quadrature points in the Sauter
       * parametric space.
       */
      const CUDATable<2, double> &quad_points =
        bem_values.quad_rule_for_common_vertex.get_points();

      /**
       * @internal The number of allocated CUDA threads for executing this
       * kernel function is always enough to hold the fetched tasks. Hence,
       * those threads with linear IDs less than @p fetched_task_num actually
       * perform the computation.
       */
      if (thread_idx < fetched_task_num)
        {
          /**
           * @internal Calculate the task index in the ring buffer.
           */
          const unsigned int task_index =
            (thread_idx + start_task_index) >= capacity ?
              (thread_idx + start_task_index - capacity) :
              (thread_idx + start_task_index);

          /**
           * @internal Initialize the quadrature value stored in the register,
           * which is efficient for accumulating values contributed from all
           * \f$k_3\f$ terms and quadrature points.
           */
          RangeNumberType quad_value = 0.;

          /**
           * @internal Iterate over each quadrature point.
           */
          for (unsigned int quad_no = 0;
               quad_no < bem_values.quad_rule_for_common_vertex.size();
               quad_no++)
            {
              /**
               * @internal Initialize the register variable for storing the
               * kernel function value at the current quadrature point, which is
               * also multiplied by Jacobians from unit cells to real cells as
               * well as shape function values in \f$K_x\f$ and \f$K_y\f$.
               *
               * \mynote{When the kernel function is @p HyperSingularRegular,
               * the scalar product of surface curls in \f$K_x\f$ and \f$K_y\f$
               * will be multiplied to @p kernel_value instead of the shape
               * function values.}
               */
              RangeNumberType kernel_value = 0.;
              /**
               * @internal Iterate over each k3 term.
               */
              for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
                {
                  /**
                   * @internal Calculate Jacobians, normal vectors, covariant
                   * matrices and quadrature point coordinates in the real cell
                   * for the current quadrature point in \f$K_x\f$.
                   */
                  kx_jacobian = BEMTools::CUDAWrappers::
                    surface_jacobian_det_normal_vector_and_covariant(
                      k3_index,
                      quad_no,
                      bem_values
                        .kx_mapping_shape_grad_matrix_table_for_common_vertex,
                      &kx_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      kx_normal,
                      kx_covariant_matrix,
                      false,
                      kernel.kernel_type == KernelType::HyperSingularRegular);

                  BEMTools::CUDAWrappers::
                    transform_quad_point_from_unit_to_permuted_real_cell(
                      k3_index,
                      quad_no,
                      bem_values.kx_mapping_shape_value_table_for_common_vertex,
                      &kx_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      kx_quad_point);

                  /**
                   * @internal Calculate Jacobians, normal vectors, covariant
                   * matrices and quadrature point coordinates in the real cell
                   * for the current quadrature point in \f$K_y\f$.
                   */
                  ky_jacobian = BEMTools::CUDAWrappers::
                    surface_jacobian_det_normal_vector_and_covariant(
                      k3_index,
                      quad_no,
                      bem_values
                        .ky_mapping_shape_grad_matrix_table_for_common_vertex,
                      &ky_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      ky_normal,
                      ky_covariant_matrix,
                      false,
                      kernel.kernel_type == KernelType::HyperSingularRegular);

                  BEMTools::CUDAWrappers::
                    transform_quad_point_from_unit_to_permuted_real_cell(
                      k3_index,
                      quad_no,
                      bem_values.ky_mapping_shape_value_table_for_common_vertex,
                      &ky_mapping_support_points_permuted_ring_buffer(
                        task_index, 0),
                      ky_quad_point);

                  if (kernel.kernel_type == HyperSingularRegular)
                    {
                      /**
                       * @internal Extract the gradient values of the current
                       * shape function at the current quadrature point in the
                       * unit cells for \f$K_x\f$ and \f$K_y\f$.
                       */
                      RangeNumberType kx_shape_grad_in_unit_cell[dim];
                      RangeNumberType ky_shape_grad_in_unit_cell[dim];
                      CUDAVector<RangeNumberType>
                        kx_shape_grad_in_unit_cell_vector(
                          kx_shape_grad_in_unit_cell, dim);
                      CUDAVector<RangeNumberType>
                        ky_shape_grad_in_unit_cell_vector(
                          ky_shape_grad_in_unit_cell, dim);

                      for (unsigned int d = 0; d < dim; d++)
                        {
                          kx_shape_grad_in_unit_cell[d] =
                            bem_values
                              .kx_shape_grad_matrix_table_for_common_vertex(
                                k3_index,
                                quad_no,
                                d,
                                i_local_index_ring_buffer[task_index]);
                          ky_shape_grad_in_unit_cell[d] =
                            bem_values
                              .ky_shape_grad_matrix_table_for_common_vertex(
                                k3_index,
                                quad_no,
                                d,
                                j_local_index_ring_buffer[task_index]);
                        }

                      /**
                       * Apply covariant transformation \f$JG^{-1}\f$ to the
                       * gradient tensors in the unit cells, then we get the
                       * gradient tensors in the real cells.
                       */
                      RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                      RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                      CUDAVector<RangeNumberType>
                        kx_shape_grad_in_real_cell_vector(
                          kx_shape_grad_in_real_cell, spacedim);
                      CUDAVector<RangeNumberType>
                        ky_shape_grad_in_real_cell_vector(
                          ky_shape_grad_in_real_cell, spacedim);

                      kx_covariant_matrix.vmult(
                        kx_shape_grad_in_real_cell_vector,
                        kx_shape_grad_in_unit_cell_vector);
                      ky_covariant_matrix.vmult(
                        ky_shape_grad_in_real_cell_vector,
                        ky_shape_grad_in_unit_cell_vector);

                      /**
                       * Calculate the surface gradient tensor of the shape
                       * functions, which is the cross product of normal vector
                       * and the volume gradient vector.
                       *
                       * \mynote{The cross product operation requires the input
                       * vectors to be transformed to tensors.}
                       */
                      Tensor<1, spacedim, RangeNumberType>
                        kx_shape_surface_curl =
                          cross_product_3d(kx_normal,
                                           kx_shape_grad_in_real_cell_vector);
                      Tensor<1, spacedim, RangeNumberType>
                        ky_shape_surface_curl =
                          cross_product_3d(ky_normal,
                                           ky_shape_grad_in_real_cell_vector);

                      kernel_value += kernel.value(kx_quad_point,
                                                   ky_quad_point,
                                                   kx_normal,
                                                   ky_normal) *
                                      kx_jacobian * ky_jacobian *
                                      scalar_product(kx_shape_surface_curl,
                                                     ky_shape_surface_curl);
                    }
                  else
                    {
                      kernel_value +=
                        kernel.value(kx_quad_point,
                                     ky_quad_point,
                                     kx_normal,
                                     ky_normal) *
                        kx_jacobian * ky_jacobian *
                        bem_values.kx_shape_value_table_for_common_vertex(
                          i_local_index_ring_buffer[task_index],
                          k3_index,
                          quad_no) *
                        bem_values.ky_shape_value_table_for_common_vertex(
                          j_local_index_ring_buffer[task_index],
                          k3_index,
                          quad_no);
                    }
                }

              quad_value +=
                kernel_value *
                IdeoBEM::Utilities::CUDAWrappers::fixed_power<3>(
                  quad_points(quad_no, 0)) *
                bem_values.quad_rule_for_common_vertex.get_weights()[quad_no];
            }

          /**
           * @internal Save the scaled quadrature value to the ring buffer.
           */
          quadrature_value_ring_buffer_gpu[task_index] =
            quad_value * kernel_factor;
        }
    }


    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    process_sauter_task_on_one_pair_of_dofs_regular(
      const KernelFunctionType<spacedim, RangeNumberType> kernel,
      const RangeNumberType                               kernel_factor,
      const unsigned int                                  capacity,
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_ring_buffer,
      CUDATable<2, Point<spacedim, RangeNumberType>>
                         ky_mapping_support_points_permuted_ring_buffer,
      unsigned int      *i_local_index_ring_buffer,
      unsigned int      *j_local_index_ring_buffer,
      RangeNumberType   *quadrature_value_ring_buffer_gpu,
      const unsigned int start_task_index,
      const unsigned int fetched_task_num,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values)
    {
      /**
       * @internal For simplicity, we use 1D linear thread grid.
       */
      const unsigned int thread_idx = blockIdx.x * blockDim.x + threadIdx.x;

      RangeNumberType                      kx_jacobian, ky_jacobian;
      Tensor<1, spacedim, RangeNumberType> kx_normal, ky_normal;
      Point<spacedim, RangeNumberType>     kx_quad_point, ky_quad_point;
      /**
       * @internal Create a wrapper matrix for the covariant matrix
       * on \f$K_x\f$. N.B. Only when the kernel function is
       * @p HyperSingularRegular, this matrix will be evaluated.
       */
      RangeNumberType                 kx_covariant_matrix_data[spacedim * dim];
      RangeNumberType                 ky_covariant_matrix_data[spacedim * dim];
      CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
        kx_covariant_matrix_data, spacedim, dim);
      CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
        ky_covariant_matrix_data, spacedim, dim);

      /**
       * @internal The number of allocated CUDA threads for executing this
       * kernel function is always enough to hold the fetched tasks. Hence,
       * those threads with linear IDs less than @p fetched_task_num actually
       * perform the computation.
       */
      if (thread_idx < fetched_task_num)
        {
          /**
           * @internal Calculate the task index in the ring buffer.
           */
          const unsigned int task_index =
            (thread_idx + start_task_index) >= capacity ?
              (thread_idx + start_task_index - capacity) :
              (thread_idx + start_task_index);

          /**
           * @internal Initialize the quadrature value stored in the register,
           * which is efficient for accumulating values contributed from all
           * \f$k_3\f$ terms and quadrature points.
           */
          RangeNumberType quad_value = 0.;

          /**
           * @internal Iterate over each quadrature point.
           */
          for (unsigned int quad_no = 0;
               quad_no < bem_values.quad_rule_for_regular.size();
               quad_no++)
            {
              /**
               * @internal Calculate Jacobians, normal vectors, covariant
               * matrices and quadrature point coordinates in the real cell for
               * the current quadrature point in \f$K_x\f$.
               */
              kx_jacobian = BEMTools::CUDAWrappers::
                surface_jacobian_det_normal_vector_and_covariant(
                  0,
                  quad_no,
                  bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
                  &kx_mapping_support_points_permuted_ring_buffer(task_index,
                                                                  0),
                  kx_normal,
                  kx_covariant_matrix,
                  false,
                  kernel.kernel_type == KernelType::HyperSingularRegular);

              BEMTools::CUDAWrappers::
                transform_quad_point_from_unit_to_permuted_real_cell(
                  0,
                  quad_no,
                  bem_values.kx_mapping_shape_value_table_for_regular,
                  &kx_mapping_support_points_permuted_ring_buffer(task_index,
                                                                  0),
                  kx_quad_point);

              /**
               * @internal Calculate Jacobians, normal vectors, covariant
               * matrices and quadrature point coordinates in the real cell for
               * the current quadrature point in \f$K_y\f$.
               */
              ky_jacobian = BEMTools::CUDAWrappers::
                surface_jacobian_det_normal_vector_and_covariant(
                  0,
                  quad_no,
                  bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
                  &ky_mapping_support_points_permuted_ring_buffer(task_index,
                                                                  0),
                  ky_normal,
                  ky_covariant_matrix,
                  false,
                  kernel.kernel_type == KernelType::HyperSingularRegular);

              BEMTools::CUDAWrappers::
                transform_quad_point_from_unit_to_permuted_real_cell(
                  0,
                  quad_no,
                  bem_values.ky_mapping_shape_value_table_for_regular,
                  &ky_mapping_support_points_permuted_ring_buffer(task_index,
                                                                  0),
                  ky_quad_point);

              if (kernel.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current
                   * shape function at the current quadrature point in the
                   * unit cells for \f$K_x\f$ and \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

                  for (unsigned int d = 0; d < dim; d++)
                    {
                      kx_shape_grad_in_unit_cell[d] =
                        bem_values.kx_shape_grad_matrix_table_for_regular(
                          0, quad_no, d, i_local_index_ring_buffer[task_index]);
                      ky_shape_grad_in_unit_cell[d] =
                        bem_values.ky_shape_grad_matrix_table_for_regular(
                          0, quad_no, d, j_local_index_ring_buffer[task_index]);
                    }

                  /**
                   * Apply covariant transformation \f$JG^{-1}\f$ to the
                   * gradient tensors in the unit cells, then we get the
                   * gradient tensors in the real cells.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector
                   * and the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors to be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(kx_normal,
                                     kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(ky_normal,
                                     ky_shape_grad_in_real_cell_vector);

                  quad_value +=
                    kernel.value(kx_quad_point,
                                 ky_quad_point,
                                 kx_normal,
                                 ky_normal) *
                    kx_jacobian * ky_jacobian *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl) *
                    bem_values.quad_rule_for_regular.get_weights()[quad_no];
                }
              else
                {
                  quad_value +=
                    kernel.value(kx_quad_point,
                                 ky_quad_point,
                                 kx_normal,
                                 ky_normal) *
                    kx_jacobian * ky_jacobian *
                    bem_values.kx_shape_value_table_for_regular(
                      i_local_index_ring_buffer[task_index], 0, quad_no) *
                    bem_values.ky_shape_value_table_for_regular(
                      j_local_index_ring_buffer[task_index], 0, quad_no) *
                    bem_values.quad_rule_for_regular.get_weights()[quad_no];
                }
            }

          /**
           * @internal Save the scaled quadrature value to the ring buffer.
           */
          quadrature_value_ring_buffer_gpu[task_index] =
            quad_value * kernel_factor;
        }
    }


    /**
     * Process all Sauter quadrature tasks in the buffer which is used for
     * assembling a row or column vector in a far field \hmat.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    process_sauter_tasks_in_buffer_for_far_field_hmat_vector(
      const KernelFunctionType<spacedim, RangeNumberType> kernel,
      const RangeNumberType                               kernel_factor,
      CUDATable<2, Point<spacedim, RangeNumberType>>
        kx_mapping_support_points_permuted_buffer,
      CUDATable<2, Point<spacedim, RangeNumberType>>
                         ky_mapping_support_points_permuted_buffer,
      unsigned int      *i_local_index_buffer,
      unsigned int      *j_local_index_buffer,
      RangeNumberType   *values_at_all_quad_points_gpu,
      const unsigned int task_num,
      const unsigned int quad_point_num,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values)
    {
      /**
       * @internal We use a 2D thread block, the first dimension represents task
       * index, while the second dimension represents quadrature point index.
       */
      const unsigned int task_index = blockIdx.x * blockDim.x + threadIdx.x;
      const unsigned int quad_no    = blockIdx.y * blockDim.y + threadIdx.y;

      RangeNumberType                      kx_jacobian, ky_jacobian;
      Tensor<1, spacedim, RangeNumberType> kx_normal, ky_normal;
      Point<spacedim, RangeNumberType>     kx_quad_point, ky_quad_point;
      /**
       * @internal Create a wrapper matrix for the covariant matrix
       * on \f$K_x\f$. N.B. Only when the kernel function is
       * @p HyperSingularRegular, this matrix will be evaluated.
       */
      RangeNumberType                 kx_covariant_matrix_data[spacedim * dim];
      RangeNumberType                 ky_covariant_matrix_data[spacedim * dim];
      CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
        kx_covariant_matrix_data, spacedim, dim);
      CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
        ky_covariant_matrix_data, spacedim, dim);

      if (task_index < task_num && quad_no < quad_point_num)
        {
          /**
           * @internal Initialize the quadrature value stored in the register,
           * which is efficient for accumulating values contributed from all
           * \f$k_3\f$ terms and quadrature points.
           */
          RangeNumberType quad_value;

          /**
           * @internal Calculate Jacobians, normal vectors, covariant
           * matrices and quadrature point coordinates in the real cell for
           * the current quadrature point in \f$K_x\f$.
           */
          kx_jacobian = BEMTools::CUDAWrappers::
            surface_jacobian_det_normal_vector_and_covariant(
              0,
              quad_no,
              bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
              &kx_mapping_support_points_permuted_buffer(task_index, 0),
              kx_normal,
              kx_covariant_matrix,
              false,
              kernel.kernel_type == KernelType::HyperSingularRegular);

          BEMTools::CUDAWrappers::
            transform_quad_point_from_unit_to_permuted_real_cell(
              0,
              quad_no,
              bem_values.kx_mapping_shape_value_table_for_regular,
              &kx_mapping_support_points_permuted_buffer(task_index, 0),
              kx_quad_point);

          /**
           * @internal Calculate Jacobians, normal vectors, covariant
           * matrices and quadrature point coordinates in the real cell for
           * the current quadrature point in \f$K_y\f$.
           */
          ky_jacobian = BEMTools::CUDAWrappers::
            surface_jacobian_det_normal_vector_and_covariant(
              0,
              quad_no,
              bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
              &ky_mapping_support_points_permuted_buffer(task_index, 0),
              ky_normal,
              ky_covariant_matrix,
              false,
              kernel.kernel_type == KernelType::HyperSingularRegular);

          BEMTools::CUDAWrappers::
            transform_quad_point_from_unit_to_permuted_real_cell(
              0,
              quad_no,
              bem_values.ky_mapping_shape_value_table_for_regular,
              &ky_mapping_support_points_permuted_buffer(task_index, 0),
              ky_quad_point);

          if (kernel.kernel_type == HyperSingularRegular)
            {
              /**
               * @internal Extract the gradient values of the current
               * shape function at the current quadrature point in the
               * unit cells for \f$K_x\f$ and \f$K_y\f$.
               */
              RangeNumberType             kx_shape_grad_in_unit_cell[dim];
              RangeNumberType             ky_shape_grad_in_unit_cell[dim];
              CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                kx_shape_grad_in_unit_cell, dim);
              CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                ky_shape_grad_in_unit_cell, dim);

              for (unsigned int d = 0; d < dim; d++)
                {
                  kx_shape_grad_in_unit_cell[d] =
                    bem_values.kx_shape_grad_matrix_table_for_regular(
                      0, quad_no, d, i_local_index_buffer[task_index]);
                  ky_shape_grad_in_unit_cell[d] =
                    bem_values.ky_shape_grad_matrix_table_for_regular(
                      0, quad_no, d, j_local_index_buffer[task_index]);
                }

              /**
               * Apply covariant transformation \f$JG^{-1}\f$ to the
               * gradient tensors in the unit cells, then we get the
               * gradient tensors in the real cells.
               */
              RangeNumberType             kx_shape_grad_in_real_cell[spacedim];
              RangeNumberType             ky_shape_grad_in_real_cell[spacedim];
              CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                kx_shape_grad_in_real_cell, spacedim);
              CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                ky_shape_grad_in_real_cell, spacedim);

              kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                        kx_shape_grad_in_unit_cell_vector);
              ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                        ky_shape_grad_in_unit_cell_vector);

              /**
               * Calculate the surface gradient tensor of the shape
               * functions, which is the cross product of normal vector
               * and the volume gradient vector.
               *
               * \mynote{The cross product operation requires the input
               * vectors to be transformed to tensors.}
               */
              Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                cross_product_3d(kx_normal, kx_shape_grad_in_real_cell_vector);
              Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                cross_product_3d(ky_normal, ky_shape_grad_in_real_cell_vector);

              quad_value =
                kernel.value(kx_quad_point,
                             ky_quad_point,
                             kx_normal,
                             ky_normal) *
                kx_jacobian * ky_jacobian *
                scalar_product(kx_shape_surface_curl, ky_shape_surface_curl) *
                bem_values.quad_rule_for_regular.get_weights()[quad_no];
            }
          else
            {
              quad_value =
                kernel.value(kx_quad_point,
                             ky_quad_point,
                             kx_normal,
                             ky_normal) *
                kx_jacobian * ky_jacobian *
                bem_values.kx_shape_value_table_for_regular(
                  i_local_index_buffer[task_index], 0, quad_no) *
                bem_values.ky_shape_value_table_for_regular(
                  j_local_index_buffer[task_index], 0, quad_no) *
                bem_values.quad_rule_for_regular.get_weights()[quad_no];
            }

          /**
           * @internal Save the scaled quadrature value to the result buffer.
           */
          values_at_all_quad_points_gpu[task_index * quad_point_num + quad_no] =
            quad_value * kernel_factor;
        }
    }


    /**
     * Accumulate quadrature values at all quadrature points for all tasks
     * associated with each vector element.
     *
     * @tparam RangeNumberType
     * @param vector_size
     * @param quad_point_num
     * @param starting_task_indices_for_dof_pairs_gpu
     * @param values_at_all_quad_points_gpu
     * @param vector_element_values_gpu
     */
    template <typename RangeNumberType = double>
    __global__ void
    accumulate_quadrature_values_into_vector(
      const unsigned int     vector_size,
      const unsigned int     quad_point_num,
      unsigned int          *starting_task_indices_for_dof_pairs_gpu,
      const RangeNumberType *values_at_all_quad_points_gpu,
      RangeNumberType       *vector_element_values_gpu)
    {
      /**
       * @internal Here we use 1D thread block and the thread index @p idx is
       * used as the element index in the vector.
       */
      const unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;

      if (idx < vector_size)
        {
          /**
           * @internal Accumulation of results into register variable.
           */
          RangeNumberType vector_element = 0.;

          for (unsigned int task_index =
                 starting_task_indices_for_dof_pairs_gpu[idx];
               task_index < starting_task_indices_for_dof_pairs_gpu[idx + 1];
               task_index++)
            {
              /**
               * @internal For each Sauter quadrature task, there are
               * @p quad_points_num number of values to be accumulated.
               */
              const unsigned int starting_index_in_result_vector =
                task_index * quad_point_num;
              for (unsigned int q = 0; q < quad_point_num; q++)
                {
                  vector_element += values_at_all_quad_points_gpu
                    [starting_index_in_result_vector + q];
                }
            }

          /**
           * @internal Copy the accumulated results from register to global
           * memory.
           */
          vector_element_values_gpu[idx] = vector_element;
        }
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ void
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      allocate(const CellNeighboringType       neighboring_type,
               const unsigned int              num_to_fetch,
               const BEMValues<dim, spacedim> &bem_values)
    {
      cell_neighboring_type                = neighboring_type;
      if_all_near_field_hmat_entries_added = false;
      head_pending                         = 0;
      head_committed                       = 0;
      tail_pending                         = 0;
      tail_committed                       = 0;
      batch_size                           = num_to_fetch;

      /**
       * @internal For each data table in @p PairCellWiseScratchData, its
       * counterpart in the ring buffer should have one more dimension which
       * corresponds to the task index.
       */
      cudaError_t error_code;

      kx_mapping_support_points_permuted_ring_buffer_cpu.reinit(
        TableIndices<2>(capacity,
                        bem_values.kx_mapping_data.n_shape_functions));
      error_code = cudaHostRegister(
        (void *)&(kx_mapping_support_points_permuted_ring_buffer_cpu(0, 0)),
        kx_mapping_support_points_permuted_ring_buffer_cpu.n_elements() *
          sizeof(Point<spacedim, RangeNumberType>),
        0);
      AssertCuda(error_code);

      ky_mapping_support_points_permuted_ring_buffer_cpu.reinit(
        TableIndices<2>(capacity,
                        bem_values.ky_mapping_data.n_shape_functions));
      error_code = cudaHostRegister(
        (void *)&(ky_mapping_support_points_permuted_ring_buffer_cpu(0, 0)),
        ky_mapping_support_points_permuted_ring_buffer_cpu.n_elements() *
          sizeof(Point<spacedim, RangeNumberType>),
        0);
      AssertCuda(error_code);

      kx_mapping_support_points_permuted_ring_buffer_gpu.allocate(
        TableIndices<2>(capacity,
                        bem_values.kx_mapping_data.n_shape_functions));
      ky_mapping_support_points_permuted_ring_buffer_gpu.allocate(
        TableIndices<2>(capacity,
                        bem_values.ky_mapping_data.n_shape_functions));

      error_code = cudaMallocHost((void **)&i_local_index_ring_buffer_cpu,
                                  capacity * sizeof(unsigned int));
      AssertCuda(error_code);
      error_code = cudaMallocHost((void **)&j_local_index_ring_buffer_cpu,
                                  capacity * sizeof(unsigned int));
      AssertCuda(error_code);

      error_code = cudaMalloc((void **)&i_local_index_ring_buffer_gpu,
                              capacity * sizeof(unsigned int));
      AssertCuda(error_code);
      error_code = cudaMalloc((void **)&j_local_index_ring_buffer_gpu,
                              capacity * sizeof(unsigned int));
      AssertCuda(error_code);

      /**
       * @internal Allocate pinned memory on the host for the quadrature values.
       */
      error_code = cudaMallocHost((void **)&quadrature_value_ring_buffer_cpu,
                                  capacity * sizeof(RangeNumberType));
      AssertCuda(error_code);

      error_code = cudaMalloc((void **)&quadrature_value_ring_buffer_gpu,
                              capacity * sizeof(RangeNumberType));
      AssertCuda(error_code);

      /**
       * @internal Initialize task status array.
       */
      for (unsigned int i = 0; i < capacity; i++)
        {
          std::atomic_init(task_status_ring_buffer + i,
                           SauterQuadratureTaskStatus::before_creation);
        }
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ void
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      release()
    {
      cudaError_t error_code;

      error_code = cudaHostUnregister(
        (void *)&(kx_mapping_support_points_permuted_ring_buffer_cpu(0, 0)));
      AssertCuda(error_code);

      error_code = cudaHostUnregister(
        (void *)&(ky_mapping_support_points_permuted_ring_buffer_cpu(0, 0)));
      AssertCuda(error_code);

      kx_mapping_support_points_permuted_ring_buffer_gpu.release();
      ky_mapping_support_points_permuted_ring_buffer_gpu.release();

      if (i_local_index_ring_buffer_cpu != nullptr)
        {
          error_code = cudaFreeHost(i_local_index_ring_buffer_cpu);
          AssertCuda(error_code);
        }

      if (j_local_index_ring_buffer_cpu != nullptr)
        {
          error_code = cudaFreeHost(j_local_index_ring_buffer_cpu);
          AssertCuda(error_code);
        }

      if (i_local_index_ring_buffer_gpu != nullptr)
        {
          error_code = cudaFree(i_local_index_ring_buffer_gpu);
          AssertCuda(error_code);
        }

      if (j_local_index_ring_buffer_gpu != nullptr)
        {
          error_code = cudaFree(j_local_index_ring_buffer_gpu);
          AssertCuda(error_code);
        }

      if (quadrature_value_ring_buffer_cpu != nullptr)
        {
          error_code = cudaFreeHost(quadrature_value_ring_buffer_cpu);
          AssertCuda(error_code);
        }

      if (quadrature_value_ring_buffer_gpu != nullptr)
        {
          error_code = cudaFree(quadrature_value_ring_buffer_gpu);
          AssertCuda(error_code);
        }
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline unsigned int
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      task_num_during_creation() const
    {
      /**
       * @internal Circular index should be used for computing.
       */
      return (tail_pending < tail_committed ? tail_pending + capacity :
                                              tail_pending) -
             tail_committed;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline unsigned int
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      task_num_ready_for_processing() const
    {
      return (tail_committed < head_pending ? tail_committed + capacity :
                                              tail_committed) -
             head_pending;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline unsigned int
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      task_num_during_processing() const
    {
      return (head_pending < head_committed ? head_pending + capacity :
                                              head_pending) -
             head_committed;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline unsigned int
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      task_num_total() const
    {
      return (tail_pending < head_committed ? tail_pending + capacity :
                                              tail_pending) -
             head_committed;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline bool
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      is_full() const
    {
      return ((tail_pending + 1) >= capacity ?
                (tail_pending + 1 - capacity) :
                (tail_pending + 1)) == head_committed;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline bool
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      is_not_full() const
    {
      return ((tail_pending + 1) >= capacity ?
                (tail_pending + 1 - capacity) :
                (tail_pending + 1)) != head_committed;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ inline bool
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      is_empty() const
    {
      return tail_pending == head_committed;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    __host__ void
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      add_task(LAPACKFullMatrixExt<RangeNumberType> *fullmat,
               const unsigned int                    fullmat_row_index,
               const unsigned int                    fullmat_col_index,
               const unsigned int                    i_local_index,
               const unsigned int                    j_local_index,
               const RangeNumberType                 mass_matrix_entry,
               const PairCellWiseScratchData<dim, spacedim, RangeNumberType>
                 &scratch_data)
    {
#if ENABLE_DEBUG == 1
      LogStream::Prefix prefix_string(
        std::string("add_task in ") +
        std::string(cell_neighboring_type_name(cell_neighboring_type)));
#endif
#if ENABLE_NVTX == 1
      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
        std::string("add_task for ") +
        std::string(cell_neighboring_type_name(cell_neighboring_type)));
#endif

      /**
       * @internal The index for the task to be added.
       */
      unsigned int task_index;

      {
#if ENABLE_NVTX == 1
        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range("wait for buffer_not_full");
#endif

        std::unique_lock<std::mutex> ul(buffer_lock);

        /**
         * @internal Wait until the number of tasks in the ring buffer is
         * smaller than its capacity minus one, i.e. there is empty space for
         * adding a new task.
         */
        buffer_not_full.wait(ul, [this]() { return this->is_not_full(); });
        task_index   = tail_pending;
        tail_pending = (tail_pending + 1) >= capacity ?
                         (tail_pending + 1 - capacity) :
                         (tail_pending + 1);

#if ENABLE_DEBUG == 1
        print_pointers(deallog);
#endif
      } // Release lock on the ring buffer

      {
#if ENABLE_NVTX == 1
        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range("creating task");
#endif

        /**
         * @internal Set the new task's status to @p during_creation using atomic
         * operation.
         */
        task_status_ring_buffer[task_index].store(
          SauterQuadratureTaskStatus::during_creation);

        /**
         * Create the new task:
         * 1. Copy the pointer to the current full matrix, matrix row and column
         * indices and mass matrix entry.
         * 2. Copy permuted mapping support points in \f$K_x\f$ into ring buffer
         * on the host.
         * 3. Copy permuted mapping support points in \f$K_y\f$ into ring buffer
         * on the host.
         * 4. Copy local DoF indices of the global DoF indices in the permuted
         * cell.
         */

        /**
         * @internal Generate task data on the host, among which the full matrix
         * pointer, row and column indices of the current matrix entry and mass
         * matrix entry will only be used during quadrature result assembly,
         * hence they are only stored on the host; for local DoF indices and
         * permuted mapping support points, they exist on both the host and
         * device.
         */
        fullmat_ring_buffer_cpu[task_index]             = fullmat;
        fullmat_row_index_ring_buffer_cpu[task_index]   = fullmat_row_index;
        fullmat_col_index_ring_buffer_cpu[task_index]   = fullmat_col_index;
        mass_matrix_entries_ring_buffer_cpu[task_index] = mass_matrix_entry;

        i_local_index_ring_buffer_cpu[task_index] = i_local_index;
        j_local_index_ring_buffer_cpu[task_index] = j_local_index;

        for (unsigned int i = 0;
             i < scratch_data.kx_mapping_support_points_permuted.size();
             i++)
          {
            kx_mapping_support_points_permuted_ring_buffer_cpu(task_index, i) =
              scratch_data.kx_mapping_support_points_permuted[i];
          }

        for (unsigned int i = 0;
             i < scratch_data.ky_mapping_support_points_permuted.size();
             i++)
          {
            ky_mapping_support_points_permuted_ring_buffer_cpu(task_index, i) =
              scratch_data.ky_mapping_support_points_permuted[i];
          }

        /**
         * @internal Up to now, the new task has been created on the host, set
         * its status to @p created using atomic operation, which can then be
         * fetched and processed by a consumer.
         */
        task_status_ring_buffer[task_index].store(
          SauterQuadratureTaskStatus::created);
      }

      /**
       * @internal Update ring buffer properties and notify consumer threads.
       */
      bool if_notify_consumers;

      {
#if ENABLE_NVTX == 1
        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range("move tail_committed");
#endif

        std::lock_guard<std::mutex> lg(buffer_lock);

        if (task_index == tail_committed)
          {
            /**
             * @internal Increment the pointer @p tail_committed until a task
             * not having been created is met or there are no added tasks either
             * during creation or created. Otherwise, do nothing.
             */
            while (true)
              {
                tail_committed = (tail_committed + 1) >= capacity ?
                                   (tail_committed + 1 - capacity) :
                                   (tail_committed + 1);

                if ((task_status_ring_buffer[tail_committed].load() !=
                     SauterQuadratureTaskStatus::created) ||
                    (tail_committed == tail_pending))
                  {
                    break;
                  }
              }
          }

        /**
         * @internal When the current task is not the last one, do not
         * notify consumer threads until the number of tasks reaches the
         * batch size.
         */
        if (task_num_ready_for_processing() >= batch_size)
          {
            if_notify_consumers = true;
          }
        else
          {
            if_notify_consumers = false;
          }

#if ENABLE_DEBUG == 1
        deallog << "Task " << task_index << " created" << std::endl;
        print_pointers(deallog);
#endif
      } // Release the ring buffer lock

      if (if_notify_consumers)
        {
          batch_task_ready.notify_one();
        }
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    template <template <int, typename> typename KernelFunctionType>
    __host__ bool
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      fetch_and_process_task_batch(
        const KernelFunctionType<spacedim, RangeNumberType> &kernel,
        const RangeNumberType                                kernel_factor,
        const unsigned int                                   block_size,
        const CUDABEMValues<dim, spacedim, RangeNumberType> &bem_values,
        std::mutex                                          &result_lock,
        const cudaStream_t                                   cuda_stream_handle)
    {
#if ENABLE_DEBUG == 1
      LogStream::Prefix prefix_string(
        std::string("process_task in ") +
        std::string(cell_neighboring_type_name(cell_neighboring_type)));
#endif
#if ENABLE_NVTX == 1
      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
        std::string("process_task for ") +
        std::string(cell_neighboring_type_name(cell_neighboring_type)));
#endif

      bool         is_finished;
      unsigned int task_num_to_fetch;
      unsigned int task_start_index_to_fetch = 0;

      {
#if ENABLE_NVTX == 1
        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
          "wait for batch_task_ready");
#endif

        std::unique_lock<std::mutex> ul(buffer_lock);

        batch_task_ready.wait(ul, [this]() {
          if (if_all_near_field_hmat_entries_added ||
              (task_num_ready_for_processing() >= batch_size))
            {
              /**
               * @internal When all quadrature tasks have been added, the
               * consumer does not wait for a whole batch of tasks but will
               * directly process the remaining tasks in the buffer.
               *
               * If all quadrature tasks have not been added, only when the
               * number of ready tasks is larger than or equal to the batch
               * size, they will be fetched and processed by the consumer
               * thread.
               */
              return true;
            }
          else
            {
              return false;
            }
        });

        task_num_to_fetch = task_num_ready_for_processing();
        if (task_num_to_fetch > batch_size)
          {
            task_num_to_fetch = batch_size;
          }

        if (if_all_near_field_hmat_entries_added && task_num_to_fetch == 0)
          {
            /**
             * @internal When all tasks have been added and there are no more
             * tasks to be fetched, the current consumer thread should exit.
             */
            is_finished = true;
          }
        else
          {
            is_finished = false;

            task_start_index_to_fetch = head_pending;
            head_pending = (head_pending + task_num_to_fetch) >= capacity ?
                             (head_pending + task_num_to_fetch - capacity) :
                             (head_pending + task_num_to_fetch);
          }

#if ENABLE_DEBUG == 1
        print_pointers(deallog);
#endif
      } // Release the buffer lock


      if (is_finished)
        {
          return is_finished;
        }
      else
        {
#if ENABLE_NVTX == 1
          IdeoBEM::CUDAWrappers::NVTXRange nvtx_range("processing task");
#endif

          /**
           * @internal When the data in the ring buffer are not
           * contiguous, we should perform two times of memory copy.
           */
          unsigned int number_of_data_until_buffer_end  = 0;
          unsigned int number_of_data_from_buffer_start = 0;
          const bool   is_task_batch_before_buffer_end =
            (task_start_index_to_fetch + task_num_to_fetch) <= capacity;

          /**
           * Set task status to be @p during_processing.
           */
          if (is_task_batch_before_buffer_end)
            {
              for (unsigned int i = task_start_index_to_fetch;
                   i < task_start_index_to_fetch + task_num_to_fetch;
                   i++)
                {
                  Assert(task_status_ring_buffer[i].load() ==
                           SauterQuadratureTaskStatus::created,
                         ExcInternalError());

                  task_status_ring_buffer[i].store(
                    SauterQuadratureTaskStatus::during_processing);
                }
            }
          else
            {
              number_of_data_until_buffer_end =
                capacity - task_start_index_to_fetch;
              number_of_data_from_buffer_start =
                task_num_to_fetch - number_of_data_until_buffer_end;

              for (unsigned int i = task_start_index_to_fetch; i < capacity;
                   i++)
                {
                  Assert(task_status_ring_buffer[i].load() ==
                           SauterQuadratureTaskStatus::created,
                         ExcInternalError());

                  task_status_ring_buffer[i].store(
                    SauterQuadratureTaskStatus::during_processing);
                }

              for (unsigned int i = 0; i < number_of_data_from_buffer_start;
                   i++)
                {
                  Assert(task_status_ring_buffer[i].load() ==
                           SauterQuadratureTaskStatus::created,
                         ExcInternalError());

                  task_status_ring_buffer[i].store(
                    SauterQuadratureTaskStatus::during_processing);
                }
            }

          cudaError_t error_code;

          /**
           * @internal Copy task data in batch from host to device.
           */
          if (is_task_batch_before_buffer_end)
            {
              kx_mapping_support_points_permuted_ring_buffer_gpu
                .assign_from_host(
                  &(kx_mapping_support_points_permuted_ring_buffer_cpu(
                    task_start_index_to_fetch, 0)),
                  kx_mapping_support_points_permuted_ring_buffer_gpu.offset(
                    task_start_index_to_fetch, 0),
                  task_num_to_fetch *
                    kx_mapping_support_points_permuted_ring_buffer_cpu.size(1),
                  cuda_stream_handle);

              ky_mapping_support_points_permuted_ring_buffer_gpu
                .assign_from_host(
                  &(ky_mapping_support_points_permuted_ring_buffer_cpu(
                    task_start_index_to_fetch, 0)),
                  ky_mapping_support_points_permuted_ring_buffer_gpu.offset(
                    task_start_index_to_fetch, 0),
                  task_num_to_fetch *
                    ky_mapping_support_points_permuted_ring_buffer_cpu.size(1),
                  cuda_stream_handle);

              error_code =
                cudaMemcpyAsync((void *)(i_local_index_ring_buffer_gpu +
                                         task_start_index_to_fetch),
                                (void *)(i_local_index_ring_buffer_cpu +
                                         task_start_index_to_fetch),
                                sizeof(unsigned int) * task_num_to_fetch,
                                cudaMemcpyHostToDevice,
                                cuda_stream_handle);
              AssertCuda(error_code);

              error_code =
                cudaMemcpyAsync((void *)(j_local_index_ring_buffer_gpu +
                                         task_start_index_to_fetch),
                                (void *)(j_local_index_ring_buffer_cpu +
                                         task_start_index_to_fetch),
                                sizeof(unsigned int) * task_num_to_fetch,
                                cudaMemcpyHostToDevice,
                                cuda_stream_handle);
              AssertCuda(error_code);
            }
          else
            {
              kx_mapping_support_points_permuted_ring_buffer_gpu
                .assign_from_host(
                  &(kx_mapping_support_points_permuted_ring_buffer_cpu(
                    task_start_index_to_fetch, 0)),
                  kx_mapping_support_points_permuted_ring_buffer_gpu.offset(
                    task_start_index_to_fetch, 0),
                  number_of_data_until_buffer_end *
                    kx_mapping_support_points_permuted_ring_buffer_cpu.size(1),
                  cuda_stream_handle);

              ky_mapping_support_points_permuted_ring_buffer_gpu
                .assign_from_host(
                  &(ky_mapping_support_points_permuted_ring_buffer_cpu(
                    task_start_index_to_fetch, 0)),
                  ky_mapping_support_points_permuted_ring_buffer_gpu.offset(
                    task_start_index_to_fetch, 0),
                  number_of_data_until_buffer_end *
                    ky_mapping_support_points_permuted_ring_buffer_cpu.size(1),
                  cuda_stream_handle);

              error_code =
                cudaMemcpyAsync((void *)(i_local_index_ring_buffer_gpu +
                                         task_start_index_to_fetch),
                                (void *)(i_local_index_ring_buffer_cpu +
                                         task_start_index_to_fetch),
                                sizeof(unsigned int) *
                                  number_of_data_until_buffer_end,
                                cudaMemcpyHostToDevice,
                                cuda_stream_handle);
              AssertCuda(error_code);

              error_code =
                cudaMemcpyAsync((void *)(j_local_index_ring_buffer_gpu +
                                         task_start_index_to_fetch),
                                (void *)(j_local_index_ring_buffer_cpu +
                                         task_start_index_to_fetch),
                                sizeof(unsigned int) *
                                  number_of_data_until_buffer_end,
                                cudaMemcpyHostToDevice,
                                cuda_stream_handle);
              AssertCuda(error_code);

              kx_mapping_support_points_permuted_ring_buffer_gpu
                .assign_from_host(
                  &(kx_mapping_support_points_permuted_ring_buffer_cpu(0, 0)),
                  kx_mapping_support_points_permuted_ring_buffer_gpu.offset(0,
                                                                            0),
                  number_of_data_from_buffer_start *
                    kx_mapping_support_points_permuted_ring_buffer_cpu.size(1),
                  cuda_stream_handle);

              ky_mapping_support_points_permuted_ring_buffer_gpu
                .assign_from_host(
                  &(ky_mapping_support_points_permuted_ring_buffer_cpu(0, 0)),
                  ky_mapping_support_points_permuted_ring_buffer_gpu.offset(0,
                                                                            0),
                  number_of_data_from_buffer_start *
                    ky_mapping_support_points_permuted_ring_buffer_cpu.size(1),
                  cuda_stream_handle);

              error_code =
                cudaMemcpyAsync((void *)(i_local_index_ring_buffer_gpu),
                                (void *)i_local_index_ring_buffer_cpu,
                                sizeof(unsigned int) *
                                  number_of_data_from_buffer_start,
                                cudaMemcpyHostToDevice,
                                cuda_stream_handle);
              AssertCuda(error_code);

              error_code =
                cudaMemcpyAsync((void *)(j_local_index_ring_buffer_gpu),
                                (void *)(j_local_index_ring_buffer_cpu),
                                sizeof(unsigned int) *
                                  number_of_data_from_buffer_start,
                                cudaMemcpyHostToDevice,
                                cuda_stream_handle);
              AssertCuda(error_code);
            }

          /**
           * @internal Recalculate the number of thread blocks to be used,
           * since the actual number of tasks may be smaller than the
           * originally expected.
           */
          const unsigned int block_num =
            (task_num_to_fetch + block_size - 1) / block_size;

          /**
           * Perform Sauter quadrature.
           *
           * N.B. In reality, because four ring buffers have been created
           * for the four cell neighboring types, the current cell
           * neighboring type is definite and only one case below is
           * reachable.
           */
          switch (cell_neighboring_type)
            {
                case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                  IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                    "process sauter task: same panel");
#endif

                  process_sauter_task_on_one_pair_of_dofs_same_panel<<<
                    block_num,
                    block_size,
                    0,
                    cuda_stream_handle>>>(
                    kernel,
                    kernel_factor,
                    capacity,
                    kx_mapping_support_points_permuted_ring_buffer_gpu,
                    ky_mapping_support_points_permuted_ring_buffer_gpu,
                    i_local_index_ring_buffer_gpu,
                    j_local_index_ring_buffer_gpu,
                    quadrature_value_ring_buffer_gpu,
                    task_start_index_to_fetch,
                    task_num_to_fetch,
                    bem_values);

                  break;
                }
                case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                  IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                    "process sauter task: common edge");
#endif

                  process_sauter_task_on_one_pair_of_dofs_common_edge<<<
                    block_num,
                    block_size,
                    0,
                    cuda_stream_handle>>>(
                    kernel,
                    kernel_factor,
                    capacity,
                    kx_mapping_support_points_permuted_ring_buffer_gpu,
                    ky_mapping_support_points_permuted_ring_buffer_gpu,
                    i_local_index_ring_buffer_gpu,
                    j_local_index_ring_buffer_gpu,
                    quadrature_value_ring_buffer_gpu,
                    task_start_index_to_fetch,
                    task_num_to_fetch,
                    bem_values);

                  break;
                }
                case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                  IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                    "process sauter task: common vertex");
#endif

                  process_sauter_task_on_one_pair_of_dofs_common_vertex<<<
                    block_num,
                    block_size,
                    0,
                    cuda_stream_handle>>>(
                    kernel,
                    kernel_factor,
                    capacity,
                    kx_mapping_support_points_permuted_ring_buffer_gpu,
                    ky_mapping_support_points_permuted_ring_buffer_gpu,
                    i_local_index_ring_buffer_gpu,
                    j_local_index_ring_buffer_gpu,
                    quadrature_value_ring_buffer_gpu,
                    task_start_index_to_fetch,
                    task_num_to_fetch,
                    bem_values);

                  break;
                }
                case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                  IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                    "process sauter task: regular");
#endif

                  process_sauter_task_on_one_pair_of_dofs_regular<<<
                    block_num,
                    block_size,
                    0,
                    cuda_stream_handle>>>(
                    kernel,
                    kernel_factor,
                    capacity,
                    kx_mapping_support_points_permuted_ring_buffer_gpu,
                    ky_mapping_support_points_permuted_ring_buffer_gpu,
                    i_local_index_ring_buffer_gpu,
                    j_local_index_ring_buffer_gpu,
                    quadrature_value_ring_buffer_gpu,
                    task_start_index_to_fetch,
                    task_num_to_fetch,
                    bem_values);

                  break;
                }
                default: {
                  Assert(false, ExcInternalError());

                  break;
                }
            }

          /**
           * Copy the quadrature result data from the device to the host.
           */
          if (is_task_batch_before_buffer_end)
            {
#if ENABLE_NVTX == 1
              IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                "assemble quadrature result");
#endif

              /**
               * @internal When the data in the ring buffer are in a
               * contiguous block, perform one time of memory copy.
               */
              error_code =
                cudaMemcpyAsync((void *)(quadrature_value_ring_buffer_cpu +
                                         task_start_index_to_fetch),
                                (void *)(quadrature_value_ring_buffer_gpu +
                                         task_start_index_to_fetch),
                                task_num_to_fetch * sizeof(RangeNumberType),
                                cudaMemcpyDeviceToHost,
                                cuda_stream_handle);
              AssertCuda(error_code);

              /**
               * @internal This synchronize ensures the completion of all memory
               * operations and kernel executions in the current CUDA stream.
               */
              error_code = cudaStreamSynchronize(cuda_stream_handle);
              AssertCuda(error_code);

              /**
               * Assemble the result to full matrix in \hmatrix node in serial.
               */
              {
                std::lock_guard<std::mutex> lg(result_lock);

                for (unsigned int i = task_start_index_to_fetch;
                     i < task_start_index_to_fetch + task_num_to_fetch;
                     i++)
                  {
                    (*fullmat_ring_buffer_cpu[i])(
                      fullmat_row_index_ring_buffer_cpu[i],
                      fullmat_col_index_ring_buffer_cpu[i]) +=
                      quadrature_value_ring_buffer_cpu[i] +
                      mass_matrix_entries_ring_buffer_cpu[i];

                    /**
                     * @internal Reset the task status to @p before_creation.
                     */
                    task_status_ring_buffer[i].store(
                      SauterQuadratureTaskStatus::before_creation);
                  }
              } // Release result lock
            }
          else
            {
#if ENABLE_NVTX == 1
              IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                "assemble quadrature result");
#endif

              error_code =
                cudaMemcpyAsync((void *)(quadrature_value_ring_buffer_cpu +
                                         task_start_index_to_fetch),
                                (void *)(quadrature_value_ring_buffer_gpu +
                                         task_start_index_to_fetch),
                                number_of_data_until_buffer_end *
                                  sizeof(RangeNumberType),
                                cudaMemcpyDeviceToHost,
                                cuda_stream_handle);
              AssertCuda(error_code);

              error_code =
                cudaMemcpyAsync((void *)(quadrature_value_ring_buffer_cpu),
                                (void *)(quadrature_value_ring_buffer_gpu),
                                number_of_data_from_buffer_start *
                                  sizeof(RangeNumberType),
                                cudaMemcpyDeviceToHost,
                                cuda_stream_handle);
              AssertCuda(error_code);

              /**
               * @internal This synchronize ensures the completion of all memory
               * operations and kernel executions in the current CUDA stream.
               */
              error_code = cudaStreamSynchronize(cuda_stream_handle);
              AssertCuda(error_code);

              /**
               * Assemble the result to full matrix in \hmatrix node in serial.
               */
              {
                std::lock_guard<std::mutex> lg(result_lock);

                for (unsigned int i = task_start_index_to_fetch; i < capacity;
                     i++)
                  {
                    (*fullmat_ring_buffer_cpu[i])(
                      fullmat_row_index_ring_buffer_cpu[i],
                      fullmat_col_index_ring_buffer_cpu[i]) +=
                      quadrature_value_ring_buffer_cpu[i] +
                      mass_matrix_entries_ring_buffer_cpu[i];

                    /**
                     * @internal Reset the task status to @p before_creation.
                     */
                    task_status_ring_buffer[i].store(
                      SauterQuadratureTaskStatus::before_creation);
                  }

                for (unsigned int i = 0; i < number_of_data_from_buffer_start;
                     i++)
                  {
                    (*fullmat_ring_buffer_cpu[i])(
                      fullmat_row_index_ring_buffer_cpu[i],
                      fullmat_col_index_ring_buffer_cpu[i]) +=
                      quadrature_value_ring_buffer_cpu[i] +
                      mass_matrix_entries_ring_buffer_cpu[i];

                    /**
                     * @internal Reset the task status to @p before_creation.
                     */
                    task_status_ring_buffer[i].store(
                      SauterQuadratureTaskStatus::before_creation);
                  }
              } // Release result lock
            }

          {
#if ENABLE_NVTX == 1
            IdeoBEM::CUDAWrappers::NVTXRange nvtx_range("move head_committed");
#endif

            std::lock_guard<std::mutex> lg(buffer_lock);

            if (task_start_index_to_fetch == head_committed)
              {
                /**
                 * @internal Increment the pointer @p head_committed until a task
                 * not having been created is met. Otherwise, do nothing.
                 */
                while (true)
                  {
                    head_committed = (head_committed + 1) >= capacity ?
                                       (head_committed + 1 - capacity) :
                                       (head_committed + 1);

                    if ((task_status_ring_buffer[head_committed].load() !=
                         SauterQuadratureTaskStatus::before_creation) ||
                        (head_committed == head_pending))
                      {
                        break;
                      }
                  }
              }

#if ENABLE_DEBUG == 1
            deallog << "Tasks [" << task_start_index_to_fetch << ", "
                    << task_start_index_to_fetch + task_num_to_fetch
                    << ") are processed" << std::endl;
            print_pointers(deallog);
#endif
          } // Release the buffer lock
        }

      /**
       * @internal When a batch of tasks have been processed by a consumer
       * thread, there are a large amount of empty spaces into which new tasks
       * can be inserted. Therefore, we use notify all producer threads to start
       * working.
       */
      buffer_not_full.notify_all();

      return is_finished;
    }


    template <int capacity, int dim, int spacedim, typename RangeNumberType>
    void
    SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>::
      print_pointers(LogStream &log_stream) const
    {
      log_stream << "head_committed=" << head_committed
                 << ", head_pending=" << head_pending
                 << ", tail_committed=" << tail_committed
                 << ", tail_pending=" << tail_pending << std::endl;
    }


    template <int dim, int spacedim, typename RangeNumberType>
    SauterQuadratureTaskBufferForVector<dim, spacedim, RangeNumberType>::
      SauterQuadratureTaskBufferForVector(
        const unsigned int              v_vector_size,
        const unsigned int              max_cells_per_dof_in_test_space,
        const unsigned int              max_cells_per_dof_in_ansatz_space,
        const BEMValues<dim, spacedim> &bem_values,
        const cudaStream_t              stream)
      : cuda_stream_handle(stream)
      , vector_size(v_vector_size)
      , capacity(vector_size * max_cells_per_dof_in_test_space *
                 max_cells_per_dof_in_ansatz_space)
      , quad_point_num(bem_values.quad_rule_for_regular.size())
      , task_num(0)
      , starting_task_indices_for_dof_pairs_cpu(nullptr)
      , starting_task_indices_for_dof_pairs_gpu(nullptr)
      , i_local_index_buffer_cpu(nullptr)
      , j_local_index_buffer_cpu(nullptr)
      , i_local_index_buffer_gpu(nullptr)
      , j_local_index_buffer_gpu(nullptr)
      , values_at_all_quad_points_gpu(nullptr)
      , vector_element_values_gpu(nullptr)
    {
      /**
       * @internal Allocate memory asynchronously on the device.
       */
      cudaError_t error_code;

      error_code =
        cudaMallocHost((void **)&starting_task_indices_for_dof_pairs_cpu,
                       sizeof(unsigned int) * (vector_size + 1));
      AssertCuda(error_code);
      error_code =
        cudaMallocAsync((void **)&starting_task_indices_for_dof_pairs_gpu,
                        sizeof(unsigned int) * (vector_size + 1),
                        cuda_stream_handle);
      AssertCuda(error_code);

      kx_mapping_support_points_permuted_buffer_gpu.allocate(
        TableIndices<2>(capacity, bem_values.kx_mapping_data.n_shape_functions),
        cuda_stream_handle);
      ky_mapping_support_points_permuted_buffer_gpu.allocate(
        TableIndices<2>(capacity, bem_values.ky_mapping_data.n_shape_functions),
        cuda_stream_handle);

      error_code = cudaMallocAsync((void **)&i_local_index_buffer_gpu,
                                   capacity * sizeof(unsigned int),
                                   cuda_stream_handle);
      AssertCuda(error_code);
      error_code = cudaMallocAsync((void **)&j_local_index_buffer_gpu,
                                   capacity * sizeof(unsigned int),
                                   cuda_stream_handle);
      AssertCuda(error_code);

      error_code =
        cudaMallocAsync((void **)&values_at_all_quad_points_gpu,
                        capacity * quad_point_num * sizeof(RangeNumberType),
                        cuda_stream_handle);
      AssertCuda(error_code);

      error_code = cudaMallocAsync((void **)&vector_element_values_gpu,
                                   vector_size * sizeof(RangeNumberType),
                                   cuda_stream_handle);
      AssertCuda(error_code);

      /**
       * @internal Allocate memory on the host.
       *
       * \mynote{These operations are placed after asynchronous memory
       * allocations on the device, in order that they can run in parallel.}
       */
      kx_mapping_support_points_permuted_buffer_cpu.reinit(
        TableIndices<2>(capacity,
                        bem_values.kx_mapping_data.n_shape_functions));
      error_code = cudaHostRegister(
        (void *)&(kx_mapping_support_points_permuted_buffer_cpu(0, 0)),
        kx_mapping_support_points_permuted_buffer_cpu.n_elements() *
          sizeof(Point<spacedim, RangeNumberType>),
        0);
      AssertCuda(error_code);

      ky_mapping_support_points_permuted_buffer_cpu.reinit(
        TableIndices<2>(capacity,
                        bem_values.ky_mapping_data.n_shape_functions));
      error_code = cudaHostRegister(
        (void *)&(ky_mapping_support_points_permuted_buffer_cpu(0, 0)),
        ky_mapping_support_points_permuted_buffer_cpu.n_elements() *
          sizeof(Point<spacedim, RangeNumberType>),
        0);
      AssertCuda(error_code);

      error_code = cudaMallocHost((void **)&i_local_index_buffer_cpu,
                                  capacity * sizeof(unsigned int));
      AssertCuda(error_code);
      error_code = cudaMallocHost((void **)&j_local_index_buffer_cpu,
                                  capacity * sizeof(unsigned int));
      AssertCuda(error_code);
    }


    template <int dim, int spacedim, typename RangeNumberType>
    SauterQuadratureTaskBufferForVector<dim, spacedim, RangeNumberType>::
      ~SauterQuadratureTaskBufferForVector()
    {
      cudaError_t error_code;

      if (starting_task_indices_for_dof_pairs_cpu != nullptr)
        {
          error_code = cudaFreeHost(starting_task_indices_for_dof_pairs_cpu);
          AssertCuda(error_code);
        }

      if (starting_task_indices_for_dof_pairs_gpu != nullptr)
        {
          error_code = cudaFreeAsync(starting_task_indices_for_dof_pairs_gpu,
                                     cuda_stream_handle);
          AssertCuda(error_code);
        }

      kx_mapping_support_points_permuted_buffer_gpu.release(cuda_stream_handle);
      ky_mapping_support_points_permuted_buffer_gpu.release(cuda_stream_handle);

      if (i_local_index_buffer_gpu != nullptr)
        {
          error_code =
            cudaFreeAsync(i_local_index_buffer_gpu, cuda_stream_handle);
          AssertCuda(error_code);
        }

      if (j_local_index_buffer_gpu != nullptr)
        {
          error_code =
            cudaFreeAsync(j_local_index_buffer_gpu, cuda_stream_handle);
          AssertCuda(error_code);
        }

      if (values_at_all_quad_points_gpu != nullptr)
        {
          error_code =
            cudaFreeAsync(values_at_all_quad_points_gpu, cuda_stream_handle);
          AssertCuda(error_code);
        }

      if (vector_element_values_gpu != nullptr)
        {
          error_code =
            cudaFreeAsync(vector_element_values_gpu, cuda_stream_handle);
          AssertCuda(error_code);
        }

      error_code = cudaHostUnregister(
        (void *)&(kx_mapping_support_points_permuted_buffer_cpu(0, 0)));
      AssertCuda(error_code);

      error_code = cudaHostUnregister(
        (void *)&(ky_mapping_support_points_permuted_buffer_cpu(0, 0)));
      AssertCuda(error_code);

      if (i_local_index_buffer_cpu != nullptr)
        {
          error_code = cudaFreeHost(i_local_index_buffer_cpu);
          AssertCuda(error_code);
        }

      if (j_local_index_buffer_cpu != nullptr)
        {
          error_code = cudaFreeHost(j_local_index_buffer_cpu);
          AssertCuda(error_code);
        }

      vector_size    = 0;
      capacity       = 0;
      quad_point_num = 0;
    }


    template <int dim, int spacedim, typename RangeNumberType>
    void
    SauterQuadratureTaskBufferForVector<dim, spacedim, RangeNumberType>::
      reinit()
    {
      /**
       * @internal Reset the number of quadrature tasks to zero.
       */
      task_num = 0;
    }


    template <int dim, int spacedim, typename RangeNumberType>
    void
    SauterQuadratureTaskBufferForVector<dim, spacedim, RangeNumberType>::
      add_task(const unsigned int i_local_index,
               const unsigned int j_local_index,
               const PairCellWiseScratchData<dim, spacedim, RangeNumberType>
                 &scratch_data)
    {
      AssertIndexRange(task_num, capacity);

      /**
       * @internal Copy task data to host, which include the array indices to
       * access DoF indices in the cell and the permuted mapping support points.
       */
      i_local_index_buffer_cpu[task_num] = i_local_index;
      j_local_index_buffer_cpu[task_num] = j_local_index;

      for (unsigned int i = 0;
           i < scratch_data.kx_mapping_support_points_permuted.size();
           i++)
        {
          kx_mapping_support_points_permuted_buffer_cpu(task_num, i) =
            scratch_data.kx_mapping_support_points_permuted[i];
        }

      for (unsigned int i = 0;
           i < scratch_data.ky_mapping_support_points_permuted.size();
           i++)
        {
          ky_mapping_support_points_permuted_buffer_cpu(task_num, i) =
            scratch_data.ky_mapping_support_points_permuted[i];
        }

      /**
       * @internal Increase the task counter.
       */
      task_num++;
    }


    template <int dim, int spacedim, typename RangeNumberType>
    template <template <int, typename> typename KernelFunctionType>
    void
    SauterQuadratureTaskBufferForVector<dim, spacedim, RangeNumberType>::
      process_tasks(
        Vector<RangeNumberType>                             &hmat_vector,
        const KernelFunctionType<spacedim, RangeNumberType> &kernel,
        const RangeNumberType                                kernel_factor,
        const CUDABEMValues<dim, spacedim, RangeNumberType> &bem_values,
        const unsigned int                                   task_num_per_block,
        const unsigned int block_size_for_quad_result_accumulation,
        const cudaStream_t cuda_stream_handle)
    {
      cudaError_t error_code;

      /**
       * @internal Copy task data from host to device.
       */
      kx_mapping_support_points_permuted_buffer_gpu.assign_from_host(
        &(kx_mapping_support_points_permuted_buffer_cpu(0, 0)),
        0,
        task_num * kx_mapping_support_points_permuted_buffer_cpu.size(1),
        cuda_stream_handle);
      ky_mapping_support_points_permuted_buffer_gpu.assign_from_host(
        &(ky_mapping_support_points_permuted_buffer_cpu(0, 0)),
        0,
        task_num * ky_mapping_support_points_permuted_buffer_cpu.size(1),
        cuda_stream_handle);

      error_code = cudaMemcpyAsync((void *)i_local_index_buffer_gpu,
                                   (void *)i_local_index_buffer_cpu,
                                   sizeof(unsigned int) * task_num,
                                   cudaMemcpyHostToDevice,
                                   cuda_stream_handle);
      AssertCuda(error_code);

      error_code = cudaMemcpyAsync((void *)j_local_index_buffer_gpu,
                                   (void *)j_local_index_buffer_cpu,
                                   sizeof(unsigned int) * task_num,
                                   cudaMemcpyHostToDevice,
                                   cuda_stream_handle);
      AssertCuda(error_code);

      error_code =
        cudaMemcpyAsync((void *)starting_task_indices_for_dof_pairs_gpu,
                        (void *)starting_task_indices_for_dof_pairs_cpu,
                        sizeof(unsigned int) * (vector_size + 1),
                        cudaMemcpyHostToDevice,
                        cuda_stream_handle);
      AssertCuda(error_code);

      /**
       * @internal Process all Sauter quadrature tasks to the
       * buffer for assembling the row or column vector.
       */
      const unsigned int quad_point_num_per_block =
        device_properties.maxThreadsPerBlock / 2 / task_num_per_block;

      dim3 threads_rect(task_num_per_block, quad_point_num_per_block);
      dim3 blocks_rect((task_num + task_num_per_block - 1) / task_num_per_block,
                       (quad_point_num + quad_point_num_per_block - 1) /
                         quad_point_num_per_block);

      /**
       * @internal Evaluation at all quadrature points for all tasks.
       */
      process_sauter_tasks_in_buffer_for_far_field_hmat_vector<<<
        blocks_rect,
        threads_rect,
        0,
        cuda_stream_handle>>>(kernel,
                              kernel_factor,
                              kx_mapping_support_points_permuted_buffer_gpu,
                              ky_mapping_support_points_permuted_buffer_gpu,
                              i_local_index_buffer_gpu,
                              j_local_index_buffer_gpu,
                              values_at_all_quad_points_gpu,
                              task_num,
                              quad_point_num,
                              bem_values);

      /**
       * Accumulate quadrature results for each vector element.
       */
      const unsigned int block_num_for_quad_result_accumulation =
        (vector_size + block_size_for_quad_result_accumulation - 1) /
        block_size_for_quad_result_accumulation;
      accumulate_quadrature_values_into_vector<<<
        block_num_for_quad_result_accumulation,
        block_size_for_quad_result_accumulation,
        0,
        cuda_stream_handle>>>(vector_size,
                              quad_point_num,
                              starting_task_indices_for_dof_pairs_gpu,
                              values_at_all_quad_points_gpu,
                              vector_element_values_gpu);

      /**
       * Copy vector element values from device to host.
       */
      error_code = cudaMemcpyAsync((void *)hmat_vector.data(),
                                   (void *)vector_element_values_gpu,
                                   sizeof(RangeNumberType) * vector_size,
                                   cudaMemcpyDeviceToHost,
                                   cuda_stream_handle);
      AssertCuda(error_code);

      error_code = cudaStreamSynchronize(cuda_stream_handle);
      AssertCuda(error_code);
    }


    /**
     * Iterate over each matrix entry of the full matrices in the collection
     * of near field \hmatrices and add it as a Sauter quadrature task.
     *
     * This is the work function associated with a producer thread.
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param near_field_leaf_set
     * @param start_leaf_set_index
     * @param end_leaf_set_index It is the pass-the-end index.
     * @param ring_buffer_for_same_panel
     * @param ring_buffer_for_common_edge
     * @param ring_buffer_for_common_vertex
     * @param ring_buffer_for_regular
     * @param kx_dof_to_cell_topo
     * @param ky_dof_to_cell_topo
     * @param bem_values
     * @param bem_values_gpu
     * @param kx_dof_handler
     * @param ky_dof_handler
     * @param kx_map_from_local_to_full_dof_indices
     * @param ky_map_from_local_to_full_dof_indices
     * @param kx_dof_i2e_numbering
     * @param ky_dof_i2e_numbering
     * @param kx_mapping
     * @param ky_mapping
     * @param map_from_kx_boundary_mesh_to_volume_mesh
     * @param map_from_ky_boundary_mesh_to_volume_mesh
     * @param method_for_cell_neighboring_type
     * @param is_kernel_symmetric
     * @param enable_build_symmetric_hmat
     */
    template <int capacity,
              int dim,
              int spacedim,
              typename RangeNumberType = double>
    void
    sauter_quadrature_for_near_field_matrix_entries_producer(
      const std::vector<HMatrix<spacedim, RangeNumberType> *>
                        &near_field_leaf_set,
      const unsigned int start_leaf_set_index,
      const unsigned int end_leaf_set_index,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_same_panel,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_edge,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_vertex,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_regular,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
        &kx_dof_to_cell_topo,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                                      &ky_dof_to_cell_topo,
      const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
      const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                      &bem_values_gpu,
      const DoFHandler<dim, spacedim> &kx_dof_handler,
      const DoFHandler<dim, spacedim> &ky_dof_handler,
      const std::vector<types::global_dof_index>
        *kx_map_from_local_to_full_dof_indices,
      const std::vector<types::global_dof_index>
        *ky_map_from_local_to_full_dof_indices,
      const std::vector<types::global_dof_index> &kx_dof_i2e_numbering,
      const std::vector<types::global_dof_index> &ky_dof_i2e_numbering,
      const MappingQGenericExt<dim, spacedim>    &kx_mapping,
      const MappingQGenericExt<dim, spacedim>    &ky_mapping,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_kx_boundary_mesh_to_volume_mesh,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_ky_boundary_mesh_to_volume_mesh,
      const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
      const bool                            is_kernel_symmetric,
      const bool                            enable_build_symmetric_hmat = false)
    {
      const std::thread::id current_thread_id = std::this_thread::get_id();
      std::cerr << "Producer thread " << current_thread_id << " starts...\n";
#if ENABLE_NVTX == 1
      nvtxNameOsThread(getpid(), "producer thread");
#endif

      /**
       * @internal Define @p PairCellWiseScratchData and @p
       * PairCellWisePerTaskData which are local to the current working
       * thread. This is mandatory because each producer thread should have
       * its own copy of these data.
       *
       * N.B. A CUDA stream is created during the construction of the scratch
       * data.
       */
      PairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data(
        current_thread_id,
        kx_dof_handler.get_fe(),
        ky_dof_handler.get_fe(),
        kx_mapping,
        ky_mapping,
        bem_values);
      PairCellWisePerTaskData<dim, spacedim, RangeNumberType> copy_data(
        kx_dof_handler.get_fe(), ky_dof_handler.get_fe());

      /**
       * @internal Iterate over each \hmatrix node in the near field leaf set.
       */
      for (unsigned int l = start_leaf_set_index; l < end_leaf_set_index; l++)
        {
#if ENABLE_NVTX == 1
          IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
            std::string("create sauter tasks for near field hmatrix ") +
            std::to_string(l));
#endif

          HMatrix<spacedim, RangeNumberType> *leaf_mat = near_field_leaf_set[l];
          LAPACKFullMatrixExt<RangeNumberType> *fullmat =
            leaf_mat->get_fullmatrix();

          Assert(fullmat != nullptr, ExcInternalError());

          /**
           * @internal The row index range and column index range follow the
           * internal DoF numbering.
           */
          const std::array<types::global_dof_index, 2> *row_index_range =
            leaf_mat->get_row_index_range();
          const std::array<types::global_dof_index, 2> *col_index_range =
            leaf_mat->get_col_index_range();

          /**
           * @internal Global DoF indices in the external DoF numbering, which
           * are associated with the current full matrix's row and column
           * indices.
           */
          types::global_dof_index i_global_dof_index;
          types::global_dof_index j_global_dof_index;

          if (enable_build_symmetric_hmat && is_kernel_symmetric)
            {
              /**
               * When the flag @p enable_build_symmetric_hmat is true and the
               * kernel function is symmetric, try to build a symmetric
               * \hmatrix. Otherwise, the whole full matrix will be built.
               */
              switch (leaf_mat->get_block_type())
                {
                    case HMatrixSupport::diagonal_block: {
                      /**
                       * A diagonal \hmatrix block as well as its associated
                       * full matrix should be symmetric.
                       */
                      Assert(
                        leaf_mat->get_property() == HMatrixSupport::symmetric,
                        ExcInvalidHMatrixProperty(leaf_mat->get_property()));
                      Assert(fullmat->get_property() ==
                               LAPACKSupport::symmetric,
                             ExcInvalidLAPACKFullMatrixProperty(
                               fullmat->get_property()));

                      /**
                       * Only evaluate the diagonal and lower triangular
                       * elements in the full matrix.
                       */
                      for (size_t fullmat_row_index = 0;
                           fullmat_row_index < fullmat->m();
                           fullmat_row_index++)
                        {
                          i_global_dof_index =
                            (kx_map_from_local_to_full_dof_indices == nullptr) ?
                              kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                   fullmat_row_index] :
                              kx_map_from_local_to_full_dof_indices->at(
                                kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                     fullmat_row_index]);

                          for (size_t fullmat_col_index = 0;
                               fullmat_col_index <= fullmat_row_index;
                               fullmat_col_index++)
                            {
                              j_global_dof_index =
                                (ky_map_from_local_to_full_dof_indices ==
                                 nullptr) ?
                                  ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                       fullmat_col_index] :
                                  ky_map_from_local_to_full_dof_indices->at(
                                    ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                         fullmat_col_index]);

                              create_sauter_quadrature_tasks_on_one_pair_of_dofs(
                                ring_buffer_for_same_panel,
                                ring_buffer_for_common_edge,
                                ring_buffer_for_common_vertex,
                                ring_buffer_for_regular,
                                fullmat,
                                fullmat_row_index,
                                fullmat_col_index,
                                i_global_dof_index,
                                j_global_dof_index,
                                kx_dof_to_cell_topo,
                                ky_dof_to_cell_topo,
                                kx_dof_handler,
                                ky_dof_handler,
                                kx_mapping,
                                ky_mapping,
                                map_from_kx_boundary_mesh_to_volume_mesh,
                                map_from_ky_boundary_mesh_to_volume_mesh,
                                method_for_cell_neighboring_type,
                                scratch_data,
                                copy_data);
                            }
                        }

                      break;
                    }
                    case HMatrixSupport::upper_triangular_block: {
                      /**
                       * Do not build \hmatrix block belonging to the upper
                       * triangular part.
                       */

                      break;
                    }
                    case HMatrixSupport::lower_triangular_block: {
                      /**
                       * When the current \hmatrix block belongs to the lower
                       * triangular part, evaluate all of its elements as
                       * usual.
                       */
                      for (size_t fullmat_row_index = 0;
                           fullmat_row_index < fullmat->m();
                           fullmat_row_index++)
                        {
                          i_global_dof_index =
                            (kx_map_from_local_to_full_dof_indices == nullptr) ?
                              kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                   fullmat_row_index] :
                              kx_map_from_local_to_full_dof_indices->at(
                                kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                     fullmat_row_index]);

                          for (size_t fullmat_col_index = 0;
                               fullmat_col_index < fullmat->n();
                               fullmat_col_index++)
                            {
                              j_global_dof_index =
                                (ky_map_from_local_to_full_dof_indices ==
                                 nullptr) ?
                                  ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                       fullmat_col_index] :
                                  ky_map_from_local_to_full_dof_indices->at(
                                    ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                         fullmat_col_index]);

                              create_sauter_quadrature_tasks_on_one_pair_of_dofs(
                                ring_buffer_for_same_panel,
                                ring_buffer_for_common_edge,
                                ring_buffer_for_common_vertex,
                                ring_buffer_for_regular,
                                fullmat,
                                fullmat_row_index,
                                fullmat_col_index,
                                i_global_dof_index,
                                j_global_dof_index,
                                kx_dof_to_cell_topo,
                                ky_dof_to_cell_topo,
                                kx_dof_handler,
                                ky_dof_handler,
                                kx_mapping,
                                ky_mapping,
                                map_from_kx_boundary_mesh_to_volume_mesh,
                                map_from_ky_boundary_mesh_to_volume_mesh,
                                method_for_cell_neighboring_type,
                                scratch_data,
                                copy_data);
                            }
                        }

                      break;
                    }
                    case HMatrixSupport::undefined_block: {
                      Assert(false,
                             ExcInvalidHMatrixBlockType(
                               leaf_mat->get_block_type()));

                      break;
                    }
                }
            }
          else
            {
              /**
               * Evaluate the whole full matrix.
               */
              for (size_t fullmat_row_index = 0;
                   fullmat_row_index < fullmat->m();
                   fullmat_row_index++)
                {
                  i_global_dof_index =
                    (kx_map_from_local_to_full_dof_indices == nullptr) ?
                      kx_dof_i2e_numbering[(*row_index_range)[0] +
                                           fullmat_row_index] :
                      kx_map_from_local_to_full_dof_indices->at(
                        kx_dof_i2e_numbering[(*row_index_range)[0] +
                                             fullmat_row_index]);

                  for (size_t fullmat_col_index = 0;
                       fullmat_col_index < fullmat->n();
                       fullmat_col_index++)
                    {
                      j_global_dof_index =
                        (ky_map_from_local_to_full_dof_indices == nullptr) ?
                          ky_dof_i2e_numbering[(*col_index_range)[0] +
                                               fullmat_col_index] :
                          ky_map_from_local_to_full_dof_indices->at(
                            ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                 fullmat_col_index]);

                      create_sauter_quadrature_tasks_on_one_pair_of_dofs(
                        ring_buffer_for_same_panel,
                        ring_buffer_for_common_edge,
                        ring_buffer_for_common_vertex,
                        ring_buffer_for_regular,
                        fullmat,
                        fullmat_row_index,
                        fullmat_col_index,
                        i_global_dof_index,
                        j_global_dof_index,
                        kx_dof_to_cell_topo,
                        ky_dof_to_cell_topo,
                        kx_dof_handler,
                        ky_dof_handler,
                        kx_mapping,
                        ky_mapping,
                        map_from_kx_boundary_mesh_to_volume_mesh,
                        map_from_ky_boundary_mesh_to_volume_mesh,
                        method_for_cell_neighboring_type,
                        scratch_data,
                        copy_data);
                    }
                }
            }
        }

      /**
       * @internal During releasing the scratch data, the CUDA stream will
       * also be destroyed.
       */
      scratch_data.release();
      copy_data.release();

      std::cerr << "Producer thread " << current_thread_id << " exits...\n";
    }


    template <int capacity,
              int dim,
              int spacedim,
              typename RangeNumberType = double>
    void
    sauter_quadrature_for_near_field_matrix_entries_producer(
      const std::vector<HMatrix<spacedim, RangeNumberType> *>
                        &near_field_leaf_set,
      const unsigned int start_leaf_set_index,
      const unsigned int end_leaf_set_index,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_same_panel,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_edge,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_vertex,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
                           &ring_buffer_for_regular,
      const RangeNumberType mass_matrix_factor,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
        &kx_dof_to_cell_topo,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                                      &ky_dof_to_cell_topo,
      const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
      const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                      &bem_values_gpu,
      const QGauss<dim>               &mass_matrix_quadrature_formula,
      const DoFHandler<dim, spacedim> &kx_dof_handler,
      const DoFHandler<dim, spacedim> &ky_dof_handler,
      const std::vector<types::global_dof_index>
        *kx_map_from_local_to_full_dof_indices,
      const std::vector<types::global_dof_index>
        *ky_map_from_local_to_full_dof_indices,
      const std::vector<types::global_dof_index> &kx_dof_i2e_numbering,
      const std::vector<types::global_dof_index> &ky_dof_i2e_numbering,
      const MappingQGenericExt<dim, spacedim>    &kx_mapping,
      const MappingQGenericExt<dim, spacedim>    &ky_mapping,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_kx_boundary_mesh_to_volume_mesh,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_ky_boundary_mesh_to_volume_mesh,
      const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
      const bool                            is_kernel_symmetric,
      const bool                            enable_build_symmetric_hmat = false)
    {
      const std::thread::id current_thread_id = std::this_thread::get_id();
      std::cerr << "Producer thread " << current_thread_id << " starts...\n";
#if ENABLE_NVTX == 1
      nvtxNameOsThread(getpid(), "producer thread");
#endif

      /**
       * Define @p CellWiseScratchData which is local to the current working thread.
       */
      CellWiseScratchDataForMassMatrix<dim, spacedim> mass_matrix_scratch_data(
        kx_dof_handler.get_fe(),
        ky_dof_handler.get_fe(),
        mass_matrix_quadrature_formula,
        update_values | update_JxW_values);

      /**
       * @internal Define @p PairCellWiseScratchData and
       * @p PairCellWisePerTaskData which are local to the current working
       * thread. This is mandatory because each producer thread should
       * have its own copy of these data.
       *
       * N.B. A CUDA stream is created during the construction of the scratch
       * data.
       */
      PairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data(
        current_thread_id,
        kx_dof_handler.get_fe(),
        ky_dof_handler.get_fe(),
        kx_mapping,
        ky_mapping,
        bem_values);
      PairCellWisePerTaskData<dim, spacedim, RangeNumberType> copy_data(
        kx_dof_handler.get_fe(), ky_dof_handler.get_fe());

      /**
       * @internal Iterate over each \hmatrix node in the near field leaf set.
       */
      for (unsigned int l = start_leaf_set_index; l < end_leaf_set_index; l++)
        {
#if ENABLE_NVTX == 1
          IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
            std::string("create sauter tasks for near field hmatrix ") +
            std::to_string(l));
#endif

          HMatrix<spacedim, RangeNumberType> *leaf_mat = near_field_leaf_set[l];
          LAPACKFullMatrixExt<RangeNumberType> *fullmat =
            leaf_mat->get_fullmatrix();

          Assert(fullmat != nullptr, ExcInternalError());

          /**
           * @internal The row index range and column index range follow the
           * internal DoF numbering.
           */
          const std::array<types::global_dof_index, 2> *row_index_range =
            leaf_mat->get_row_index_range();
          const std::array<types::global_dof_index, 2> *col_index_range =
            leaf_mat->get_col_index_range();

          /**
           * @internal Global DoF indices in the external DoF numbering, which
           * are associated with the current full matrix's row and column
           * indices.
           */
          types::global_dof_index i_global_dof_index;
          types::global_dof_index j_global_dof_index;

          if (enable_build_symmetric_hmat && is_kernel_symmetric)
            {
              /**
               * When the flag @p enable_build_symmetric_hmat is true and the
               * kernel function is symmetric, try to build a symmetric
               * \hmatrix. Otherwise, the whole full matrix will be built.
               */
              switch (leaf_mat->get_block_type())
                {
                    case HMatrixSupport::diagonal_block: {
                      /**
                       * A diagonal \hmatrix block as well as its associated
                       * full matrix should be symmetric.
                       */
                      Assert(
                        leaf_mat->get_property() == HMatrixSupport::symmetric,
                        ExcInvalidHMatrixProperty(leaf_mat->get_property()));
                      Assert(fullmat->get_property() ==
                               LAPACKSupport::symmetric,
                             ExcInvalidLAPACKFullMatrixProperty(
                               fullmat->get_property()));

                      /**
                       * Only evaluate the diagonal and lower triangular
                       * elements in the full matrix.
                       */
                      for (size_t fullmat_row_index = 0;
                           fullmat_row_index < fullmat->m();
                           fullmat_row_index++)
                        {
                          i_global_dof_index =
                            (kx_map_from_local_to_full_dof_indices == nullptr) ?
                              kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                   fullmat_row_index] :
                              kx_map_from_local_to_full_dof_indices->at(
                                kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                     fullmat_row_index]);

                          for (size_t fullmat_col_index = 0;
                               fullmat_col_index <= fullmat_row_index;
                               fullmat_col_index++)
                            {
                              j_global_dof_index =
                                (ky_map_from_local_to_full_dof_indices ==
                                 nullptr) ?
                                  ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                       fullmat_col_index] :
                                  ky_map_from_local_to_full_dof_indices->at(
                                    ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                         fullmat_col_index]);

                              create_sauter_quadrature_tasks_on_one_pair_of_dofs(
                                ring_buffer_for_same_panel,
                                ring_buffer_for_common_edge,
                                ring_buffer_for_common_vertex,
                                ring_buffer_for_regular,
                                mass_matrix_factor,
                                fullmat,
                                fullmat_row_index,
                                fullmat_col_index,
                                i_global_dof_index,
                                j_global_dof_index,
                                kx_dof_to_cell_topo,
                                ky_dof_to_cell_topo,
                                kx_dof_handler,
                                ky_dof_handler,
                                kx_mapping,
                                ky_mapping,
                                map_from_kx_boundary_mesh_to_volume_mesh,
                                map_from_ky_boundary_mesh_to_volume_mesh,
                                method_for_cell_neighboring_type,
                                mass_matrix_scratch_data,
                                scratch_data,
                                copy_data);
                            }
                        }

                      break;
                    }
                    case HMatrixSupport::upper_triangular_block: {
                      /**
                       * Do not build \hmatrix block belonging to the upper
                       * triangular part.
                       */

                      break;
                    }
                    case HMatrixSupport::lower_triangular_block: {
                      /**
                       * When the current \hmatrix block belongs to the lower
                       * triangular part, evaluate all of its elements as
                       * usual.
                       */
                      for (size_t fullmat_row_index = 0;
                           fullmat_row_index < fullmat->m();
                           fullmat_row_index++)
                        {
                          i_global_dof_index =
                            (kx_map_from_local_to_full_dof_indices == nullptr) ?
                              kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                   fullmat_row_index] :
                              kx_map_from_local_to_full_dof_indices->at(
                                kx_dof_i2e_numbering[(*row_index_range)[0] +
                                                     fullmat_row_index]);

                          for (size_t fullmat_col_index = 0;
                               fullmat_col_index < fullmat->n();
                               fullmat_col_index++)
                            {
                              j_global_dof_index =
                                (ky_map_from_local_to_full_dof_indices ==
                                 nullptr) ?
                                  ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                       fullmat_col_index] :
                                  ky_map_from_local_to_full_dof_indices->at(
                                    ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                         fullmat_col_index]);

                              create_sauter_quadrature_tasks_on_one_pair_of_dofs(
                                ring_buffer_for_same_panel,
                                ring_buffer_for_common_edge,
                                ring_buffer_for_common_vertex,
                                ring_buffer_for_regular,
                                mass_matrix_factor,
                                fullmat,
                                fullmat_row_index,
                                fullmat_col_index,
                                i_global_dof_index,
                                j_global_dof_index,
                                kx_dof_to_cell_topo,
                                ky_dof_to_cell_topo,
                                kx_dof_handler,
                                ky_dof_handler,
                                kx_mapping,
                                ky_mapping,
                                map_from_kx_boundary_mesh_to_volume_mesh,
                                map_from_ky_boundary_mesh_to_volume_mesh,
                                method_for_cell_neighboring_type,
                                mass_matrix_scratch_data,
                                scratch_data,
                                copy_data);
                            }
                        }

                      break;
                    }
                    case HMatrixSupport::undefined_block: {
                      Assert(false,
                             ExcInvalidHMatrixBlockType(
                               leaf_mat->get_block_type()));

                      break;
                    }
                }
            }
          else
            {
              /**
               * Evaluate the whole full matrix.
               */
              for (size_t fullmat_row_index = 0;
                   fullmat_row_index < fullmat->m();
                   fullmat_row_index++)
                {
                  i_global_dof_index =
                    (kx_map_from_local_to_full_dof_indices == nullptr) ?
                      kx_dof_i2e_numbering[(*row_index_range)[0] +
                                           fullmat_row_index] :
                      kx_map_from_local_to_full_dof_indices->at(
                        kx_dof_i2e_numbering[(*row_index_range)[0] +
                                             fullmat_row_index]);

                  for (size_t fullmat_col_index = 0;
                       fullmat_col_index < fullmat->n();
                       fullmat_col_index++)
                    {
                      j_global_dof_index =
                        (ky_map_from_local_to_full_dof_indices == nullptr) ?
                          ky_dof_i2e_numbering[(*col_index_range)[0] +
                                               fullmat_col_index] :
                          ky_map_from_local_to_full_dof_indices->at(
                            ky_dof_i2e_numbering[(*col_index_range)[0] +
                                                 fullmat_col_index]);

                      create_sauter_quadrature_tasks_on_one_pair_of_dofs(
                        ring_buffer_for_same_panel,
                        ring_buffer_for_common_edge,
                        ring_buffer_for_common_vertex,
                        ring_buffer_for_regular,
                        mass_matrix_factor,
                        fullmat,
                        fullmat_row_index,
                        fullmat_col_index,
                        i_global_dof_index,
                        j_global_dof_index,
                        kx_dof_to_cell_topo,
                        ky_dof_to_cell_topo,
                        kx_dof_handler,
                        ky_dof_handler,
                        kx_mapping,
                        ky_mapping,
                        map_from_kx_boundary_mesh_to_volume_mesh,
                        map_from_ky_boundary_mesh_to_volume_mesh,
                        method_for_cell_neighboring_type,
                        mass_matrix_scratch_data,
                        scratch_data,
                        copy_data);
                    }
                }
            }
        }

      /**
       * @internal During releasing the scratch data, the CUDA stream will
       * also be destroyed.
       */
      scratch_data.release();
      copy_data.release();

      std::cerr << "Producer thread " << current_thread_id << " exits...\n";
    }


    template <int capacity,
              int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    void
    sauter_quadrature_for_near_field_matrix_entries_consumer(
      const KernelFunctionType<spacedim, RangeNumberType> &kernel,
      const RangeNumberType                                kernel_factor,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
                                                          &ring_buffer,
      const CUDABEMValues<dim, spacedim, RangeNumberType> &bem_values_gpu,
      std::mutex                                          &result_lock,
      const unsigned int                                   block_size)
    {
      const std::thread::id current_thread_id = std::this_thread::get_id();
      std::cerr << "Consumer thread " << current_thread_id << " for "
                << cell_neighboring_type_name(ring_buffer.cell_neighboring_type)
                << " starts..." << std::endl;
#if ENABLE_NVTX == 1
      std::string nvtx_thread_name(std::string("consumer thread for ") +
                                   std::string(cell_neighboring_type_name(
                                     ring_buffer.cell_neighboring_type)));
      nvtxNameOsThread(getpid(), nvtx_thread_name.c_str());
#endif

      const unsigned int consumer_wait_time_ms = 1;

      cudaStream_t cuda_stream_handle;
      cudaError_t  error_code = cudaStreamCreate(&cuda_stream_handle);
      AssertCuda(error_code);

      /**
       * Continuously process the Sauter quadrature tasks in the ring buffer
       * for same panel case.
       */
      while (!ring_buffer.fetch_and_process_task_batch(kernel,
                                                       kernel_factor,
                                                       block_size,
                                                       bem_values_gpu,
                                                       result_lock,
                                                       cuda_stream_handle))
        {
          std::this_thread::sleep_for(
            std::chrono::milliseconds(consumer_wait_time_ms));
        }

      error_code = cudaStreamDestroy(cuda_stream_handle);
      AssertCuda(error_code);

      std::cerr << "Consumer thread " << current_thread_id << " for "
                << cell_neighboring_type_name(ring_buffer.cell_neighboring_type)
                << " exits..." << std::endl;
    }

    /**
     * Create Sauter quadrature tasks which are associated with a pair of
     * global DoF indices in the external numbering with respect to the
     * complete DoF set in the DoF handler.
     *
     * The created tasks are added to the ring buffer with the corresponding
     * cell neighboring type.
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param ring_buffer_for_same_panel
     * @param ring_buffer_for_common_edge
     * @param ring_buffer_for_common_vertex
     * @param ring_buffer_for_regular
     * @param fullmat
     * @param fullmat_row_index
     * @param fullmat_col_index
     * @param i_global_dof_index Global DoF index in the external numbering. If only a subset of
     * the complete DoFs in the associated DoF handler is effective or
     * selected, this index is with respect to the complete DoF set.
     * @param j_global_dof_index Global DoF index in the external numbering. If only a subset of
     * the complete DoFs in the associated DoF handler is effective or
     * selected, this index is with respect to the complete DoF set.
     * @param kx_dof_to_cell_topo
     * @param ky_dof_to_cell_topo
     * @param kx_dof_handler
     * @param ky_dof_handler
     * @param kx_mapping
     * @param ky_mapping
     * @param map_from_kx_boundary_mesh_to_volume_mesh
     * @param map_from_ky_boundary_mesh_to_volume_mesh
     * @param method_for_cell_neighboring_type
     * @param scratch_data
     * @param copy_data
     * @param is_last_task
     */
    template <int capacity,
              int dim,
              int spacedim,
              typename RangeNumberType = double>
    void
    create_sauter_quadrature_tasks_on_one_pair_of_dofs(
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_same_panel,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_edge,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_vertex,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
                                           &ring_buffer_for_regular,
      LAPACKFullMatrixExt<RangeNumberType> *fullmat,
      const unsigned int                    fullmat_row_index,
      const unsigned int                    fullmat_col_index,
      const types::global_dof_index         i_global_dof_index,
      const types::global_dof_index         j_global_dof_index,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
        &kx_dof_to_cell_topo,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                              &ky_dof_to_cell_topo,
      const DoFHandler<dim, spacedim>         &kx_dof_handler,
      const DoFHandler<dim, spacedim>         &ky_dof_handler,
      const MappingQGenericExt<dim, spacedim> &kx_mapping,
      const MappingQGenericExt<dim, spacedim> &ky_mapping,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_kx_boundary_mesh_to_volume_mesh,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_ky_boundary_mesh_to_volume_mesh,
      const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
      PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
      PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
    {
      //#if ENABLE_DEBUG == 1
      //      LogStream::Prefix prefix_string("create_sauter_task");
      //#endif
#if ENABLE_NVTX == 1
      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
        std::string("create sauter tasks for dof pair (") +
        std::to_string(i_global_dof_index) + std::string(",") +
        std::to_string(j_global_dof_index) + std::string(")"));
#endif

      /**
       * Iterate over each cell in the support of the basis function
       * associated with the i-th global DoF in the external numbering.
       */
      for (auto kx_cell_iter_pointer : kx_dof_to_cell_topo[i_global_dof_index])
        {
          const typename DoFHandler<dim, spacedim>::cell_iterator
            &kx_cell_iter = *kx_cell_iter_pointer;

          /**
           * @internal Calculate the real support points in the cell \f$K_x\f$
           * via the mapping object. Because the calculated mapping support
           * points are stored in a member variable in the mapping object
           * @p kx_mapping and the current function will called by several
           * producer threads, computation of the mapping support points should
           * be performed in a local copy of @p kx_mapping.
           */
          MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
          kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
          /**
           * Copy the newly calculated support points into @p ScratchData.
           */
          scratch_data.kx_mapping_support_points_in_default_order =
            kx_mapping_copy.get_support_points();
          /**
           * Update the DoF indices.
           */
          kx_cell_iter->get_dof_indices(
            scratch_data.kx_local_dof_indices_in_default_dof_order);

          /**
           * Iterate over each cell in the support of the basis function
           * associated with the j-th global DoF in the external numbering.
           */
          for (auto ky_cell_iter_pointer :
               ky_dof_to_cell_topo[j_global_dof_index])
            {
              const typename DoFHandler<dim, spacedim>::cell_iterator
                &ky_cell_iter = *ky_cell_iter_pointer;

              /**
               * Detect the cell neighboring type based on cell vertex
               * indices.
               */
              CellNeighboringType cell_neighboring_type =
                detect_cell_neighboring_type<dim, spacedim>(
                  method_for_cell_neighboring_type,
                  kx_cell_iter,
                  ky_cell_iter,
                  map_from_kx_boundary_mesh_to_volume_mesh,
                  map_from_ky_boundary_mesh_to_volume_mesh,
                  scratch_data.common_vertex_pair_local_indices);

              /**
               * @internal Similar to making a local copy of @p kx_mapping.
               */
              MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
              ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
              /**
               * Copy the newly calculated support points into @p ScratchData.
               */
              scratch_data.ky_mapping_support_points_in_default_order =
                ky_mapping_copy.get_support_points();
              /**
               * Update the DoF indices.
               */
              ky_cell_iter->get_dof_indices(
                scratch_data.ky_local_dof_indices_in_default_dof_order);

              permute_dofs_and_mapping_support_points_for_sauter_quad(
                scratch_data,
                copy_data,
                cell_neighboring_type,
                kx_cell_iter,
                ky_cell_iter,
                kx_mapping_copy,
                ky_mapping_copy);

              /**
               * @internal Find the array index in the permuted DoF indices of
               * \f$K_x\f$ for the i-th global DoF in the external numbering
               * with respect to the complete DoF set in the DoF handler.
               *
               * \mynote{The permuted cell DoF indices are stored in the
               * @p CopyData.}
               */
              typename std::vector<types::global_dof_index>::const_iterator
                i_iter =
                  std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                            copy_data.kx_local_dof_indices_permuted.end(),
                            i_global_dof_index);
              Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                     ExcInternalError());
              unsigned int i_index =
                i_iter - copy_data.kx_local_dof_indices_permuted.begin();

              /**
               * @internal Find the array index in the permuted DoF indices of
               * \f$K_y\f$ for the j-th global DoF in the external numbering
               * with respect to the complete DoF set in the DoF handler.
               */
              typename std::vector<types::global_dof_index>::const_iterator
                j_iter =
                  std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                            copy_data.ky_local_dof_indices_permuted.end(),
                            j_global_dof_index);
              Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                     ExcInternalError());
              unsigned int j_index =
                j_iter - copy_data.ky_local_dof_indices_permuted.begin();

              //#if ENABLE_DEBUG == 1
              //              deallog << "Create Sauter task: " << fullmat <<
              //              ","
              //                      << i_global_dof_index << "," <<
              //                      j_global_dof_index << ","
              //                      << fullmat_row_index << "," <<
              //                      fullmat_col_index << ","
              //                      << i_index << "," << j_index <<
              //                      std::endl;
              //#endif

              switch (cell_neighboring_type)
                {
                    case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: same panel");
#endif

                      ring_buffer_for_same_panel.add_task(fullmat,
                                                          fullmat_row_index,
                                                          fullmat_col_index,
                                                          i_index,
                                                          j_index,
                                                          0.,
                                                          scratch_data);

                      break;
                    }
                    case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: common edge");
#endif

                      ring_buffer_for_common_edge.add_task(fullmat,
                                                           fullmat_row_index,
                                                           fullmat_col_index,
                                                           i_index,
                                                           j_index,
                                                           0.,
                                                           scratch_data);

                      break;
                    }
                    case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: common vertex");
#endif

                      ring_buffer_for_common_vertex.add_task(fullmat,
                                                             fullmat_row_index,
                                                             fullmat_col_index,
                                                             i_index,
                                                             j_index,
                                                             0.,
                                                             scratch_data);

                      break;
                    }
                    case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: regular");
#endif

                      ring_buffer_for_regular.add_task(fullmat,
                                                       fullmat_row_index,
                                                       fullmat_col_index,
                                                       i_index,
                                                       j_index,
                                                       0.,
                                                       scratch_data);

                      break;
                    }
                    default: {
                      Assert(false, ExcInternalError());

                      break;
                    }
                }
            }
        }
    }


    /**
     * Create Sauter quadrature tasks which are associated with a pair of
     * global DoF indices in the external numbering with respect to the
     * complete set of DoFs in the DoF handler. This version handles the case
     * when a mass matrix (scaled by @p mass_matrix_factor) is to be added to
     * the bilinear form.
     *
     * \mynote{Before performing the Sauter quadrature, we first compute the
     * mass matrix entry if needed. The final result will be the addition of
     * this value and the Sauter quadrature result.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param ring_buffer_for_same_panel
     * @param ring_buffer_for_common_edge
     * @param ring_buffer_for_common_vertex
     * @param ring_buffer_for_regular
     * @param mass_matrix_factor
     * @param fullmat
     * @param fullmat_row_index
     * @param fullmat_col_index
     * @param i_global_dof_index Global DoF index in the external numbering. If only a subset of
     * the complete DoFs in the associated DoF handler is effective or
     * selected, this index is with respect to the complete DoF set.
     * @param j_global_dof_index Global DoF index in the external numbering. If only a subset of
     * the complete DoFs in the associated DoF handler is effective or
     * selected, this index is with respect to the complete DoF set.
     * @param kx_dof_to_cell_topo
     * @param ky_dof_to_cell_topo
     * @param kx_dof_handler
     * @param ky_dof_handler
     * @param kx_mapping
     * @param ky_mapping
     * @param map_from_kx_boundary_mesh_to_volume_mesh
     * @param map_from_ky_boundary_mesh_to_volume_mesh
     * @param method_for_cell_neighboring_type
     * @param mass_matrix_scratch_data
     * @param scratch_data
     * @param copy_data
     * @param is_last_task
     */
    template <int capacity,
              int dim,
              int spacedim,
              typename RangeNumberType = double>
    void
    create_sauter_quadrature_tasks_on_one_pair_of_dofs(
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_same_panel,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_edge,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
        &ring_buffer_for_common_vertex,
      SauterQuadratureTaskRingBuffer<capacity, dim, spacedim, RangeNumberType>
                                           &ring_buffer_for_regular,
      const RangeNumberType                 mass_matrix_factor,
      LAPACKFullMatrixExt<RangeNumberType> *fullmat,
      const unsigned int                    fullmat_row_index,
      const unsigned int                    fullmat_col_index,
      const types::global_dof_index         i_global_dof_index,
      const types::global_dof_index         j_global_dof_index,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
        &kx_dof_to_cell_topo,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                              &ky_dof_to_cell_topo,
      const DoFHandler<dim, spacedim>         &kx_dof_handler,
      const DoFHandler<dim, spacedim>         &ky_dof_handler,
      const MappingQGenericExt<dim, spacedim> &kx_mapping,
      const MappingQGenericExt<dim, spacedim> &ky_mapping,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_kx_boundary_mesh_to_volume_mesh,
      const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                     typename Triangulation<dim + 1, spacedim>::face_iterator>
        &map_from_ky_boundary_mesh_to_volume_mesh,
      const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
      CellWiseScratchDataForMassMatrix<dim, spacedim> &mass_matrix_scratch_data,
      PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
      PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
    {
#if ENABLE_DEBUG == 1
      LogStream::Prefix prefix_string("create_sauter_task");
#endif
#if ENABLE_NVTX == 1
      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
        std::string("create sauter tasks for dof pair (") +
        std::to_string(i_global_dof_index) + std::string(",") +
        std::to_string(j_global_dof_index) + std::string(")"));
#endif

      /**
       * Iterate over each cell in the support of the basis function
       * associated with the i-th global DoF in the external numbering.
       */
      for (auto kx_cell_iter_pointer : kx_dof_to_cell_topo[i_global_dof_index])
        {
          const typename DoFHandler<dim, spacedim>::cell_iterator
                            &kx_cell_iter  = *kx_cell_iter_pointer;
          const unsigned int kx_cell_index = kx_cell_iter->active_cell_index();

          /**
           * @internal Calculate the real support points in the cell \f$K_x\f$
           * via the mapping object. Because the calculated mapping support
           * points are stored in a member variable in the mapping object
           * @p kx_mapping and the current function will called by several
           * producer threads, computation of the mapping support points should
           * be performed in a local copy of @p kx_mapping.
           */
          MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
          kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
          /**
           * Copy the newly calculated support points into @p ScratchData.
           */
          scratch_data.kx_mapping_support_points_in_default_order =
            kx_mapping_copy.get_support_points();
          /**
           * Update the DoF indices.
           */
          kx_cell_iter->get_dof_indices(
            scratch_data.kx_local_dof_indices_in_default_dof_order);

          /**
           * Iterate over each cell in the support of the basis function
           * associated with the j-th global DoF in the external numbering.
           */
          for (auto ky_cell_iter_pointer :
               ky_dof_to_cell_topo[j_global_dof_index])
            {
              const typename DoFHandler<dim, spacedim>::cell_iterator
                                &ky_cell_iter = *ky_cell_iter_pointer;
              const unsigned int ky_cell_index =
                ky_cell_iter->active_cell_index();

              /**
               * Detect the cell neighboring type based on cell vertex
               * indices.
               */
              CellNeighboringType cell_neighboring_type =
                detect_cell_neighboring_type<dim, spacedim>(
                  method_for_cell_neighboring_type,
                  kx_cell_iter,
                  ky_cell_iter,
                  map_from_kx_boundary_mesh_to_volume_mesh,
                  map_from_ky_boundary_mesh_to_volume_mesh,
                  scratch_data.common_vertex_pair_local_indices);

              /**
               * @internal Similar to making a local copy of @p kx_mapping.
               */
              MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
              ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
              /**
               * Copy the newly calculated support points into @p ScratchData.
               */
              scratch_data.ky_mapping_support_points_in_default_order =
                ky_mapping_copy.get_support_points();
              /**
               * Update the DoF indices.
               */
              ky_cell_iter->get_dof_indices(
                scratch_data.ky_local_dof_indices_in_default_dof_order);

              permute_dofs_and_mapping_support_points_for_sauter_quad(
                scratch_data,
                copy_data,
                cell_neighboring_type,
                kx_cell_iter,
                ky_cell_iter,
                kx_mapping_copy,
                ky_mapping_copy);

              /**
               * @internal Before creating the Sauter quadrature task, compute
               * the FEM mass matrix entry if necessary.
               *
               * \myalert{N.B. The DoF handlers for the test and ansatz spaces
               * should be constructed on a same triangulation. Then the
               * following comparison <code>kx_cell_index ==
               * ky_cell_index</code> is meaningful.}
               */
              RangeNumberType mass_matrix_entry = 0.;
              if ((kx_cell_index == ky_cell_index) && (mass_matrix_factor != 0))
                {
#if ENABLE_NVTX == 1
                  IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                    "compute mass matrix entry");
#endif

                  Assert(cell_neighboring_type ==
                           CellNeighboringType::SamePanel,
                         ExcInternalError());

                  /**
                   * @internal Update the finite element values for the test
                   * space.
                   */
                  mass_matrix_scratch_data.fe_values_for_test_space.reinit(
                    kx_cell_iter);

                  /**
                   * @internal Update the finite element values for the trial
                   * space.
                   *
                   * \mynote{N.B. The @p FEValues related to the trial
                   * space must also be updated, since the trial space may
                   * be different from the test space.}
                   */
                  mass_matrix_scratch_data.fe_values_for_trial_space.reinit(
                    ky_cell_iter);

                  const unsigned int n_q_points =
                    mass_matrix_scratch_data.fe_values_for_test_space
                      .get_quadrature()
                      .size();
                  /**
                   * @internal The trial space is on a same triangulation as
                   * the test space and they share a same quadrature object.
                   * Hence, we make assertion about their quadrature sizes.
                   */
                  AssertDimension(n_q_points,
                                  mass_matrix_scratch_data
                                    .fe_values_for_trial_space.get_quadrature()
                                    .size());

                  /**
                   * Get the array index of the global DoF index
                   * @p i_global_dof_index in the current cell \f$K_x\f$.
                   *
                   * \mynote{N.B. The local DoF index in \f$K_x\f$ is
                   * searched from the list of DoF indices held in the
                   * @p ScratchData for BEM. This is valid because the
                   * test and trial spaces associated with the mass matrix
                   * and the BEM bilinear form are the same.
                   *
                   * Since there is no support point permutation during FEM
                   * mass matrix assembly, the cell DoF indices here are in
                   * the default order. BTW, the permuted cell DoF indices are
                   * stored in the @p CopyData.}
                   */
                  auto i_local_dof_iter = std::find(
                    scratch_data.kx_local_dof_indices_in_default_dof_order
                      .begin(),
                    scratch_data.kx_local_dof_indices_in_default_dof_order
                      .end(),
                    i_global_dof_index);
                  Assert(i_local_dof_iter !=
                           scratch_data
                             .kx_local_dof_indices_in_default_dof_order.end(),
                         ExcMessage(
                           std::string("Cannot find the global DoF index ") +
                           std::to_string(i_global_dof_index) +
                           std::string(" in the list of cell DoF indices!")));
                  const unsigned int i_local_dof_index =
                    i_local_dof_iter -
                    scratch_data.kx_local_dof_indices_in_default_dof_order
                      .begin();

                  /**
                   * Get the array index of the global DoF index
                   * @p j_global_dof_index in the current cell \f$K_y\f$.
                   */
                  auto j_local_dof_iter = std::find(
                    scratch_data.ky_local_dof_indices_in_default_dof_order
                      .begin(),
                    scratch_data.ky_local_dof_indices_in_default_dof_order
                      .end(),
                    j_global_dof_index);
                  Assert(j_local_dof_iter !=
                           scratch_data
                             .ky_local_dof_indices_in_default_dof_order.end(),
                         ExcMessage(
                           std::string("Cannot find the global DoF index ") +
                           std::to_string(j_global_dof_index) +
                           std::string(" in the list of cell DoF indices!")));
                  const unsigned int j_local_dof_index =
                    j_local_dof_iter -
                    scratch_data.ky_local_dof_indices_in_default_dof_order
                      .begin();

                  /**
                   * @internal Iterate over each quadrature point.
                   */
                  for (unsigned int q = 0; q < n_q_points; q++)
                    {
                      mass_matrix_entry +=
                        mass_matrix_scratch_data.fe_values_for_test_space
                          .shape_value(i_local_dof_index, q) *
                        mass_matrix_scratch_data.fe_values_for_trial_space
                          .shape_value(j_local_dof_index, q) *
                        mass_matrix_scratch_data.fe_values_for_test_space.JxW(
                          q);
                    }

                  mass_matrix_entry *= mass_matrix_factor;

#if ENABLE_DEBUG == 1
                  deallog << "kx=" << kx_cell_index << ", ky=" << ky_cell_index
                          << ", i=" << i_global_dof_index
                          << ", j=" << j_global_dof_index
                          << ", Mij=" << mass_matrix_entry << std::endl;
#endif
                }

              /**
               * @internal Now we start to create the Sauter quadrature task.
               */

              /**
               * @internal Find the array index in the permuted DoF indices of
               * \f$K_x\f$ for the i-th global DoF in the external numbering
               * with respect to the complete DoF set in the DoF handler.
               *
               * \mynote{The permuted cell DoF indices are stored in the
               * @p CopyData.}
               */
              typename std::vector<types::global_dof_index>::const_iterator
                i_iter =
                  std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                            copy_data.kx_local_dof_indices_permuted.end(),
                            i_global_dof_index);
              Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                     ExcInternalError());
              unsigned int i_index =
                i_iter - copy_data.kx_local_dof_indices_permuted.begin();

              /**
               * @internal Find the array index in the permuted DoF indices of
               * \f$K_y\f$ for the j-th global DoF in the external numbering
               * with respect to the complete DoF set in the DoF handler.
               */
              typename std::vector<types::global_dof_index>::const_iterator
                j_iter =
                  std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                            copy_data.ky_local_dof_indices_permuted.end(),
                            j_global_dof_index);
              Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                     ExcInternalError());
              unsigned int j_index =
                j_iter - copy_data.ky_local_dof_indices_permuted.begin();

              //#if ENABLE_DEBUG == 1
              //              deallog << "Create Sauter task: " << fullmat <<
              //              ","
              //                      << i_global_dof_index << "," <<
              //                      j_global_dof_index << ","
              //                      << fullmat_row_index << "," <<
              //                      fullmat_col_index << ","
              //                      << i_index << "," << j_index
              //                      << ",mass=" << mass_matrix_entry <<
              //                      std::endl;
              //#endif

              switch (cell_neighboring_type)
                {
                    case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: same panel");
#endif

                      ring_buffer_for_same_panel.add_task(fullmat,
                                                          fullmat_row_index,
                                                          fullmat_col_index,
                                                          i_index,
                                                          j_index,
                                                          mass_matrix_entry,
                                                          scratch_data);

                      break;
                    }
                    case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: common edge");
#endif

                      ring_buffer_for_common_edge.add_task(fullmat,
                                                           fullmat_row_index,
                                                           fullmat_col_index,
                                                           i_index,
                                                           j_index,
                                                           mass_matrix_entry,
                                                           scratch_data);

                      break;
                    }
                    case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: common vertex");
#endif

                      ring_buffer_for_common_vertex.add_task(fullmat,
                                                             fullmat_row_index,
                                                             fullmat_col_index,
                                                             i_index,
                                                             j_index,
                                                             mass_matrix_entry,
                                                             scratch_data);

                      break;
                    }
                    case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                      IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                        "add sauter task: regular");
#endif

                      ring_buffer_for_regular.add_task(fullmat,
                                                       fullmat_row_index,
                                                       fullmat_col_index,
                                                       i_index,
                                                       j_index,
                                                       mass_matrix_entry,
                                                       scratch_data);

                      break;
                    }
                    default: {
                      Assert(false, ExcInternalError());

                      break;
                    }
                }
            }
        }
    }


    /**
     * This version is used for assembling a row or column vector in a far field
     * \hmat.
     *
     * \mynote{The cell neighboring type for each pair of cells is @p regular.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param task_buffer
     * @param vector_element_index
     * @param i_global_dof_index
     * @param j_global_dof_index
     * @param kx_dof_to_cell_topo
     * @param ky_dof_to_cell_topo
     * @param kx_dof_handler
     * @param ky_dof_handler
     * @param kx_mapping
     * @param ky_mapping
     * @param scratch_data
     * @param copy_data
     * @return Number of Sauter quadrature tasks for the pair of DoFs.
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    unsigned int
    create_sauter_quadrature_tasks_on_one_pair_of_dofs(
      SauterQuadratureTaskBufferForVector<dim, spacedim, RangeNumberType>
                                   &task_buffer,
      const types::global_dof_index i_global_dof_index,
      const types::global_dof_index j_global_dof_index,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
        &kx_dof_to_cell_topo,
      const std::vector<
        std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                              &ky_dof_to_cell_topo,
      const DoFHandler<dim, spacedim>         &kx_dof_handler,
      const DoFHandler<dim, spacedim>         &ky_dof_handler,
      const MappingQGenericExt<dim, spacedim> &kx_mapping,
      const MappingQGenericExt<dim, spacedim> &ky_mapping,
      PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
      PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
    {
      unsigned int number_of_tasks_for_dof_pair = 0;

      /**
       * Iterate over each cell in the support of the basis function
       * associated with the i-th global DoF in the external numbering.
       */
      for (auto kx_cell_iter_pointer : kx_dof_to_cell_topo[i_global_dof_index])
        {
          const typename DoFHandler<dim, spacedim>::cell_iterator
            &kx_cell_iter = *kx_cell_iter_pointer;

          /**
           * @internal Calculate the real support points in the cell \f$K_x\f$
           * via the mapping object. Because the calculated mapping support
           * points are stored in a member variable in the mapping object
           * @p kx_mapping and the current function will called by several
           * producer threads, computation of the mapping support points should
           * be performed in a local copy of @p kx_mapping.
           */
          MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
          kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
          /**
           * Copy the newly calculated support points into @p ScratchData.
           */
          scratch_data.kx_mapping_support_points_in_default_order =
            kx_mapping_copy.get_support_points();
          /**
           * Update the DoF indices.
           */
          kx_cell_iter->get_dof_indices(
            scratch_data.kx_local_dof_indices_in_default_dof_order);

          /**
           * Iterate over each cell in the support of the basis function
           * associated with the j-th global DoF in the external numbering.
           */
          for (auto ky_cell_iter_pointer :
               ky_dof_to_cell_topo[j_global_dof_index])
            {
              const typename DoFHandler<dim, spacedim>::cell_iterator
                &ky_cell_iter = *ky_cell_iter_pointer;

              /**
               * @internal Similar to making a local copy of @p kx_mapping.
               */
              MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
              ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
              /**
               * Copy the newly calculated support points into @p ScratchData.
               */
              scratch_data.ky_mapping_support_points_in_default_order =
                ky_mapping_copy.get_support_points();
              /**
               * Update the DoF indices.
               */
              ky_cell_iter->get_dof_indices(
                scratch_data.ky_local_dof_indices_in_default_dof_order);

              permute_dofs_and_mapping_support_points_for_sauter_quad(
                scratch_data,
                copy_data,
                CellNeighboringType::Regular,
                kx_cell_iter,
                ky_cell_iter,
                kx_mapping_copy,
                ky_mapping_copy);

              /**
               * @internal Find the array index in the permuted DoF indices of
               * \f$K_x\f$ for the i-th global DoF in the external numbering
               * with respect to the complete DoF set in the DoF handler.
               *
               * \mynote{The permuted cell DoF indices are stored in the
               * @p CopyData.}
               */
              typename std::vector<types::global_dof_index>::const_iterator
                i_iter =
                  std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                            copy_data.kx_local_dof_indices_permuted.end(),
                            i_global_dof_index);
              Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                     ExcInternalError());
              unsigned int i_index =
                i_iter - copy_data.kx_local_dof_indices_permuted.begin();

              /**
               * @internal Find the array index in the permuted DoF indices of
               * \f$K_y\f$ for the j-th global DoF in the external numbering
               * with respect to the complete DoF set in the DoF handler.
               */
              typename std::vector<types::global_dof_index>::const_iterator
                j_iter =
                  std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                            copy_data.ky_local_dof_indices_permuted.end(),
                            j_global_dof_index);
              Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                     ExcInternalError());
              unsigned int j_index =
                j_iter - copy_data.ky_local_dof_indices_permuted.begin();

              task_buffer.add_task(i_index, j_index, scratch_data);

              number_of_tasks_for_dof_pair++;
            }
        }

      return number_of_tasks_for_dof_pair;
    }
  } // namespace CUDAWrappers


  /**
   * Precalculate surface Jacobians and normal vectors to be used in the
   * Sauter quadrature.
   *
   * \alert{Computation of the Jacobian matrix as well as related quantities
   * such as normal vector, covariant transformation matrix, metric tensor,
   * etc., is related to the mapping object and has nothing to do with the
   * finite element. A mapping object is used to describe geometry, while a
   * finite element object is used to describe the ansatz or test functions.}
   *
   * \mynote{This version involves @p PairCellWiseScratchData.}
   *
   * @param scratch
   * @param data
   * @param cell_neighboring_type
   * @param active_quad_rule
   */
  template <int dim, int spacedim, typename RangeNumberType = double>
  __host__ void
  calc_jacobian_normals_for_sauter_quad(
    const CellNeighboringType                        cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch,
    const QGauss<dim * 2>                                   &active_quad_rule)
  {
    switch (cell_neighboring_type)
      {
          case SamePanel: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<dim>::vertices_per_cell,
                   ExcInternalError());

            /**
             * Precalculate surface Jacobians and normal vectors at each
             * quadrature point in the current pair of cells.
             * \mynote{They are stored in the @p scratch data.}
             */
            for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_jacobians_same_panel(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.kx_mapping_support_points_permuted,
                        scratch.kx_normals_same_panel(k3_index, q));

                    scratch.ky_jacobians_same_panel(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.ky_mapping_support_points_permuted,
                        scratch.ky_normals_same_panel(k3_index, q));

                    scratch.kx_quad_points_same_panel(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.kx_mapping_shape_value_table_for_same_panel,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_quad_points_same_panel(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.ky_mapping_shape_value_table_for_same_panel,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonEdge: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<2>::vertices_per_face,
                   ExcInternalError());

            // Precalculate surface Jacobians and normal vectors.
            for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_jacobians_common_edge(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.kx_mapping_support_points_permuted,
                        scratch.kx_normals_common_edge(k3_index, q));

                    scratch.ky_jacobians_common_edge(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.ky_mapping_support_points_permuted,
                        scratch.ky_normals_common_edge(k3_index, q));

                    scratch.kx_quad_points_common_edge(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.kx_mapping_shape_value_table_for_common_edge,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_quad_points_common_edge(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.ky_mapping_shape_value_table_for_common_edge,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonVertex: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 1,
                   ExcInternalError());

            // Precalculate surface Jacobians and normal vectors.
            for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_jacobians_common_vertex(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.kx_mapping_support_points_permuted,
                        scratch.kx_normals_common_vertex(k3_index, q));

                    scratch.ky_jacobians_common_vertex(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.ky_mapping_support_points_permuted,
                        scratch.ky_normals_common_vertex(k3_index, q));

                    scratch.kx_quad_points_common_vertex(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_value_table_for_common_vertex,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_quad_points_common_vertex(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_value_table_for_common_vertex,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case Regular: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 0,
                   ExcInternalError());

            // Precalculate surface Jacobians and normal vectors.
            for (unsigned int q = 0; q < active_quad_rule.size(); q++)
              {
                scratch.kx_jacobians_regular(0, q) =
                  surface_jacobian_det_and_normal_vector(
                    0,
                    q,
                    bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
                    scratch.kx_mapping_support_points_permuted,
                    scratch.kx_normals_regular(0, q));

                scratch.ky_jacobians_regular(0, q) =
                  surface_jacobian_det_and_normal_vector(
                    0,
                    q,
                    bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
                    scratch.ky_mapping_support_points_permuted,
                    scratch.ky_normals_regular(0, q));

                scratch.kx_quad_points_regular(0, q) =
                  transform_unit_to_permuted_real_cell(
                    0,
                    q,
                    bem_values.kx_mapping_shape_value_table_for_regular,
                    scratch.kx_mapping_support_points_permuted);

                scratch.ky_quad_points_regular(0, q) =
                  transform_unit_to_permuted_real_cell(
                    0,
                    q,
                    bem_values.ky_mapping_shape_value_table_for_regular,
                    scratch.ky_mapping_support_points_permuted);
              }

            break;
          }
          default: {
            Assert(false, ExcNotImplemented());
          }
      }
  }


  /**
   *
   *
   * @param scratch
   * @param cell_neighboring_type
   * @param bem_values
   * @param active_quad_rule
   */
  template <int dim, int spacedim, typename RangeNumberType = double>
  __host__ void
  calc_covariant_transformations(
    const CellNeighboringType                        cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch,
    const QGauss<dim * 2>                                   &active_quad_rule)
  {
    switch (cell_neighboring_type)
      {
          case SamePanel: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<dim>::vertices_per_cell,
                   ExcInternalError());

            for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_covariants_same_panel(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_covariants_same_panel(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonEdge: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<2>::vertices_per_face,
                   ExcInternalError());

            for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_covariants_common_edge(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_covariants_common_edge(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonVertex: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 1,
                   ExcInternalError());

            for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_covariants_common_vertex(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_covariants_common_vertex(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case Regular: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 0,
                   ExcInternalError());

            for (unsigned int q = 0; q < active_quad_rule.size(); q++)
              {
                scratch.kx_covariants_regular(0, q) =
                  surface_covariant_transformation(
                    0,
                    q,
                    bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
                    scratch.kx_mapping_support_points_permuted);

                scratch.ky_covariants_regular(0, q) =
                  surface_covariant_transformation(
                    0,
                    q,
                    bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
                    scratch.ky_mapping_support_points_permuted);
              }

            break;
          }
          default: {
            Assert(false, ExcNotImplemented());
          }
      }
  }


  /**
   * Apply the Sauter's quadrature rule to the kernel function pulled back to
   * the Sauter's parametric space. The result will also be multiplied by a
   * factor. This version uses the precalculated @p BEMValues.
   *
   * @param quad_rule
   * @param f
   * @param factor
   * @param component
   * @return
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  __host__ RangeNumberType
  ApplyQuadratureUsingBEMValues(
    const Quadrature<dim * 2> &quad_rule,
    const IdeoBEM::CUDAWrappers::KernelPulledbackToSauterSpace<
      dim,
      spacedim,
      KernelFunctionType,
      RangeNumberType>                                            &f,
    const RangeNumberType                                          factor,
    const BEMValues<dim, spacedim, RangeNumberType>               &bem_values,
    const PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    unsigned int component = 0)
  {
    RangeNumberType result = 0.;

    const std::vector<double> &quad_weights = quad_rule.get_weights();

    for (unsigned int q = 0; q < quad_rule.size(); q++)
      {
        // Evaluate the integrand with precalculated shape values and shape
        // gradient matrices.
        result +=
          f.value(q, bem_values, scratch_data, component) * quad_weights[q];
      }

    return result * factor;
  }


  template <int dim, int spacedim, typename RangeNumberType = double>
  const QGauss<dim * 2> &
  select_sauter_quad_rule_from_bem_values(
    const CellNeighboringType                        cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values)
  {
    switch (cell_neighboring_type)
      {
          case SamePanel: {
            return bem_values.quad_rule_for_same_panel;
          }
          case CommonEdge: {
            return bem_values.quad_rule_for_common_edge;
          }
          case CommonVertex: {
            return bem_values.quad_rule_for_common_vertex;
          }
          case Regular: {
            return bem_values.quad_rule_for_regular;
          }
          default: {
            Assert(false, ExcNotImplemented());

            return bem_values.quad_rule_for_same_panel;
          }
      }
  }


  /**
   * Perform the Galerkin-BEM double integral with respect to a boundary
   * integral operator (represented as the input kernel function) using
   * Sauter's quadrature for the DoFs in a pair of cells \f$K_x\f$ and
   * \f$K_y\f$.
   *
   * \mynote{When the boundary integral operator is the hyper singular
   * operator, the regularized bilinear form in \f$\mathbb{R}^3\f$ is \f[
   * \left\langle Du,v \right\rangle_{\Gamma} =
   * \frac{1}{4\pi}\int_{\Gamma}\int_{\Gamma}
   * \frac{\underline{\curl}_{\Gamma}u(y)\cdot\underline{\curl}_{\Gamma}v(x)}{\abs{x-y}}
   * ds_x ds_y.
   * \f]
   * It needs special treatment, i.e. calculation of the surface curl of the
   * basis functions for ansatz and test functions.}
   *
   * \mynote{This is only applicable to the case when a full matrix for a
   * boundary integral operator is to be constructed. Therefore, this function
   * is only meaningful for algorithm verification. In real application, an
   * \hmatrix should be built. Also note that even for the near field matrix
   * node in an \hmatrix, which must be a full matrix, the Sauter's quadrature
   * is built in the paradigm of "on a pair of DoFs" instead of "on a pair of
   * cells". This is because the two cluster trees associated with an \hmatrix
   * use partition by DoF support points in stead of partition by cells.
   *
   * Before calling this function, the mapping support points and DoF indices
   * in \f$K_x\f$ should have been calculated.}
   *
   * @param kernel
   * @param factor
   * @param kx_cell_iter
   * @param ky_cell_iter
   * @param kx_mapping
   * @param ky_mapping
   * @param map_from_kx_mesh_to_volume_mesh
   * @param map_from_ky_mesh_to_volume_mesh
   * @param method_for_cell_neighboring_type
   * @param bem_values
   * @param scratch
   * @param data
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  void
  sauter_assemble_on_one_pair_of_cells(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const typename DoFHandler<dim, spacedim>::active_cell_iterator
      &kx_cell_iter,
    const typename DoFHandler<dim, spacedim>::active_cell_iterator
                                            &ky_cell_iter,
    const MappingQGenericExt<dim, spacedim> &kx_mapping,
    const MappingQGenericExt<dim, spacedim> &ky_mapping,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_kx_boundary_mesh_to_volume_mesh,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_ky_boundary_mesh_to_volume_mesh,
    const BEMTools::DetectCellNeighboringTypeMethod
      method_for_cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType>         &bem_values,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
  {
    /**
     * Detect the cell neighboring type based on cell vertex indices.
     */
    CellNeighboringType cell_neighboring_type =
      detect_cell_neighboring_type<dim, spacedim>(
        method_for_cell_neighboring_type,
        kx_cell_iter,
        ky_cell_iter,
        map_from_kx_boundary_mesh_to_volume_mesh,
        map_from_ky_boundary_mesh_to_volume_mesh,
        scratch_data.common_vertex_pair_local_indices);

    /**
     * Create a quadrature rule, which depends on the cell neighboring type.
     */
    const QGauss<dim * 2> active_quad_rule =
      select_sauter_quad_rule_from_bem_values(cell_neighboring_type,
                                              bem_values);

    const FiniteElement<dim, spacedim> &kx_fe = kx_cell_iter->get_fe();
    const FiniteElement<dim, spacedim> &ky_fe = ky_cell_iter->get_fe();

    const unsigned int kx_n_dofs = kx_fe.dofs_per_cell;
    const unsigned int ky_n_dofs = ky_fe.dofs_per_cell;

    /**
     * @internal Calculate the mapping support points and DoF indices in the
     * cell \f$K_y\f$ via the mapping object. N.B. Corresponding data for
     * \f$K_x\f$ have already been calculated in the outer loop of
     * @p assemble_bem_full_matrix.
     *
     * Because the calculated mapping support points are stored in a member
     * variable in the mapping object @p ky_mapping and several instances of
     * this function will be called by @p assemble_bem_full_matrix in parallel,
     * @p ky_mapping is passed as const reference into this function and a
     * local copy @p ky_mapping_copy of it is made.
     */
    MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
    ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
    scratch_data.ky_mapping_support_points_in_default_order =
      ky_mapping_copy.get_support_points();
    ky_cell_iter->get_dof_indices(
      scratch_data.ky_local_dof_indices_in_default_dof_order);

    permute_dofs_and_mapping_support_points_for_sauter_quad(
      scratch_data,
      copy_data,
      cell_neighboring_type,
      kx_cell_iter,
      ky_cell_iter,
      kx_mapping,
      ky_mapping_copy);

    calc_jacobian_normals_for_sauter_quad(cell_neighboring_type,
                                          bem_values,
                                          scratch_data,
                                          active_quad_rule);

    /**
     * When the bilinear form for the hyper singular operator is evaluated,
     * the covariant transformation is required.
     */
    if (kernel.kernel_type == HyperSingularRegular)
      {
        calc_covariant_transformations(cell_neighboring_type,
                                       bem_values,
                                       scratch_data,
                                       active_quad_rule);
      }

    /**
     *  Clear the local matrix in case that it is reused from another
     *  finished task. N.B. Its memory has already been allocated in the
     *  constructor of @p CellPairWisePerTaskData.
     */
    copy_data.local_pair_cell_matrix.reinit(
      copy_data.kx_local_dof_indices_permuted.size(),
      copy_data.ky_local_dof_indices_permuted.size());

    // Iterate over DoFs for test function space in \f$K_x\f$.
    for (unsigned int i = 0; i < kx_n_dofs; i++)
      {
        // Iterate over DoFs for trial function space in \f$K_y\f$.
        for (unsigned int j = 0; j < ky_n_dofs; j++)
          {
            // Pullback the kernel function to unit cell.
            IdeoBEM::CUDAWrappers::KernelPulledbackToUnitCell<
              dim,
              spacedim,
              KernelFunctionType,
              RangeNumberType>
              kernel_pullback_on_unit(kernel, cell_neighboring_type, i, j);

            // Pullback the kernel function to Sauter parameter space.
            IdeoBEM::CUDAWrappers::KernelPulledbackToSauterSpace<
              dim,
              spacedim,
              KernelFunctionType,
              RangeNumberType>
              kernel_pullback_on_sauter(kernel_pullback_on_unit,
                                        cell_neighboring_type);

            // Apply Sauter numerical quadrature.
            copy_data.local_pair_cell_matrix(i, j) =
              ApplyQuadratureUsingBEMValues(active_quad_rule,
                                            kernel_pullback_on_sauter,
                                            kernel_factor,
                                            bem_values,
                                            scratch_data);
          }
      }
  }


  /**
   * Perform Galerkin-BEM double integral with respect to a given kernel on a
   * pair of DoFs \f$(i, j)\f$ using the Sauter quadrature.
   *
   * Assume \f$\mathcal{K}_i\f$ is the collection of cells sharing the DoF
   * support point \f$i\f$ and \f$\mathcal{K}_j\f$ is the collection of cells
   * sharing the DoF support point \f$j\f$. Then Galerkin-BEM double integral
   * will be over each cell pair which is comprised of an arbitrary cell in
   * \f$\mathcal{K}_i\f$ and an arbitrary cell in \f$\mathcal{K}_j\f$.
   *
   * \mynote{The DoF indices \f$(i, j)\f$ are global, i.e. global in the sense
   * of all DoFs contained in the associated DoF handlers. In mixed boundary
   * value problem, when dealing with the Dirichlet function space, only a
   * subset of these DoFs are selected. Hence, the DoF indices in an \hmat are
   * local, i.e. local in the sense of DoF indices renumbered for the subset.
   * When coming to this function for Sauter quadrature, the global DoF
   * indices should be used.}
   *
   * @param kernel
   * @param factor
   * @param i Global DoF index in the external numbering. If only a subset of
   * the complete DoFs in the associated DoF handler is effective or selected,
   * this index is with respect to the complete DoF set.
   * @param j Global DoF index in the external numbering. If only a subset of
   * the complete DoFs in the associated DoF handler is effective or selected,
   * this index is with respect to the complete DoF set.
   * @param kx_dof_to_cell_topo
   * @param ky_dof_to_cell_topo
   * @param bem_values
   * @param kx_dof_handler
   * @param ky_dof_handler
   * @param kx_mapping
   * @param ky_mapping
   * @param map_from_kx_boundary_mesh_to_volume_mesh
   * @param map_from_ky_boundary_mesh_to_volume_mesh
   * @param method_for_cell_neighboring_type
   * @param scratch_data
   * @param copy_data
   * @return
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  RangeNumberType
  sauter_assemble_on_one_pair_of_dofs(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const types::global_dof_index                        i,
    const types::global_dof_index                        j,
    const std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
      &kx_dof_to_cell_topo,
    const std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                                    &ky_dof_to_cell_topo,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                            &bem_values_gpu,
    const DoFHandler<dim, spacedim>         &kx_dof_handler,
    const DoFHandler<dim, spacedim>         &ky_dof_handler,
    const MappingQGenericExt<dim, spacedim> &kx_mapping,
    const MappingQGenericExt<dim, spacedim> &ky_mapping,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_kx_boundary_mesh_to_volume_mesh,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_ky_boundary_mesh_to_volume_mesh,
    const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    IdeoBEM::CUDAWrappers::
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                                                            &scratch_data_gpu,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
  {
#if ENABLE_NVTX == 1
    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
      "sauter_assemble_on_one_pair_of_dofs");
#endif

    /**
     * @internal This result variable accumulates all contributions from pairs
     * of cells related to the global DoF indices @p (i,j).
     */
    RangeNumberType double_integral = 0.0;

    /**
     * Iterate over each cell in the support of the basis function associated
     * with the i-th DoF.
     */
    for (auto kx_cell_iter_pointer : kx_dof_to_cell_topo[i])
      {
        const typename DoFHandler<dim, spacedim>::cell_iterator &kx_cell_iter =
          *kx_cell_iter_pointer;

        /**
         * @internal Calculate the real support points in the cell \f$K_x\f$
         * via the mapping object. Because the calculated mapping support
         * points are
         * stored in a member variable in the mapping object @p kx_mapping and
         * the caller @p fill_hmatrix_leaf_node_with_aca_plus of this function
         * will be executed in several TBB threads, computation of the mapping
         * support points should be performed in thread dependent local copies
         * of @p kx_mapping.
         */
        MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
        kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
        /**
         * Copy the newly calculated support points into @p ScratchData.
         */
        scratch_data.kx_mapping_support_points_in_default_order =
          kx_mapping_copy.get_support_points();
        /**
         * Update the DoF indices.
         */
        kx_cell_iter->get_dof_indices(
          scratch_data.kx_local_dof_indices_in_default_dof_order);

        /**
         * Iterate over each cell in the support of the basis function
         * associated with the j-th DoF.
         */
        for (auto ky_cell_iter_pointer : ky_dof_to_cell_topo[j])
          {
            const typename DoFHandler<dim, spacedim>::cell_iterator
              &ky_cell_iter = *ky_cell_iter_pointer;

            /**
             * Detect the cell neighboring type based on cell vertex indices.
             */
            CellNeighboringType cell_neighboring_type =
              detect_cell_neighboring_type<dim, spacedim>(
                method_for_cell_neighboring_type,
                kx_cell_iter,
                ky_cell_iter,
                map_from_kx_boundary_mesh_to_volume_mesh,
                map_from_ky_boundary_mesh_to_volume_mesh,
                scratch_data.common_vertex_pair_local_indices);

            /**
             * @internal Similar to making a local copy of @p kx_mapping.
             */
            MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
            ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
            /**
             * Copy the newly calculated support points into @p ScratchData.
             */
            scratch_data.ky_mapping_support_points_in_default_order =
              ky_mapping_copy.get_support_points();
            /**
             * Update the DoF indices.
             */
            ky_cell_iter->get_dof_indices(
              scratch_data.ky_local_dof_indices_in_default_dof_order);

            permute_dofs_and_mapping_support_points_for_sauter_quad(
              scratch_data,
              copy_data,
              cell_neighboring_type,
              kx_cell_iter,
              ky_cell_iter,
              kx_mapping_copy,
              ky_mapping_copy);

            /**
             * @internal Copy the scratch data and copy data from CPU to GPU.
             */
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            scratch_data_gpu.assign_from_host(scratch_data);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "copy scratch data from host to device");
#endif

            /**
             * @internal Determine thread block dimensions.
             */
            dim3 blocks_rect;
            dim3 threads_rect;
            IdeoBEM::CUDAWrappers::
              configure_thread_blocks_for_sauter_quadrature_points(
                cell_neighboring_type, bem_values, blocks_rect, threads_rect);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            switch (cell_neighboring_type)
              {
                  case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: same panel");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_same_panel<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: common edge");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_edge<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: common vertex");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_vertex<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: regular");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_regular<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());

                    break;
                  }
              }

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "calculate Jacobians and normal vectors");
#endif

            /**
             * When the bilinear form for the hyper singular operator is
             * evaluated, the covariant transformation is required.
             */
            if (kernel.kernel_type == HyperSingularRegular)
              {
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
                scratch_data.timer.start();
#endif
                switch (cell_neighboring_type)
                  {
                      case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: same panel");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_same_panel<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: common edge");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_edge<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: common vertex");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_vertex<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: regular");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_regular<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      default: {
                        Assert(false, ExcInternalError());

                        break;
                      }
                  }

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
                scratch_data.timer.stop();
                print_wall_time(scratch_data.log_stream,
                                scratch_data.timer,
                                "calculate covariant matrices");
#endif
              }

            /**
             * Find the index of the i-th DoF in the permuted DoF indices of
             * \f$K_x\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              i_iter =
                std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                          copy_data.kx_local_dof_indices_permuted.end(),
                          i);
            Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int i_index =
              i_iter - copy_data.kx_local_dof_indices_permuted.begin();

            /**
             * Find the index of the j-th DoF in the permuted DoF indices of
             * \f$K_y\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              j_iter =
                std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                          copy_data.ky_local_dof_indices_permuted.end(),
                          j);
            Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int j_index =
              j_iter - copy_data.ky_local_dof_indices_permuted.begin();

            // Apply 4d Sauter numerical quadrature on the GPU device.
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            switch (cell_neighboring_type)
              {
                  case BEMTools::CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: same panel");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesSamePanel<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: common edge");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonEdge<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: common vertex");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonVertex<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: regular");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesRegular<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());
                    break;
                  }
              }

            /**
             * @internal Extract quadrature results accumulated from all
             * thread blocks.
             */
            cudaError_t error_code = cudaMemcpyAsync(
              (void *)(scratch_data.quad_values_in_thread_blocks),
              (void *)(scratch_data_gpu.quad_values_in_thread_blocks),
              sizeof(RangeNumberType) * blocks_rect.x,
              cudaMemcpyDeviceToHost,
              scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            for (unsigned int t = 0; t < blocks_rect.x; t++)
              {
                double_integral += scratch_data.quad_values_in_thread_blocks[t];
              }
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "apply Sauter quadrature");
#endif
          }
      }

    return double_integral;
  }


  /**
   * Perform Galerkin-BEM double integral with respect to a given kernel on a
   * pair of DoFs \f$(i, j)\f$ using the Sauter quadrature. In addition,
   * entries of the mass matrix are calculated and added into this matrix.
   * Because the evaluation of mass matrix entries involves integrating the
   * product of two shape functions with compact support, there is no
   * long-range interaction between them like the case in BEM. Hence, these
   * mass matrix entries are added into the near field \hmat node.
   *
   * Assume \f$\mathcal{K}_i\f$ is the collection of cells sharing the DoF
   * support point \f$i\f$ and \f$\mathcal{K}_j\f$ is the collection of cells
   * sharing the DoF support point \f$j\f$. Then Galerkin-BEM double integral
   * will be over each cell pair which is comprised of an arbitrary cell in
   * \f$\mathcal{K}_i\f$ and an arbitrary cell in \f$\mathcal{K}_j\f$.
   *
   * \mynote{The DoF indices \f$(i, j)\f$ are global, i.e. global in the sense
   * of all DoFs contained in the associated DoF handlers. In mixed boundary
   * value problem, when dealing with the Dirichlet function space, only a
   * subset of these DoFs are selected. Hence, the DoF indices in an \hmat are
   * local, i.e. local in the sense of DoF indices renumbered for the subset.
   * When coming to this function for Sauter quadrature, the global DoF
   * indices should be used.}
   *
   *
   * @param kernel
   * @param kernel_factor
   * @param mass_matrix_factor
   * @param i Global DoF index in the external numbering. If only a subset of
   * the complete DoFs in the associated DoF handler is effective or selected,
   * this index is with respect to the complete DoF set.
   * @param j Global DoF index in the external numbering. If only a subset of
   * the complete DoFs in the associated DoF handler is effective or selected,
   * this index is with respect to the complete DoF set.
   * @param kx_dof_to_cell_topo
   * @param ky_dof_to_cell_topo
   * @param bem_values
   * @param kx_dof_handler
   * @param ky_dof_handler
   * @param kx_mapping
   * @param ky_mapping
   * @param map_from_kx_boundary_mesh_to_volume_mesh
   * @param map_from_ky_boundary_mesh_to_volume_mesh
   * @param method_for_cell_neighboring_type
   * @param mass_matrix_scratch_data
   * @param scratch_data
   * @param copy_data
   * @return
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  RangeNumberType
  sauter_assemble_on_one_pair_of_dofs(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const RangeNumberType                                mass_matrix_factor,
    const types::global_dof_index                        i,
    const types::global_dof_index                        j,
    const std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
      &kx_dof_to_cell_topo,
    const std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                                    &ky_dof_to_cell_topo,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                            &bem_values_gpu,
    const DoFHandler<dim, spacedim>         &kx_dof_handler,
    const DoFHandler<dim, spacedim>         &ky_dof_handler,
    const MappingQGenericExt<dim, spacedim> &kx_mapping,
    const MappingQGenericExt<dim, spacedim> &ky_mapping,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_kx_boundary_mesh_to_volume_mesh,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_ky_boundary_mesh_to_volume_mesh,
    const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
    CellWiseScratchDataForMassMatrix<dim, spacedim> &mass_matrix_scratch_data,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    IdeoBEM::CUDAWrappers::
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                                                            &scratch_data_gpu,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
  {
#if ENABLE_NVTX == 1
    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
      "sauter_assemble_on_one_pair_of_dofs with mass matrix");
#endif

    /**
     * @internal This result variable accumulates all contributions from pairs
     * of cells related to the global DoF indices @p (i,j).
     */
    RangeNumberType double_integral = 0.0;

    /**
     * Iterate over each cell in the support of the basis function associated
     * with the i-th DoF.
     */
    for (auto kx_cell_iter_pointer : kx_dof_to_cell_topo[i])
      {
        const typename DoFHandler<dim, spacedim>::cell_iterator &kx_cell_iter =
          *kx_cell_iter_pointer;
        const unsigned int kx_cell_index = kx_cell_iter->active_cell_index();

        /**
         * @internal Calculate the real support points in the cell \f$K_x\f$
         * via the mapping object. Because the calculated mapping support
         * points are
         * stored in a member variable in the mapping object @p kx_mapping and
         * the caller @p fill_hmatrix_leaf_node_with_aca_plus of this function
         * will be executed in several TBB threads, computation of the mapping
         * support points should be performed in thread dependent local copies
         * of @p kx_mapping.
         */
        MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
        kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
        /**
         * Copy the newly calculated support points into @p ScratchData.
         */
        scratch_data.kx_mapping_support_points_in_default_order =
          kx_mapping_copy.get_support_points();
        /**
         * Update the DoF indices.
         */
        kx_cell_iter->get_dof_indices(
          scratch_data.kx_local_dof_indices_in_default_dof_order);

        /**
         * Iterate over each cell in the support of the basis function
         * associated with the j-th DoF.
         */
        for (auto ky_cell_iter_pointer : ky_dof_to_cell_topo[j])
          {
            const typename DoFHandler<dim, spacedim>::cell_iterator
                              &ky_cell_iter = *ky_cell_iter_pointer;
            const unsigned int ky_cell_index =
              ky_cell_iter->active_cell_index();

            /**
             * Detect the cell neighboring type based on cell vertex indices.
             */
            CellNeighboringType cell_neighboring_type =
              detect_cell_neighboring_type<dim, spacedim>(
                method_for_cell_neighboring_type,
                kx_cell_iter,
                ky_cell_iter,
                map_from_kx_boundary_mesh_to_volume_mesh,
                map_from_ky_boundary_mesh_to_volume_mesh,
                scratch_data.common_vertex_pair_local_indices);

            /**
             * Calculate the real support points in the cell \f$K_y\f$ via the
             * mapping object. Since such data will be held and updated
             * in-situ in the mapping object, which has been passed into this
             * working function by const reference, a copy of it should be
             * made.
             */
            MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
            ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
            /**
             * Copy the newly calculated support points into @p ScratchData.
             */
            scratch_data.ky_mapping_support_points_in_default_order =
              ky_mapping_copy.get_support_points();
            /**
             * Update the DoF indices.
             */
            ky_cell_iter->get_dof_indices(
              scratch_data.ky_local_dof_indices_in_default_dof_order);

            permute_dofs_and_mapping_support_points_for_sauter_quad(
              scratch_data,
              copy_data,
              cell_neighboring_type,
              kx_cell_iter,
              ky_cell_iter,
              kx_mapping_copy,
              ky_mapping_copy);

            /**
             * @internal Copy the scratch data and copy data from CPU to GPU.
             */
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            scratch_data_gpu.assign_from_host(scratch_data);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "copy scratch data from host to device");
#endif

            // Determine thread block dimensions.
            dim3 blocks_rect;
            dim3 threads_rect;
            IdeoBEM::CUDAWrappers::
              configure_thread_blocks_for_sauter_quadrature_points(
                cell_neighboring_type, bem_values, blocks_rect, threads_rect);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            switch (cell_neighboring_type)
              {
                  case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: same panel");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_same_panel<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: common edge");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_edge<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: common vertex");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_vertex<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "calc jacobian: regular");
#endif

                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_regular<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());

                    break;
                  }
              }

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "calculate Jacobians and normal vectors");
#endif

            /**
             * When the bilinear form for the hyper singular operator is
             * evaluated, the covariant transformation is required.
             */
            if (kernel.kernel_type == HyperSingularRegular)
              {
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
                scratch_data.timer.start();
#endif
                switch (cell_neighboring_type)
                  {
                      case CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: same panel");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_same_panel<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: common edge");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_edge<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: common vertex");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_vertex<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                        IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                          "calc covariant: regular");
#endif

                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_regular<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      default: {
                        Assert(false, ExcInternalError());

                        break;
                      }
                  }

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
                scratch_data.timer.stop();
                print_wall_time(scratch_data.log_stream,
                                scratch_data.timer,
                                "calculate covariant matrices");
#endif
              }

            /**
             * Find the index of the i-th DoF in the permuted DoF indices of
             * \f$K_x\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              i_iter =
                std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                          copy_data.kx_local_dof_indices_permuted.end(),
                          i);
            Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int i_index =
              i_iter - copy_data.kx_local_dof_indices_permuted.begin();

            /**
             * Find the index of the j-th DoF in the permuted DoF indices of
             * \f$K_y\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              j_iter =
                std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                          copy_data.ky_local_dof_indices_permuted.end(),
                          j);
            Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int j_index =
              j_iter - copy_data.ky_local_dof_indices_permuted.begin();

            // Apply 4d Sauter numerical quadrature on the GPU device.
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            switch (cell_neighboring_type)
              {
                  case BEMTools::CellNeighboringType::SamePanel: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: same panel");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesSamePanel<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonEdge: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: common edge");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonEdge<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonVertex: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: common vertex");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonVertex<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::Regular: {
#if ENABLE_NVTX == 1
                    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                      "apply quadrature: regular");
#endif

                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesRegular<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());
                    break;
                  }
              }

            /**
             * @internal Extract quadrature results accumulated from all
             * thread blocks.
             */
            cudaError_t error_code = cudaMemcpyAsync(
              (void *)(scratch_data.quad_values_in_thread_blocks),
              (void *)(scratch_data_gpu.quad_values_in_thread_blocks),
              sizeof(RangeNumberType) * blocks_rect.x,
              cudaMemcpyDeviceToHost,
              scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            for (unsigned int t = 0; t < blocks_rect.x; t++)
              {
                double_integral += scratch_data.quad_values_in_thread_blocks[t];
              }
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "apply Sauter quadrature");
#endif

            /**
             * Append the FEM mass matrix contribution.
             *
             * \myalert{N.B. The DoF handlers for the test and ansatz spaces
             * should be constructed on a same triangulation. Then the
             * following
             * comparison @p kx_cell_index == ky_cell_index is meaningful.}
             */
            if ((kx_cell_index == ky_cell_index) && (mass_matrix_factor != 0))
              {
#if ENABLE_NVTX == 1
                IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                  "compute mass matrix");
#endif

                Assert(cell_neighboring_type == CellNeighboringType::SamePanel,
                       ExcInternalError());

                // Update the finite element values for the test space.
                mass_matrix_scratch_data.fe_values_for_test_space.reinit(
                  kx_cell_iter);

                /**
                 * Update the finite element values for the trial space.
                 *
                 * \mynote{N.B. The @p FEValues related to the trial
                 * space must also be updated, since the trial space may
                 * be different from the test space.}
                 */
                mass_matrix_scratch_data.fe_values_for_trial_space.reinit(
                  ky_cell_iter);

                const unsigned int n_q_points =
                  mass_matrix_scratch_data.fe_values_for_test_space
                    .get_quadrature()
                    .size();
                // The trial space is on a same triangulation as the test
                // space and they share a same quadrature object.
                AssertDimension(n_q_points,
                                mass_matrix_scratch_data
                                  .fe_values_for_trial_space.get_quadrature()
                                  .size());

                /**
                 * Get the index of the global DoF index \f$i\f$ in
                 * the current cell \f$K_x\f$.
                 *
                 * \mynote{N.B. The local DoF index in \f$K_x\f$ is
                 * searched from the list from DoF indices held in the
                 * @p ScratchData for BEM. This is valid because the
                 * test and trial spaces associated with the mass matrix
                 * and the BEM bilinear form are the same.
                 *
                 * Since there is no support point permutation during FEM mass
                 * matrix assembly, the DoF indices here are in the default
                 * order.}
                 */
                auto i_local_dof_iter = std::find(
                  scratch_data.kx_local_dof_indices_in_default_dof_order
                    .begin(),
                  scratch_data.kx_local_dof_indices_in_default_dof_order.end(),
                  i);
                Assert(i_local_dof_iter !=
                         scratch_data.kx_local_dof_indices_in_default_dof_order
                           .end(),
                       ExcMessage(
                         std::string("Cannot find the global DoF index ") +
                         std::to_string(i) +
                         std::string(" in the list of cell DoF indices!")));
                const unsigned int i_local_dof_index =
                  i_local_dof_iter -
                  scratch_data.kx_local_dof_indices_in_default_dof_order
                    .begin();

                /**
                 * Get the index of the global DoF index \f$j\f$ in
                 * the current cell \f$K_y\f$.
                 */
                auto j_local_dof_iter = std::find(
                  scratch_data.ky_local_dof_indices_in_default_dof_order
                    .begin(),
                  scratch_data.ky_local_dof_indices_in_default_dof_order.end(),
                  j);
                Assert(j_local_dof_iter !=
                         scratch_data.ky_local_dof_indices_in_default_dof_order
                           .end(),
                       ExcMessage(
                         std::string("Cannot find the global DoF index ") +
                         std::to_string(j) +
                         std::string(" in the list of cell DoF indices!")));
                const unsigned int j_local_dof_index =
                  j_local_dof_iter -
                  scratch_data.ky_local_dof_indices_in_default_dof_order
                    .begin();

                for (unsigned int q = 0; q < n_q_points; q++)
                  {
                    double_integral +=
                      mass_matrix_factor *
                      mass_matrix_scratch_data.fe_values_for_test_space
                        .shape_value(i_local_dof_index, q) *
                      mass_matrix_scratch_data.fe_values_for_trial_space
                        .shape_value(j_local_dof_index, q) *
                      mass_matrix_scratch_data.fe_values_for_test_space.JxW(q);
                  }
              }
          }
      }

    return double_integral;
  }


  /**
   * Sauter quadrature for a pair of DoFs which belongs to a far field \hmatrix
   * node.
   *
   * 1. There is no contribution from mass matrix.
   * 2. There is no need to detect the cell neighboring type, since it can only
   * be @p regular.
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  RangeNumberType
  sauter_assemble_on_one_pair_of_dofs(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const types::global_dof_index                        i,
    const types::global_dof_index                        j,
    const std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
      &kx_dof_to_cell_topo,
    const std::vector<
      std::vector<const typename DoFHandler<dim, spacedim>::cell_iterator *>>
                                                    &ky_dof_to_cell_topo,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                                            &bem_values_gpu,
    const DoFHandler<dim, spacedim>                         &kx_dof_handler,
    const DoFHandler<dim, spacedim>                         &ky_dof_handler,
    const MappingQGenericExt<dim, spacedim>                 &kx_mapping,
    const MappingQGenericExt<dim, spacedim>                 &ky_mapping,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    IdeoBEM::CUDAWrappers::
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                                                            &scratch_data_gpu,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data)
  {
#if ENABLE_NVTX == 1
    IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
      "sauter_assemble_on_one_pair_of_dofs for far field");
#endif

    /**
     * @internal This result variable accumulates all contributions from pairs
     * of cells related to the global DoF indices @p (i,j).
     */
    RangeNumberType double_integral = 0.0;

    /**
     * Iterate over each cell in the support of the basis function associated
     * with the i-th DoF.
     */
    for (auto kx_cell_iter_pointer : kx_dof_to_cell_topo[i])
      {
        const typename DoFHandler<dim, spacedim>::cell_iterator &kx_cell_iter =
          *kx_cell_iter_pointer;

        /**
         * @internal Calculate the real support points in the cell \f$K_x\f$
         * via the mapping object. Because the calculated mapping support
         * points are
         * stored in a member variable in the mapping object @p kx_mapping and
         * the caller @p fill_hmatrix_leaf_node_with_aca_plus of this function
         * will be executed in several TBB threads, computation of the mapping
         * support points should be performed in thread dependent local copies
         * of @p kx_mapping.
         */
        MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
        kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
        /**
         * Copy the newly calculated support points into @p ScratchData.
         */
        scratch_data.kx_mapping_support_points_in_default_order =
          kx_mapping_copy.get_support_points();
        /**
         * Update the DoF indices.
         */
        kx_cell_iter->get_dof_indices(
          scratch_data.kx_local_dof_indices_in_default_dof_order);

        /**
         * Iterate over each cell in the support of the basis function
         * associated with the j-th DoF.
         */
        for (auto ky_cell_iter_pointer : ky_dof_to_cell_topo[j])
          {
            const typename DoFHandler<dim, spacedim>::cell_iterator
              &ky_cell_iter = *ky_cell_iter_pointer;

            /**
             * @internal Similar to making a local copy of @p kx_mapping.
             */
            MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
            ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
            /**
             * Copy the newly calculated support points into @p ScratchData.
             */
            scratch_data.ky_mapping_support_points_in_default_order =
              ky_mapping_copy.get_support_points();
            /**
             * Update the DoF indices.
             */
            ky_cell_iter->get_dof_indices(
              scratch_data.ky_local_dof_indices_in_default_dof_order);

            permute_dofs_and_mapping_support_points_for_sauter_quad(
              scratch_data,
              copy_data,
              CellNeighboringType::Regular,
              kx_cell_iter,
              ky_cell_iter,
              kx_mapping_copy,
              ky_mapping_copy);

            /**
             * @internal Copy the scratch data and copy data from CPU to GPU.
             */
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            scratch_data_gpu.assign_from_host(scratch_data);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "copy scratch data from host to device");
#endif

            /**
             * @internal Determine thread block dimensions.
             */
            dim3 blocks_rect;
            dim3 threads_rect;
            IdeoBEM::CUDAWrappers::
              configure_thread_blocks_for_sauter_quadrature_points(
                CellNeighboringType::Regular,
                bem_values,
                blocks_rect,
                threads_rect);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            {
#if ENABLE_NVTX == 1
              IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                "calc jacobian: regular");
#endif

              IdeoBEM::CUDAWrappers::
                calc_jacobian_normals_for_sauter_quad_regular<<<
                  blocks_rect,
                  threads_rect,
                  0,
                  scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                     scratch_data_gpu);
            }

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "calculate Jacobians and normal vectors");
#endif

            /**
             * When the bilinear form for the hyper singular operator is
             * evaluated, the covariant transformation is required.
             */
            if (kernel.kernel_type == HyperSingularRegular)
              {
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
                scratch_data.timer.start();
#endif
#if ENABLE_NVTX == 1
                IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                  "calc covariant: regular");
#endif

                IdeoBEM::CUDAWrappers::calc_covariant_transformations_regular<<<
                  blocks_rect,
                  threads_rect,
                  0,
                  scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                     scratch_data_gpu);

#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
                scratch_data.timer.stop();
                print_wall_time(scratch_data.log_stream,
                                scratch_data.timer,
                                "calculate covariant matrices");
#endif
              }

            /**
             * Find the index of the i-th DoF in the permuted DoF indices of
             * \f$K_x\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              i_iter =
                std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                          copy_data.kx_local_dof_indices_permuted.end(),
                          i);
            Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int i_index =
              i_iter - copy_data.kx_local_dof_indices_permuted.begin();

            /**
             * Find the index of the j-th DoF in the permuted DoF indices of
             * \f$K_y\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              j_iter =
                std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                          copy_data.ky_local_dof_indices_permuted.end(),
                          j);
            Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int j_index =
              j_iter - copy_data.ky_local_dof_indices_permuted.begin();

            // Apply 4d Sauter numerical quadrature on the GPU device.
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.start();
#endif
            {
#if ENABLE_NVTX == 1
              IdeoBEM::CUDAWrappers::NVTXRange nvtx_range(
                "apply quadrature: regular");
#endif

              IdeoBEM::CUDAWrappers::ApplyQuadratureUsingBEMValuesRegular<<<
                blocks_rect.x,
                threads_rect.x,
                threads_rect.x * sizeof(RangeNumberType),
                scratch_data.cuda_stream_handle>>>(kernel,
                                                   kernel_factor,
                                                   i_index,
                                                   j_index,
                                                   bem_values_gpu,
                                                   scratch_data_gpu);
            }

            /**
             * @internal Extract quadrature results accumulated from all
             * thread blocks.
             */
            cudaError_t error_code = cudaMemcpyAsync(
              (void *)(scratch_data.quad_values_in_thread_blocks),
              (void *)(scratch_data_gpu.quad_values_in_thread_blocks),
              sizeof(RangeNumberType) * blocks_rect.x,
              cudaMemcpyDeviceToHost,
              scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            for (unsigned int t = 0; t < blocks_rect.x; t++)
              {
                double_integral += scratch_data.quad_values_in_thread_blocks[t];
              }
#if ENABLE_DEBUG == 1 && ENABLE_TIMER == 1
            scratch_data.timer.stop();
            print_wall_time(scratch_data.log_stream,
                            scratch_data.timer,
                            "apply Sauter quadrature");
#endif
          }
      }

    return double_integral;
  }
} // namespace IdeoBEM

#endif /* INCLUDE_SAUTER_QUADRATURE_HCU_ */
