/**
 * @file sauter_quadrature.h
 * @brief Introduction of sauter_quadrature.h
 *
 * @date 2022-03-02
 * @author Jihuan Tian
 */
#ifndef INCLUDE_SAUTER_QUADRATURE_HCU_
#define INCLUDE_SAUTER_QUADRATURE_HCU_

#include <deal.II/base/logstream.h>
#include <deal.II/base/utilities.h>

#include <deal.II/dofs/dof_accessor.h>
#include <deal.II/dofs/dof_handler.h>

#include <deal.II/fe/fe.h>
#include <deal.II/fe/fe_data.h>

#include <deal.II/grid/tria_accessor.h>

#include <deal.II/lac/full_matrix.h>

#include <cuda_runtime.h>
#include <stdio.h>
#include <stdlib.h>

#include <algorithm>
#include <iostream>
#include <iterator>
#include <vector>

#include "bem_kernels.hcu"
#include "bem_values.h"
#include "config.h"
#include "cu_bem_values.hcu"
#include "cu_qgauss.hcu"
#include "debug_tools.hcu"
#include "mapping_q_generic_ext.h"
#include "sauter_quadrature_tools.h"

namespace IdeoBEM
{
  using namespace dealii;

  /**
   * Build the topology for "DoF support point-to-cell" relation.
   *
   * \mynote{2022-06-06 This topology is needed when the continuous finite
   * element such as @p FE_Q is adopted. For the discontinuous finite element
   * such as @p FE_DGQ, the DoFs in a cell are separated from those in other
   * cells. Hence, such point-to-cell topology is not necessary.}
   *
   * @param dof_to_cell_topo
   * @param dof_handler
   * @param fe_index
   */
  template <int dim, int spacedim>
  void
  build_dof_to_cell_topology(
    std::vector<std::vector<unsigned int>> &dof_to_cell_topo,
    const DoFHandler<dim, spacedim>        &dof_handler,
    const unsigned int                      fe_index = 0)
  {
    const types::global_dof_index        n_dofs = dof_handler.n_dofs();
    const FiniteElement<dim, spacedim>  &fe     = dof_handler.get_fe(fe_index);
    const unsigned int                   dofs_per_cell = fe.dofs_per_cell;
    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);

    dof_to_cell_topo.resize(n_dofs);

    /**
     * Iterate over each active cell in the triangulation and extract the DoF
     * indices.
     */
    for (const typename DoFHandler<dim, spacedim>::active_cell_iterator &cell :
         dof_handler.active_cell_iterators())
      {
        cell->get_dof_indices(local_dof_indices);
        for (auto dof_index : local_dof_indices)
          {
            dof_to_cell_topo[dof_index].push_back(cell->active_cell_index());
          }
      }
  }


  /**
   * Build the topology for "DoF support point-to-cell" relation.
   *
   * This version handles the case when a subset of the complete DoFs contained
   * in a DoF handler is selected. This case is met in the DoF handlers for
   * Dirichlet space in the mixed boundary value problem. Therefore, a map from
   * local to full DoF indices is passed.
   *
   * \mynote{2022-06-06 This topology is needed when the continuous finite
   * element such as @p FE_Q is adopted. For the discontinuous finite element
   * such as @p FE_DGQ, the DoFs in a cell are separated from those in other
   * cells. Hence, such point-to-cell topology is not necessary.}
   *
   * @param dof_to_cell_topo
   * @param dof_handler
   * @param map_from_local_to_full_dof_indices
   * @param fe_index
   */
  //  template <int dim, int spacedim>
  //  void
  //  build_dof_to_cell_topology(
  //    std::vector<std::vector<unsigned int>> &dof_to_cell_topo,
  //    const DoFHandler<dim, spacedim> &       dof_handler,
  //    const std::vector<types::global_dof_index>
  //      &                map_from_local_to_full_dof_indices,
  //    const unsigned int fe_index = 0)
  //  {
  //    const types::global_dof_index n_dofs =
  //      map_from_local_to_full_dof_indices.size();
  //    const FiniteElement<dim, spacedim> & fe = dof_handler.get_fe(fe_index);
  //    const unsigned int                   dofs_per_cell = fe.dofs_per_cell;
  //    std::vector<types::global_dof_index> local_dof_indices(dofs_per_cell);
  //
  //    dof_to_cell_topo.resize(n_dofs);
  //
  //    /**
  //     * Iterate over each active cell in the triangulation and extract the
  //     DoF
  //     * indices.
  //     */
  //    dof_handler.
  //    for (const typename DoFHandler<dim, spacedim>::active_cell_iterator
  //    &cell :
  //         dof_handler.active_cell_iterators())
  //      {
  //        cell->get_dof_indices(local_dof_indices);
  //        for (auto dof_index : local_dof_indices)
  //          {
  //            dof_to_cell_topo[dof_index].push_back(cell->active_cell_index());
  //          }
  //      }
  //  }


  /**
   * Print out the topological information about DoF support point-to-cell
   * relation.
   *
   * @param dof_to_cell_topo
   */
  void
  print_dof_to_cell_topology(
    const std::vector<std::vector<unsigned int>> &dof_to_cell_topo);


  /**
   * Get the DoF indices associated with the cell vertices from a list of DoF
   * indices which have been arranged in either the forward or backward
   * lexicographic order. In this overloaded version, the results are returned
   * in an array as the function's return value.
   *
   * \mynote{There are <code>GeometryInfo<dim>::vertices_per_cell</code>
   * vertices in the returned array, among which the last two vertex DoF indices
   * have been swapped in this function so that the whole list of vertex DoF
   * indices in the returned array are arranged in either the clockwise or
   * counter clockwise order instead of the original lexicographic(zigzag)
   * order.}
   *
   * @param fe
   * @param dof_indices List of DoF indices in either the forward or backward
   * lexicographic order.
   * @return List of DoF indices for the cell vertices or corners with the last
   * two swapped.
   */
  template <int dim, int spacedim>
  std::array<types::global_dof_index, GeometryInfo<dim>::vertices_per_cell>
  get_vertex_dof_indices_swapped(
    const FiniteElement<dim, spacedim>         &fe,
    const std::vector<types::global_dof_index> &dof_indices)
  {
    Assert(dim == 2, ExcNotImplemented());

    std::array<types::global_dof_index, GeometryInfo<dim>::vertices_per_cell>
      vertex_dof_indices;

    /**
     * When the finite element is L2, such as @p FE_DGQ, its member
     * @p dofs_per_face is 0. Therefore, here we manually calculate the number
     * of DoFs per face.
     */
    const unsigned int dofs_per_face =
      fe.dofs_per_face > 0 ?
        fe.dofs_per_face :
        static_cast<unsigned int>(
          dealii::Utilities::fixed_power<dim - 1>(fe.degree + 1));

    vertex_dof_indices[0] = dof_indices[0];
    vertex_dof_indices[1] = dof_indices[dofs_per_face - 1];
    vertex_dof_indices[2] = dof_indices[dof_indices.size() - 1];
    vertex_dof_indices[3] =
      dof_indices[dof_indices.size() - 1 - (dofs_per_face - 1)];

    return vertex_dof_indices;
  }


  /**
   * Get the DoF indices associated with the cell vertices from a list of DoF
   * indices which have been arranged in either the forward or backward
   * lexicographic order. In this overloaded version, the results are returned
   * in an array as the last argument of this function.
   *
   * \mynote{There are <code>GeometryInfo<dim>::vertices_per_cell</code>
   * vertices in the returned array, among which the last two vertex DoF indices
   * have been swapped in this function so that the whole list of vertex DoF
   * indices in the returned array are arranged in either the clockwise or
   * counter clockwise order instead of the original lexicographic(zigzag)
   * order.}
   *
   * @param fe
   * @param dof_indices List of DoF indices in either the forward or backward
   * lexicographic order.
   * @param vertex_dof_indices [out] List of DoF indices for the cell vertices
   * or corners with the last two swapped.
   */
  template <int dim, int spacedim>
  void
  get_vertex_dof_indices_swapped(
    const FiniteElement<dim, spacedim>         &fe,
    const std::vector<types::global_dof_index> &dof_indices,
    std::array<types::global_dof_index, GeometryInfo<dim>::vertices_per_cell>
      &vertex_dof_indices)
  {
    Assert(dim == 2, ExcNotImplemented());

    /**
     * When the finite element is L2, such as @p FE_DGQ, its member
     * @p dofs_per_face is 0. Therefore, here we manually calculate the number
     * of DoFs per face.
     */
    const unsigned int dofs_per_face =
      fe.dofs_per_face > 0 ?
        fe.dofs_per_face :
        static_cast<unsigned int>(
          dealii::Utilities::fixed_power<dim - 1>(fe.degree + 1));

    vertex_dof_indices[0] = dof_indices[0];
    vertex_dof_indices[1] = dof_indices[dofs_per_face - 1];
    vertex_dof_indices[2] = dof_indices[dof_indices.size() - 1];
    vertex_dof_indices[3] =
      dof_indices[dof_indices.size() - 1 - (dofs_per_face - 1)];
  }


  /**
   * Get the local index for the starting vertex in the cell by checking
   * the list of numbers assigned to cell vertices with the last two swapped.
   *
   * \mynote{There are two cases to be processed here, common edge and common
   * vertex.
   * 1. In the common edge case, there are two DoF indices in the vector
   * <code>vertex_dof_index_intersection</code>. Then their array indices wrt.
   * the vector <code>local_vertex_dof_indices_swapped</code> will be
   * searched. By considering this vector as a closed loop list, the two DoF
   * indices in this vector are successively located and the first one of which
   * is the vertex to start subsequent DoF traversing.
   * 2. In the common vertex case, since there is only one DoF index in the
   * vector @p vertex_dof_index_intersection, this vertex is the starting point.}
   *
   * @param common_vertex_dof_indices The vector storing the pairs of vertex
   * DoF indices in \f$K_x\f$ and \f$K_y\f$, which share common vertices.
   * @param local_vertex_dof_indices_swapped Vertex DoF indices with the last
   * two swapped, which have been obtained from the function
   * @p get_vertex_dof_indices_swapped.
   * @param is_first_cell If the common vertex DoF indices in the first cell or
   * the second cell are to be extracted.
   * @return The array index for the starting vertex, wrt. the original list of
   * vertex DoF indices, i.e. the last two elements of which are not swapped.
   */
  template <int vertices_per_cell>
  unsigned int
  get_start_vertex_local_index_in_cell_from_vertex_numbers(
    const std::vector<std::pair<unsigned int, unsigned int>>
      &common_vertex_pair_local_indices,
    const std::array<unsigned int, vertices_per_cell>
              &vertex_local_indices_in_cell_with_last_two_swapped,
    const bool is_first_cell)
  {
    /**
     * The local index of the starting vertex should be in the range [0,
     * vertices_per_cell). Therefore, we use @p vertices_per_cell as its
     * initial invalid value.
     */
    unsigned int starting_vertex_local_index = vertices_per_cell;

    switch (common_vertex_pair_local_indices.size())
      {
          case 2: // Common edge case
          {
            unsigned int first_vertex_local_index;
            unsigned int second_vertex_local_index;

            if (is_first_cell)
              {
                first_vertex_local_index =
                  common_vertex_pair_local_indices[0].first;
                second_vertex_local_index =
                  common_vertex_pair_local_indices[1].first;
              }
            else
              {
                first_vertex_local_index =
                  common_vertex_pair_local_indices[0].second;
                second_vertex_local_index =
                  common_vertex_pair_local_indices[1].second;
              }

            typename std::array<unsigned int, vertices_per_cell>::const_iterator
              first_common_vertex_iterator = std::find(
                vertex_local_indices_in_cell_with_last_two_swapped.cbegin(),
                vertex_local_indices_in_cell_with_last_two_swapped.cend(),
                first_vertex_local_index);
            Assert(first_common_vertex_iterator !=
                     vertex_local_indices_in_cell_with_last_two_swapped.cend(),
                   ExcInternalError());

            if ((first_common_vertex_iterator + 1) !=
                vertex_local_indices_in_cell_with_last_two_swapped.cend())
              {
                if (*(first_common_vertex_iterator + 1) ==
                    second_vertex_local_index)
                  {
                    starting_vertex_local_index = first_vertex_local_index;
                  }
                else
                  {
                    starting_vertex_local_index = second_vertex_local_index;
                  }
              }
            else
              {
                if ((*vertex_local_indices_in_cell_with_last_two_swapped
                        .cbegin()) == second_vertex_local_index)
                  {
                    starting_vertex_local_index = first_vertex_local_index;
                  }
                else
                  {
                    starting_vertex_local_index = second_vertex_local_index;
                  }
              }

            break;
          }
          case 1: // Common vertex case
          {
            starting_vertex_local_index =
              is_first_cell ? common_vertex_pair_local_indices[0].first :
                              common_vertex_pair_local_indices[0].second;

            break;
          }
        default:
          Assert(false, ExcInternalError());
          break;
      }

    return starting_vertex_local_index;
  }


  /**
   * Permute DoFs support points in real cells and their associated global
   * DoF indices for Sauter quadrature, the behavior of which depends on the
   * detected cell neighboring types.
   *
   * \mynote{This version involves @p PairCellWiseScratchData and
   * @p PairCellWisePerTaskData.
   *
   * Inside this function, whether DoF indices in \f$K_x\f$ will be extracted
   * depends on the flag @p is_scratch_data_for_kx_uncalculated. The DoF
   * indices in \f$K_y\f$ will always be extracted.}
   *
   * @param scratch
   * @param data
   * @param kx_cell_iter
   * @param ky_cell_iter
   * @param kx_mapping
   * @param ky_mapping
   */
  template <int dim, int spacedim, typename RangeNumberType = double>
  void
  permute_dofs_and_mapping_support_points_for_sauter_quad(
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &data,
    const CellNeighboringType cell_neighboring_type,
    const typename DoFHandler<dim, spacedim>::cell_iterator &kx_cell_iter,
    const typename DoFHandler<dim, spacedim>::cell_iterator &ky_cell_iter,
    const MappingQGenericExt<dim, spacedim>                 &kx_mapping,
    const MappingQGenericExt<dim, spacedim>                 &ky_mapping,
    const bool is_scratch_data_for_kx_uncalculated = true)
  {
    // Geometry information.
    const unsigned int vertices_per_cell = GeometryInfo<dim>::vertices_per_cell;

    const FiniteElement<dim, spacedim> &kx_fe = kx_cell_iter->get_fe();
    const FiniteElement<dim, spacedim> &ky_fe = ky_cell_iter->get_fe();

    // N.B. The vector holding local DoF indices has to have the right size
    // before being passed to the function <code>get_dof_indices</code>. And
    // this has been performed during the initialization of the scratch data.
    if (is_scratch_data_for_kx_uncalculated)
      {
        kx_cell_iter->get_dof_indices(
          scratch.kx_local_dof_indices_in_default_dof_order);
      }
    ky_cell_iter->get_dof_indices(
      scratch.ky_local_dof_indices_in_default_dof_order);

    switch (cell_neighboring_type)
      {
          case SamePanel: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     vertices_per_cell,
                   ExcInternalError());

            /**
             * Permute mapping support points into lexicographic order.
             */
            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_poly_space_numbering_inverse,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_poly_space_numbering_inverse,
                           scratch.ky_mapping_support_points_permuted);

            /**
             * Permute DoF indices in the finite elements.
             */
            if (kx_fe.dofs_per_cell > 1)
              {
                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_fe_poly_space_numbering_inverse,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ(0). Permutation is not needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                /**
                 * Get DoF indices in the lexicographic
                 * order.
                 */
                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_fe_poly_space_numbering_inverse,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          case CommonEdge: {
            /**
             * This part handles the common edge case of Sauter's quadrature
             * rule.
             * 1. Get the DoF indices in the lexicographic order for \f$K_x\f$.
             * 2. Get the DoF indices in the reversed lexicographic order for
             * \f$K_y\f$.
             * 3. Extract only those DoF indices which are located at cell
             * vertices in \f$K_x\f$ and \f$K_y\f$. N.B. The DoF indices for the
             * last two vertices are swapped, such that the four vertices are in
             * either clockwise or counter clockwise order.
             * 4. Determine the starting vertex for \f$K_x\f$ and regenerate the
             * permutation numbering for traversing in the forward lexicographic
             * order by starting from this vertex.
             * 5. Determine the starting vertex for \f$K_y\f$ and regenerate the
             * permutation numbering for traversing in the backward
             * lexicographic order by starting from this vertex.
             * 6. Apply the newly generated permutation numbering scheme to
             * support points and DoF indices in the original default DoF order.
             */

            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<dim>::vertices_per_face,
                   ExcInternalError());

            // Determine the starting vertex index in \f$K_x\f$.
            unsigned int kx_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 1, 3, 2}},
                                   true);
            AssertIndexRange(kx_starting_vertex_local_index, vertices_per_cell);

            // Determine the starting vertex index in \f$K_y\f$.
            unsigned int ky_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 2, 3, 1}},
                                   false);
            AssertIndexRange(ky_starting_vertex_local_index, vertices_per_cell);

            // Generate the permutation of support points for the mappings.
            generate_forward_mapping_support_point_permutation(
              kx_mapping,
              kx_starting_vertex_local_index,
              scratch.kx_mapping_support_point_permutation);
            generate_backward_mapping_support_point_permutation(
              ky_mapping,
              ky_starting_vertex_local_index,
              scratch.ky_mapping_support_point_permutation);

            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_support_point_permutation,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_support_point_permutation,
                           scratch.ky_mapping_support_points_permuted);

            if (kx_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_x\f$ by starting
                // from <code>kx_starting_vertex_index</code>.
                generate_forward_dof_permutation(
                  kx_fe,
                  kx_starting_vertex_local_index,
                  scratch.kx_local_dof_permutation);

                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_local_dof_permutation,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_y\f$ by starting
                // from <code>ky_starting_vertex_index</code>.
                generate_backward_dof_permutation(
                  ky_fe,
                  ky_starting_vertex_local_index,
                  scratch.ky_local_dof_permutation);

                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_local_dof_permutation,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          case CommonVertex: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 1,
                   ExcInternalError());

            // Determine the starting vertex index in \f$K_x\f$.
            unsigned int kx_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 1, 3, 2}},
                                   true);
            AssertIndexRange(kx_starting_vertex_local_index, vertices_per_cell);

            // Determine the starting vertex index in \f$K_y\f$.
            unsigned int ky_starting_vertex_local_index =
              get_start_vertex_local_index_in_cell_from_vertex_numbers<
                vertices_per_cell>(scratch.common_vertex_pair_local_indices,
                                   {{0, 2, 3, 1}},
                                   false);
            AssertIndexRange(ky_starting_vertex_local_index, vertices_per_cell);

            // Generate the permutation of support points for the mappings.
            generate_forward_mapping_support_point_permutation(
              kx_mapping,
              kx_starting_vertex_local_index,
              scratch.kx_mapping_support_point_permutation);
            generate_forward_mapping_support_point_permutation(
              ky_mapping,
              ky_starting_vertex_local_index,
              scratch.ky_mapping_support_point_permutation);

            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_support_point_permutation,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_support_point_permutation,
                           scratch.ky_mapping_support_points_permuted);

            if (kx_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_x\f$ by starting
                // from <code>kx_starting_vertex_index</code>.
                generate_forward_dof_permutation(
                  kx_fe,
                  kx_starting_vertex_local_index,
                  scratch.kx_local_dof_permutation);

                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_local_dof_permutation,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                // Generate the permutation of DoFs in \f$K_y\f$ by starting
                // from <code>ky_starting_vertex_index</code>.
                generate_forward_dof_permutation(
                  ky_fe,
                  ky_starting_vertex_local_index,
                  scratch.ky_local_dof_permutation);

                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_local_dof_permutation,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          case Regular: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 0,
                   ExcInternalError());

            /**
             * Permute mapping support points into lexicographic order.
             */
            permute_vector(scratch.kx_mapping_support_points_in_default_order,
                           scratch.kx_mapping_poly_space_numbering_inverse,
                           scratch.kx_mapping_support_points_permuted);
            permute_vector(scratch.ky_mapping_support_points_in_default_order,
                           scratch.ky_mapping_poly_space_numbering_inverse,
                           scratch.ky_mapping_support_points_permuted);

            if (kx_fe.dofs_per_cell > 1)
              {
                permute_vector(
                  scratch.kx_local_dof_indices_in_default_dof_order,
                  scratch.kx_fe_poly_space_numbering_inverse,
                  data.kx_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.kx_local_dof_indices_permuted[0] =
                  scratch.kx_local_dof_indices_in_default_dof_order[0];
              }

            if (ky_fe.dofs_per_cell > 1)
              {
                permute_vector(
                  scratch.ky_local_dof_indices_in_default_dof_order,
                  scratch.ky_fe_poly_space_numbering_inverse,
                  data.ky_local_dof_indices_permuted);
              }
            else
              {
                /**
                 * Handle the case when the finite element order is 0, i.e. for
                 * @p FE_DGQ. Then there is no permutation needed.
                 */
                data.ky_local_dof_indices_permuted[0] =
                  scratch.ky_local_dof_indices_in_default_dof_order[0];
              }

            break;
          }
          default: {
            Assert(false, ExcInternalError());
          }
      }

    /**
     * @internal Extract two components from the permuted mapping support
     * points.
     */
    for (unsigned int i = 0;
         i < scratch.kx_mapping_support_points_permuted.size();
         i++)
      {
        scratch.kx_mapping_support_points_permuted_xy_components[i](0) =
          scratch.kx_mapping_support_points_permuted[i](0);
        scratch.kx_mapping_support_points_permuted_xy_components[i](1) =
          scratch.kx_mapping_support_points_permuted[i](1);

        scratch.kx_mapping_support_points_permuted_yz_components[i](0) =
          scratch.kx_mapping_support_points_permuted[i](1);
        scratch.kx_mapping_support_points_permuted_yz_components[i](1) =
          scratch.kx_mapping_support_points_permuted[i](2);

        scratch.kx_mapping_support_points_permuted_zx_components[i](0) =
          scratch.kx_mapping_support_points_permuted[i](2);
        scratch.kx_mapping_support_points_permuted_zx_components[i](1) =
          scratch.kx_mapping_support_points_permuted[i](0);
      }

    for (unsigned int i = 0;
         i < scratch.ky_mapping_support_points_permuted.size();
         i++)
      {
        scratch.ky_mapping_support_points_permuted_xy_components[i](0) =
          scratch.ky_mapping_support_points_permuted[i](0);
        scratch.ky_mapping_support_points_permuted_xy_components[i](1) =
          scratch.ky_mapping_support_points_permuted[i](1);

        scratch.ky_mapping_support_points_permuted_yz_components[i](0) =
          scratch.ky_mapping_support_points_permuted[i](1);
        scratch.ky_mapping_support_points_permuted_yz_components[i](1) =
          scratch.ky_mapping_support_points_permuted[i](2);

        scratch.ky_mapping_support_points_permuted_zx_components[i](0) =
          scratch.ky_mapping_support_points_permuted[i](2);
        scratch.ky_mapping_support_points_permuted_zx_components[i](1) =
          scratch.ky_mapping_support_points_permuted[i](0);
      }
  }


  namespace CUDAWrappers
  {
    /**
     * Determine the dimensions of block rectangle and thread rectangle.
     *
     * @param cell_neighboring_type
     * @param blocks_rect
     * @param threads_rect
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    void
    configure_thread_blocks(
      const CellNeighboringType                        cell_neighboring_type,
      const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
      dim3                                            &blocks_rect,
      dim3                                            &threads_rect)
    {
      switch (cell_neighboring_type)
        {
            case SamePanel: {
              threads_rect.x = 32;
              threads_rect.y = 8;

              blocks_rect.x = (bem_values.quad_rule_for_same_panel.size() +
                               threads_rect.x - 1) /
                              threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            case CommonEdge: {
              threads_rect.x = 32;
              threads_rect.y = 6;

              blocks_rect.x = (bem_values.quad_rule_for_common_edge.size() +
                               threads_rect.x - 1) /
                              threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            case CommonVertex: {
              threads_rect.x = 32;
              threads_rect.y = 4;

              blocks_rect.x = (bem_values.quad_rule_for_common_vertex.size() +
                               threads_rect.x - 1) /
                              threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            case Regular: {
              threads_rect.x = 32;
              threads_rect.y = 1;

              blocks_rect.x =
                (bem_values.quad_rule_for_regular.size() + threads_rect.x - 1) /
                threads_rect.x;
              blocks_rect.y = 1;

              break;
            }
            default: {
              threads_rect.x = 32;
              threads_rect.y = 1;

              blocks_rect.x = 1;
              blocks_rect.y = 1;

              break;
            }
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and quadrature points in
     * the real cell to be used in the Sauter quadrature (for same panel cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_same_panel(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_same_panel.size();
           q += stride)
        {
          scratch_data.kx_jacobians_same_panel(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.kx_mapping_shape_grad_matrix_table_for_same_panel,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_same_panel(threadIdx.y, q));

          scratch_data.ky_jacobians_same_panel(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.ky_mapping_shape_grad_matrix_table_for_same_panel,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_same_panel(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_value_table_for_same_panel,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_same_panel(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_value_table_for_same_panel,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_same_panel(threadIdx.y, q));
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and qudrature points in
     * the real cell to be used in the Sauter quadrature (for common edge cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_common_edge(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_common_edge.size();
           q += stride)
        {
          scratch_data.kx_jacobians_common_edge(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.kx_mapping_shape_grad_matrix_table_for_common_edge,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_common_edge(threadIdx.y, q));

          scratch_data.ky_jacobians_common_edge(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.ky_mapping_shape_grad_matrix_table_for_common_edge,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_common_edge(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_value_table_for_common_edge,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_common_edge(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_value_table_for_common_edge,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_common_edge(threadIdx.y, q));
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and quadrature points in
     * the real cell to be used in the Sauter quadrature (for common vertex cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_common_vertex(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_common_vertex.size();
           q += stride)
        {
          scratch_data.kx_jacobians_common_vertex(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.kx_mapping_shape_grad_matrix_table_for_common_vertex,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_common_vertex(threadIdx.y, q));

          scratch_data.ky_jacobians_common_vertex(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.ky_mapping_shape_grad_matrix_table_for_common_vertex,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_common_vertex(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_value_table_for_common_vertex,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_common_vertex(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_value_table_for_common_vertex,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_common_vertex(threadIdx.y, q));
        }
    }


    /**
     * Precalculate surface Jacobians, normal vectors and quadrature points in
     * the real cell to be used in the Sauter quadrature (for regular cell
     * neighboring type). This function runs on the GPU device.
     *
     * 2D thread grid is adopted. The X direction is associated with quadrature
     * point index, while the Y direction is associated with k3 index.
     *
     * \mynote{Since this is a @p __global__ function, the parameters should be
     * passed by value.}
     *
     * \alert{Computation of the Jacobian matrix as well as related quantities
     * such as normal vector, covariant transformation matrix, metric tensor,
     * etc., is related to the mapping object and has nothing to do with the
     * finite element. A mapping object is used to describe geometry, while a
     * finite element object is used to describe the ansatz or test functions.}
     *
     * @tparam dim
     * @tparam spacedim
     * @tparam RangeNumberType
     * @param bem_values
     * @param scratch_data
     */
    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_jacobian_normals_for_sauter_quad_regular(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_regular.size();
           q += stride)
        {
          scratch_data.kx_jacobians_regular(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
              scratch_data.kx_mapping_support_points_permuted,
              scratch_data.kx_mapping_support_points_permuted_xy_components,
              scratch_data.kx_mapping_support_points_permuted_yz_components,
              scratch_data.kx_mapping_support_points_permuted_zx_components,
              scratch_data.kx_normals_regular(threadIdx.y, q));

          scratch_data.ky_jacobians_regular(threadIdx.y, q) =
            BEMTools::CUDAWrappers::surface_jacobian_det_and_normal_vector(
              threadIdx.y,
              q,
              bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
              scratch_data.ky_mapping_support_points_permuted,
              scratch_data.ky_mapping_support_points_permuted_xy_components,
              scratch_data.ky_mapping_support_points_permuted_yz_components,
              scratch_data.ky_mapping_support_points_permuted_zx_components,
              scratch_data.ky_normals_regular(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_value_table_for_regular,
            scratch_data.kx_mapping_support_points_permuted,
            scratch_data.kx_quad_points_regular(threadIdx.y, q));

          BEMTools::CUDAWrappers::transform_unit_to_permuted_real_cell(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_value_table_for_regular,
            scratch_data.ky_mapping_support_points_permuted,
            scratch_data.ky_quad_points_regular(threadIdx.y, q));
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_same_panel(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_same_panel.size();
           q += stride)
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_same_panel(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_grad_matrix_table_for_same_panel,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_same_panel(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_grad_matrix_table_for_same_panel,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_common_edge(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_common_edge.size();
           q += stride)
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_common_edge(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_grad_matrix_table_for_common_edge,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_common_edge(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_grad_matrix_table_for_common_edge,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_common_vertex(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_common_vertex.size();
           q += stride)
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_common_vertex(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_grad_matrix_table_for_common_vertex,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_common_vertex(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_grad_matrix_table_for_common_vertex,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    template <int dim, int spacedim, typename RangeNumberType = double>
    __global__ void
    calc_covariant_transformations_regular(
      const CUDABEMValues<dim, spacedim, RangeNumberType>         bem_values,
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType> scratch_data)
    {
      // Number of threads in a row in the thread grid.
      const unsigned int thread_num_per_row = gridDim.x * blockDim.x;
      // Thread index in the X direction, which is mapped to the quadrature
      // point index.
      const unsigned int current_quad_point_index =
        thread_num_per_row * blockIdx.y +
        (blockIdx.x * blockDim.x + threadIdx.x);
      // Quadrature point index stride for the current thread to process the
      // next point. If enough thread blocks are allocated, each thread
      // processes only one quadrature point.
      const unsigned int stride = thread_num_per_row * gridDim.y;

      for (unsigned int q = current_quad_point_index;
           q < bem_values.quad_rule_for_regular.size();
           q += stride)
        {
          CUDAFullMatrix<RangeNumberType> kx_covariant(
            &(scratch_data.kx_covariants_regular(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
            scratch_data.kx_mapping_support_points_permuted,
            kx_covariant);

          CUDAFullMatrix<RangeNumberType> ky_covariant(
            &(scratch_data.ky_covariants_regular(threadIdx.y, q, 0, 0)),
            spacedim,
            dim);
          BEMTools::CUDAWrappers::surface_covariant_transformation(
            threadIdx.y,
            q,
            bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
            scratch_data.ky_mapping_support_points_permuted,
            ky_covariant);
        }
    }


    /**
     * Sum using warp reduce.
     *
     * @param data
     * @param tid
     */
    template <typename RangeNumberType = double>
    __device__ void
    warpReduce(volatile RangeNumberType *data, const unsigned int tid)
    {
      assert(warpSize == 32);

      RangeNumberType temp;
      temp = data[tid + 16];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 8];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 4];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 2];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
      temp = data[tid + 1];
      __syncwarp();
      data[tid] += temp;
      __syncwarp();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the same
     * panel case.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesSamePanel(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = (blockIdx.x * blockDim.x) + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_same_panel.size())
        {
          const CUDATable<2, double> &quad_points =
            bem_values.quad_rule_for_same_panel.get_points();

          /**
           * @internal Evaluate the kernel function at the current quadrature
           * point, which is an accumulation of all @p k3 terms.
           */
          RangeNumberType kernel_value = 0.;
          for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
            {
              if (kernel_function.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current shape
                   * function at the current quadrature point in the unit cell
                   * for \f$K_x\f$ as well as \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

                  for (unsigned int i = 0; i < dim; i++)
                    {
                      kx_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_same_panel(
                          k3_index, quad_no, i, kx_dof_index);
                      ky_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_same_panel(
                          k3_index, quad_no, i, ky_dof_index);
                    }

                  /**
                   * Apply covariant transformation to the gradient tensors in
                   * the unit cell.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.kx_covariants_same_panel(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);
                  CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.ky_covariants_same_panel(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector and
                   * the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.kx_normals_same_panel(k3_index, quad_no),
                      kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.ky_normals_same_panel(k3_index, quad_no),
                      ky_shape_grad_in_real_cell_vector);

                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.ky_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.kx_normals_same_panel(k3_index, quad_no),
                      scratch_data.ky_normals_same_panel(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_same_panel(k3_index, quad_no) *
                    scratch_data.ky_jacobians_same_panel(k3_index, quad_no) *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl);
                }
              else
                {
                  /**
                   * @internal Evaluate the kernel function at the specified
                   * pair of points in the real cells with their normal vectors,
                   * the result of which is then multiplied by the Jacobians
                   * from unit cell to real cell and shape function values.
                   */
                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.ky_quad_points_same_panel(k3_index, quad_no),
                      scratch_data.kx_normals_same_panel(k3_index, quad_no),
                      scratch_data.ky_normals_same_panel(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_same_panel(k3_index, quad_no) *
                    scratch_data.ky_jacobians_same_panel(k3_index, quad_no) *
                    bem_values.kx_shape_value_table_for_same_panel(kx_dof_index,
                                                                   k3_index,
                                                                   quad_no) *
                    bem_values.ky_shape_value_table_for_same_panel(ky_dof_index,
                                                                   k3_index,
                                                                   quad_no);
                }
            }

          /**
           * @internal Multiply the kernel function value with the Jacobian from
           * the Sauter 4D parametric space to the unit cell, then with the
           * quadrature weights. The result is stored into the shared memory.
           */
          quad_values_in_thread_block[tid] =
            kernel_value * quad_points(quad_no, 0) *
            (1 - quad_points(quad_no, 0)) *
            (1 - quad_points(quad_no, 0) * quad_points(quad_no, 1)) *
            bem_values.quad_rule_for_same_panel.get_weights()[quad_no];
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result (scaled by the factor) for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the common
     * edge case.
     *
     * \myalert{During the computation, the normal vector of \f$K_y\f$} should
     * be negated.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesCommonEdge(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = (blockIdx.x * blockDim.x) + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_common_edge.size())
        {
          const CUDATable<2, double> &quad_points =
            bem_values.quad_rule_for_common_edge.get_points();
          RangeNumberType jacobian_det1 = quad_points(quad_no, 0) *
                                          quad_points(quad_no, 0) *
                                          (1 - quad_points(quad_no, 0));
          RangeNumberType jacobian_det2 =
            quad_points(quad_no, 0) * quad_points(quad_no, 0) *
            (1 - quad_points(quad_no, 0) * quad_points(quad_no, 1));

          /**
           * @internal Evaluate the kernel function at the current quadrature
           * point, which is an accumulation of all @p k3 terms.
           */
          RangeNumberType kernel_value = 0.;
          for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
            {
              if (kernel_function.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current shape
                   * function at the current quadrature point in the unit cell
                   * for \f$K_x\f$ as well as \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

                  for (unsigned int i = 0; i < dim; i++)
                    {
                      kx_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_common_edge(
                          k3_index, quad_no, i, kx_dof_index);
                      ky_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_common_edge(
                          k3_index, quad_no, i, ky_dof_index);
                    }

                  /**
                   * Apply covariant transformation to the gradient tensors in
                   * the unit cell.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.kx_covariants_common_edge(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);
                  CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.ky_covariants_common_edge(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector and
                   * the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.kx_normals_common_edge(k3_index, quad_no),
                      kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(
                      -scratch_data.ky_normals_common_edge(k3_index, quad_no),
                      ky_shape_grad_in_real_cell_vector);

                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.ky_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.kx_normals_common_edge(k3_index, quad_no),
                      -scratch_data.ky_normals_common_edge(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_edge(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_edge(k3_index, quad_no) *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl) *
                    (k3_index <= 1 ? jacobian_det1 : jacobian_det2);
                }
              else
                {
                  /**
                   * @internal Evaluate the kernel function at the specified
                   * pair of points in the real cells with their normal vectors,
                   * the result of which is then multiplied by the Jacobians
                   * from unit cell to real cell, Jacobian from Sauter 4D space
                   * to unit cell, and shape function values.
                   */
                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.ky_quad_points_common_edge(k3_index,
                                                              quad_no),
                      scratch_data.kx_normals_common_edge(k3_index, quad_no),
                      -scratch_data.ky_normals_common_edge(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_edge(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_edge(k3_index, quad_no) *
                    bem_values.kx_shape_value_table_for_common_edge(
                      kx_dof_index, k3_index, quad_no) *
                    bem_values.ky_shape_value_table_for_common_edge(
                      ky_dof_index, k3_index, quad_no) *
                    (k3_index <= 1 ? jacobian_det1 : jacobian_det2);
                }
            }

          /**
           * @internal Multiply the kernel function value with the quadrature
           * weights. The result is stored into the shared memory.
           */
          quad_values_in_thread_block[tid] =
            kernel_value *
            bem_values.quad_rule_for_common_edge.get_weights()[quad_no];
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the common
     * vertex case.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesCommonVertex(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = (blockIdx.x * blockDim.x) + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_common_vertex.size())
        {
          const CUDATable<2, double> &quad_points =
            bem_values.quad_rule_for_common_vertex.get_points();

          /**
           * @internal Evaluate the kernel function at the current quadrature
           * point, which is an accumulation of all @p k3 terms.
           */
          RangeNumberType kernel_value = 0.;
          for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
            {
              if (kernel_function.kernel_type == HyperSingularRegular)
                {
                  /**
                   * @internal Extract the gradient values of the current shape
                   * function at the current quadrature point in the unit cell
                   * for \f$K_x\f$ as well as \f$K_y\f$.
                   */
                  RangeNumberType             kx_shape_grad_in_unit_cell[dim];
                  RangeNumberType             ky_shape_grad_in_unit_cell[dim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                    kx_shape_grad_in_unit_cell, dim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                    ky_shape_grad_in_unit_cell, dim);

                  for (unsigned int i = 0; i < dim; i++)
                    {
                      kx_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_common_vertex(
                          k3_index, quad_no, i, kx_dof_index);
                      ky_shape_grad_in_unit_cell[i] =
                        bem_values.kx_shape_grad_matrix_table_for_common_vertex(
                          k3_index, quad_no, i, ky_dof_index);
                    }

                  /**
                   * Apply covariant transformation to the gradient tensors in
                   * the unit cell.
                   */
                  RangeNumberType kx_shape_grad_in_real_cell[spacedim];
                  RangeNumberType ky_shape_grad_in_real_cell[spacedim];
                  CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                    kx_shape_grad_in_real_cell, spacedim);
                  CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                    ky_shape_grad_in_real_cell, spacedim);

                  CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.kx_covariants_common_vertex(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);
                  CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                    const_cast<RangeNumberType *>(
                      &(scratch_data.ky_covariants_common_vertex(
                        k3_index, quad_no, 0, 0))),
                    spacedim,
                    dim);

                  kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                            kx_shape_grad_in_unit_cell_vector);
                  ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                            ky_shape_grad_in_unit_cell_vector);

                  /**
                   * Calculate the surface gradient tensor of the shape
                   * functions, which is the cross product of normal vector and
                   * the volume gradient vector.
                   *
                   * \mynote{The cross product operation requires the input
                   * vectors be transformed to tensors.}
                   */
                  Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.kx_normals_common_vertex(k3_index, quad_no),
                      kx_shape_grad_in_real_cell_vector);
                  Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                    cross_product_3d(
                      scratch_data.ky_normals_common_vertex(k3_index, quad_no),
                      ky_shape_grad_in_real_cell_vector);

                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.ky_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.kx_normals_common_vertex(k3_index, quad_no),
                      scratch_data.ky_normals_common_vertex(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_vertex(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_vertex(k3_index, quad_no) *
                    scalar_product(kx_shape_surface_curl,
                                   ky_shape_surface_curl);
                }
              else
                {
                  /**
                   * @internal Evaluate the kernel function at the specified
                   * pair of points in the real cells with their normal vectors,
                   * the result of which is then multiplied by the Jacobians
                   * from unit cell to real cell and shape function values.
                   */
                  kernel_value +=
                    kernel_function.value(
                      scratch_data.kx_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.ky_quad_points_common_vertex(k3_index,
                                                                quad_no),
                      scratch_data.kx_normals_common_vertex(k3_index, quad_no),
                      scratch_data.ky_normals_common_vertex(k3_index, quad_no),
                      component) *
                    scratch_data.kx_jacobians_common_vertex(k3_index, quad_no) *
                    scratch_data.ky_jacobians_common_vertex(k3_index, quad_no) *
                    bem_values.kx_shape_value_table_for_common_vertex(
                      kx_dof_index, k3_index, quad_no) *
                    bem_values.ky_shape_value_table_for_common_vertex(
                      ky_dof_index, k3_index, quad_no);
                }
            }

          /**
           * @internal Multiply the kernel function value with the Jacobian from
           * the Sauter 4D parametric space to the unit cell, then with the
           * quadrature weights. The result is stored into the shared memory.
           */
          quad_values_in_thread_block[tid] =
            kernel_value *
            IdeoBEM::Utilities::CUDAWrappers::fixed_power<3>(
              quad_points(quad_no, 0)) *
            bem_values.quad_rule_for_common_vertex.get_weights()[quad_no];
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result (scaled by the factor) for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }


    /**
     * Apply Sauter quadrature directly to the kernel function for the regular
     * case.
     */
    template <int dim,
              int spacedim,
              template <int, typename>
              typename KernelFunctionType,
              typename RangeNumberType = double>
    __global__ void
    ApplyQuadratureUsingBEMValuesRegular(
      const KernelFunctionType<spacedim, RangeNumberType> kernel_function,
      const RangeNumberType                               factor,
      const unsigned int                                  kx_dof_index,
      const unsigned int                                  ky_dof_index,
      const CUDABEMValues<dim, spacedim, RangeNumberType> bem_values,
      const CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                   scratch_data,
      unsigned int component = 0)
    {
      /**
       * @internal Result array in the shared memory which stores the quadrature
       * results for each thread in a same thread block. Its dimension should be
       * @p blockDim.x.
       */
      extern __shared__ RangeNumberType quad_values_in_thread_block[];

      const unsigned int tid     = threadIdx.x;
      const unsigned int quad_no = (blockIdx.x * blockDim.x) + threadIdx.x;

      if (quad_no < bem_values.quad_rule_for_regular.size())
        {
          if (kernel_function.kernel_type == HyperSingularRegular)
            {
              /**
               * @internal Extract the gradient values of the current shape
               * function at the current quadrature point in the unit cell
               * for \f$K_x\f$ as well as \f$K_y\f$.
               */
              RangeNumberType             kx_shape_grad_in_unit_cell[dim];
              RangeNumberType             ky_shape_grad_in_unit_cell[dim];
              CUDAVector<RangeNumberType> kx_shape_grad_in_unit_cell_vector(
                kx_shape_grad_in_unit_cell, dim);
              CUDAVector<RangeNumberType> ky_shape_grad_in_unit_cell_vector(
                ky_shape_grad_in_unit_cell, dim);

              for (unsigned int i = 0; i < dim; i++)
                {
                  kx_shape_grad_in_unit_cell[i] =
                    bem_values.kx_shape_grad_matrix_table_for_regular(
                      0, quad_no, i, kx_dof_index);
                  ky_shape_grad_in_unit_cell[i] =
                    bem_values.kx_shape_grad_matrix_table_for_regular(
                      0, quad_no, i, ky_dof_index);
                }

              /**
               * Apply covariant transformation to the gradient tensors in
               * the unit cell.
               */
              RangeNumberType             kx_shape_grad_in_real_cell[spacedim];
              RangeNumberType             ky_shape_grad_in_real_cell[spacedim];
              CUDAVector<RangeNumberType> kx_shape_grad_in_real_cell_vector(
                kx_shape_grad_in_real_cell, spacedim);
              CUDAVector<RangeNumberType> ky_shape_grad_in_real_cell_vector(
                ky_shape_grad_in_real_cell, spacedim);

              CUDAFullMatrix<RangeNumberType> kx_covariant_matrix(
                const_cast<RangeNumberType *>(
                  &(scratch_data.kx_covariants_regular(0, quad_no, 0, 0))),
                spacedim,
                dim);
              CUDAFullMatrix<RangeNumberType> ky_covariant_matrix(
                const_cast<RangeNumberType *>(
                  &(scratch_data.ky_covariants_regular(0, quad_no, 0, 0))),
                spacedim,
                dim);

              kx_covariant_matrix.vmult(kx_shape_grad_in_real_cell_vector,
                                        kx_shape_grad_in_unit_cell_vector);
              ky_covariant_matrix.vmult(ky_shape_grad_in_real_cell_vector,
                                        ky_shape_grad_in_unit_cell_vector);

              /**
               * Calculate the surface gradient tensor of the shape
               * functions, which is the cross product of normal vector and
               * the volume gradient vector.
               *
               * \mynote{The cross product operation requires the input
               * vectors be transformed to tensors.}
               */
              Tensor<1, spacedim, RangeNumberType> kx_shape_surface_curl =
                cross_product_3d(scratch_data.kx_normals_regular(0, quad_no),
                                 kx_shape_grad_in_real_cell_vector);
              Tensor<1, spacedim, RangeNumberType> ky_shape_surface_curl =
                cross_product_3d(scratch_data.ky_normals_regular(0, quad_no),
                                 ky_shape_grad_in_real_cell_vector);

              quad_values_in_thread_block[tid] =
                kernel_function.value(
                  scratch_data.kx_quad_points_regular(0, quad_no),
                  scratch_data.ky_quad_points_regular(0, quad_no),
                  scratch_data.kx_normals_regular(0, quad_no),
                  scratch_data.ky_normals_regular(0, quad_no),
                  component) *
                scratch_data.kx_jacobians_regular(0, quad_no) *
                scratch_data.ky_jacobians_regular(0, quad_no) *
                scalar_product(kx_shape_surface_curl, ky_shape_surface_curl) *
                bem_values.quad_rule_for_regular.get_weights()[quad_no];
            }
          else
            {
              /**
               * @internal Evaluate the kernel function at the specified
               * pair of points in the real cells with their normal vectors,
               * the result of which is then multiplied by the Jacobians
               * from unit cell to real cell, shape function values and
               * quadrature weights.
               */
              quad_values_in_thread_block[tid] =
                kernel_function.value(
                  scratch_data.kx_quad_points_regular(0, quad_no),
                  scratch_data.ky_quad_points_regular(0, quad_no),
                  scratch_data.kx_normals_regular(0, quad_no),
                  scratch_data.ky_normals_regular(0, quad_no),
                  component) *
                scratch_data.kx_jacobians_regular(0, quad_no) *
                scratch_data.ky_jacobians_regular(0, quad_no) *
                bem_values.kx_shape_value_table_for_regular(kx_dof_index,
                                                            0,
                                                            quad_no) *
                bem_values.ky_shape_value_table_for_regular(ky_dof_index,
                                                            0,
                                                            quad_no) *
                bem_values.quad_rule_for_regular.get_weights()[quad_no];
            }
        }
      else
        {
          quad_values_in_thread_block[tid] = 0.;
        }

      /**
       * @internal Synchronize all the threads in the current thread block for
       * finishing their function evaluation at the specified quadrature point.
       */
      __syncthreads();

      /**
       * @internal Sum up the values saved in the shared memory.
       */
      //! Accumulate the data in a thread block. N.B. Here s is the stride for
      //! sum and 2*s is the stride for classification of the array into
      //! segments.
      for (unsigned int s = blockDim.x / 2; s >= warpSize; s >>= 1)
        {
          if (tid < s)
            {
              quad_values_in_thread_block[tid] +=
                quad_values_in_thread_block[tid + s];
            }

          __syncthreads();
        }

      if (tid < warpSize)
        {
          warpReduce(quad_values_in_thread_block, tid);
        }

      //! Write out the result (scaled by the factor) for the current block.
      if (tid == 0)
        {
          scratch_data.quad_values_in_thread_blocks[blockIdx.x] =
            quad_values_in_thread_block[0] * factor;
        }

      __syncthreads();
    }
  } // namespace CUDAWrappers


  /**
   * Precalculate surface Jacobians and normal vectors to be used in the Sauter
   * quadrature.
   *
   * \alert{Computation of the Jacobian matrix as well as related quantities
   * such as normal vector, covariant transformation matrix, metric tensor,
   * etc., is related to the mapping object and has nothing to do with the
   * finite element. A mapping object is used to describe geometry, while a
   * finite element object is used to describe the ansatz or test functions.}
   *
   * \mynote{This version involves @p PairCellWiseScratchData.}
   *
   * @param scratch
   * @param data
   * @param cell_neighboring_type
   * @param active_quad_rule
   */
  template <int dim, int spacedim, typename RangeNumberType = double>
  __host__ void
  calc_jacobian_normals_for_sauter_quad(
    const CellNeighboringType                        cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch,
    const QGauss<dim * 2>                                   &active_quad_rule)
  {
    switch (cell_neighboring_type)
      {
          case SamePanel: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<dim>::vertices_per_cell,
                   ExcInternalError());

            /**
             * Precalculate surface Jacobians and normal vectors at each
             * quadrature point in the current pair of cells.
             * \mynote{They are stored in the @p scratch data.}
             */
            for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_jacobians_same_panel(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.kx_mapping_support_points_permuted,
                        scratch.kx_normals_same_panel(k3_index, q));

                    scratch.ky_jacobians_same_panel(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.ky_mapping_support_points_permuted,
                        scratch.ky_normals_same_panel(k3_index, q));

                    scratch.kx_quad_points_same_panel(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.kx_mapping_shape_value_table_for_same_panel,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_quad_points_same_panel(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.ky_mapping_shape_value_table_for_same_panel,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonEdge: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<2>::vertices_per_face,
                   ExcInternalError());

            // Precalculate surface Jacobians and normal vectors.
            for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_jacobians_common_edge(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.kx_mapping_support_points_permuted,
                        scratch.kx_normals_common_edge(k3_index, q));

                    scratch.ky_jacobians_common_edge(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.ky_mapping_support_points_permuted,
                        scratch.ky_normals_common_edge(k3_index, q));

                    scratch.kx_quad_points_common_edge(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.kx_mapping_shape_value_table_for_common_edge,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_quad_points_common_edge(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values.ky_mapping_shape_value_table_for_common_edge,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonVertex: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 1,
                   ExcInternalError());

            // Precalculate surface Jacobians and normal vectors.
            for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_jacobians_common_vertex(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.kx_mapping_support_points_permuted,
                        scratch.kx_normals_common_vertex(k3_index, q));

                    scratch.ky_jacobians_common_vertex(k3_index, q) =
                      surface_jacobian_det_and_normal_vector(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.ky_mapping_support_points_permuted,
                        scratch.ky_normals_common_vertex(k3_index, q));

                    scratch.kx_quad_points_common_vertex(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_value_table_for_common_vertex,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_quad_points_common_vertex(k3_index, q) =
                      transform_unit_to_permuted_real_cell(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_value_table_for_common_vertex,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case Regular: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 0,
                   ExcInternalError());

            // Precalculate surface Jacobians and normal vectors.
            for (unsigned int q = 0; q < active_quad_rule.size(); q++)
              {
                scratch.kx_jacobians_regular(0, q) =
                  surface_jacobian_det_and_normal_vector(
                    0,
                    q,
                    bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
                    scratch.kx_mapping_support_points_permuted,
                    scratch.kx_normals_regular(0, q));

                scratch.ky_jacobians_regular(0, q) =
                  surface_jacobian_det_and_normal_vector(
                    0,
                    q,
                    bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
                    scratch.ky_mapping_support_points_permuted,
                    scratch.ky_normals_regular(0, q));

                scratch.kx_quad_points_regular(0, q) =
                  transform_unit_to_permuted_real_cell(
                    0,
                    q,
                    bem_values.kx_mapping_shape_value_table_for_regular,
                    scratch.kx_mapping_support_points_permuted);

                scratch.ky_quad_points_regular(0, q) =
                  transform_unit_to_permuted_real_cell(
                    0,
                    q,
                    bem_values.ky_mapping_shape_value_table_for_regular,
                    scratch.ky_mapping_support_points_permuted);
              }

            break;
          }
          default: {
            Assert(false, ExcNotImplemented());
          }
      }
  }


  /**
   *
   *
   * @param scratch
   * @param cell_neighboring_type
   * @param bem_values
   * @param active_quad_rule
   */
  template <int dim, int spacedim, typename RangeNumberType = double>
  __host__ void
  calc_covariant_transformations(
    const CellNeighboringType                        cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch,
    const QGauss<dim * 2>                                   &active_quad_rule)
  {
    switch (cell_neighboring_type)
      {
          case SamePanel: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<dim>::vertices_per_cell,
                   ExcInternalError());

            for (unsigned int k3_index = 0; k3_index < 8; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_covariants_same_panel(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_covariants_same_panel(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_same_panel,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonEdge: {
            Assert(scratch.common_vertex_pair_local_indices.size() ==
                     GeometryInfo<2>::vertices_per_face,
                   ExcInternalError());

            for (unsigned int k3_index = 0; k3_index < 6; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_covariants_common_edge(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_covariants_common_edge(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_edge,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case CommonVertex: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 1,
                   ExcInternalError());

            for (unsigned int k3_index = 0; k3_index < 4; k3_index++)
              {
                for (unsigned int q = 0; q < active_quad_rule.size(); q++)
                  {
                    scratch.kx_covariants_common_vertex(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .kx_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.kx_mapping_support_points_permuted);

                    scratch.ky_covariants_common_vertex(k3_index, q) =
                      surface_covariant_transformation(
                        k3_index,
                        q,
                        bem_values
                          .ky_mapping_shape_grad_matrix_table_for_common_vertex,
                        scratch.ky_mapping_support_points_permuted);
                  }
              }

            break;
          }
          case Regular: {
            Assert(scratch.common_vertex_pair_local_indices.size() == 0,
                   ExcInternalError());

            for (unsigned int q = 0; q < active_quad_rule.size(); q++)
              {
                scratch.kx_covariants_regular(0, q) =
                  surface_covariant_transformation(
                    0,
                    q,
                    bem_values.kx_mapping_shape_grad_matrix_table_for_regular,
                    scratch.kx_mapping_support_points_permuted);

                scratch.ky_covariants_regular(0, q) =
                  surface_covariant_transformation(
                    0,
                    q,
                    bem_values.ky_mapping_shape_grad_matrix_table_for_regular,
                    scratch.ky_mapping_support_points_permuted);
              }

            break;
          }
          default: {
            Assert(false, ExcNotImplemented());
          }
      }
  }


  /**
   * Apply the Sauter's quadrature rule to the kernel function pulled back to
   * the Sauter's parametric space. The result will also be multiplied by a
   * factor. This version uses the precalculated @p BEMValues.
   *
   * @param quad_rule
   * @param f
   * @param factor
   * @param component
   * @return
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  __host__ RangeNumberType
  ApplyQuadratureUsingBEMValues(
    const Quadrature<dim * 2> &quad_rule,
    const IdeoBEM::CUDAWrappers::KernelPulledbackToSauterSpace<
      dim,
      spacedim,
      KernelFunctionType,
      RangeNumberType>                                            &f,
    const RangeNumberType                                          factor,
    const BEMValues<dim, spacedim, RangeNumberType>               &bem_values,
    const PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    unsigned int component = 0)
  {
    RangeNumberType result = 0.;

    const std::vector<double> &quad_weights = quad_rule.get_weights();

    for (unsigned int q = 0; q < quad_rule.size(); q++)
      {
        // Evaluate the integrand with precalculated shape values and shape
        // gradient matrices.
        result +=
          f.value(q, bem_values, scratch_data, component) * quad_weights[q];
      }

    return result * factor;
  }


  template <int dim, int spacedim, typename RangeNumberType = double>
  const QGauss<dim * 2> &
  select_sauter_quad_rule_from_bem_values(
    const CellNeighboringType                        cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType> &bem_values)
  {
    switch (cell_neighboring_type)
      {
          case SamePanel: {
            return bem_values.quad_rule_for_same_panel;
          }
          case CommonEdge: {
            return bem_values.quad_rule_for_common_edge;
          }
          case CommonVertex: {
            return bem_values.quad_rule_for_common_vertex;
          }
          case Regular: {
            return bem_values.quad_rule_for_regular;
          }
          default: {
            Assert(false, ExcNotImplemented());

            return bem_values.quad_rule_for_same_panel;
          }
      }
  }


  /**
   * Perform the Galerkin-BEM double integral with respect to a boundary
   * integral operator (represented as the input kernel function) using
   * Sauter's quadrature for the DoFs in a pair of cells \f$K_x\f$ and
   * \f$K_y\f$.
   *
   * \mynote{When the boundary integral operator is the hyper singular operator,
   * the regularized bilinear form in \f$\mathbb{R}^3\f$ is
   * \f[
   * \left\langle Du,v \right\rangle_{\Gamma} =
   * \frac{1}{4\pi}\int_{\Gamma}\int_{\Gamma}
   * \frac{\underline{\curl}_{\Gamma}u(y)\cdot\underline{\curl}_{\Gamma}v(x)}{\abs{x-y}}
   * ds_x ds_y.
   * \f]
   * It needs special treatment, i.e. calculation of the surface curl of the
   * basis functions for ansatz and test functions.}
   *
   * \mynote{This is only applicable to the case when a full matrix for a
   * boundary integral operator is to be constructed. Therefore, this function
   * is only meaningful for algorithm verification. In real application, an
   * \hmatrix should be built. Also note that even for the near field matrix
   * node in an \hmatrix, which must be a full matrix, the Sauter's quadrature
   * is built in the paradigm of "on a pair of DoFs" instead of "on a pair of
   * cells". This is because the two cluster trees associated with an \hmatrix
   * use partition by DoF support points in stead of partition by cells.}
   *
   * @param kernel
   * @param factor
   * @param kx_cell_iter
   * @param ky_cell_iter
   * @param kx_mapping
   * @param ky_mapping
   * @param map_from_kx_mesh_to_volume_mesh
   * @param map_from_ky_mesh_to_volume_mesh
   * @param method_for_cell_neighboring_type
   * @param bem_values
   * @param scratch
   * @param data
   * @param is_scratch_data_for_kx_uncalculated
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  void
  sauter_assemble_on_one_pair_of_cells(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const typename DoFHandler<dim, spacedim>::active_cell_iterator
      &kx_cell_iter,
    const typename DoFHandler<dim, spacedim>::active_cell_iterator
                                            &ky_cell_iter,
    const MappingQGenericExt<dim, spacedim> &kx_mapping,
    const MappingQGenericExt<dim, spacedim> &ky_mapping,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_kx_boundary_mesh_to_volume_mesh,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_ky_boundary_mesh_to_volume_mesh,
    const BEMTools::DetectCellNeighboringTypeMethod
      method_for_cell_neighboring_type,
    const BEMValues<dim, spacedim, RangeNumberType>         &bem_values,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data,
    const bool is_scratch_data_for_kx_uncalculated = true)
  {
    /**
     * Detect the cell neighboring type based on cell vertex indices.
     */
    CellNeighboringType cell_neighboring_type =
      detect_cell_neighboring_type<dim, spacedim>(
        method_for_cell_neighboring_type,
        kx_cell_iter,
        ky_cell_iter,
        map_from_kx_boundary_mesh_to_volume_mesh,
        map_from_ky_boundary_mesh_to_volume_mesh,
        scratch_data.common_vertex_pair_local_indices);

    /**
     * Create a quadrature rule, which depends on the cell neighboring type.
     */
    const QGauss<dim * 2> active_quad_rule =
      select_sauter_quad_rule_from_bem_values(cell_neighboring_type,
                                              bem_values);

    const FiniteElement<dim, spacedim> &kx_fe = kx_cell_iter->get_fe();
    const FiniteElement<dim, spacedim> &ky_fe = ky_cell_iter->get_fe();

    const unsigned int kx_n_dofs = kx_fe.dofs_per_cell;
    const unsigned int ky_n_dofs = ky_fe.dofs_per_cell;

    /**
     * Calculate the real support points in the cell \f$K_x\f$ as well as
     * \f$K_y\f$ via the mapping object. Since such data will be held and
     * updated in-situ in the mapping object, which has been passed into this
     * working function by const reference, a copy of it should be made.
     */
    MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
    if (is_scratch_data_for_kx_uncalculated)
      {
        kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
      }
    MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
    ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);

    /**
     * Copy the newly calculated support points into @p ScratchData.
     */
    if (is_scratch_data_for_kx_uncalculated)
      {
        scratch_data.kx_mapping_support_points_in_default_order =
          kx_mapping_copy.get_support_points();
      }
    scratch_data.ky_mapping_support_points_in_default_order =
      ky_mapping_copy.get_support_points();

    permute_dofs_and_mapping_support_points_for_sauter_quad(
      scratch_data,
      copy_data,
      cell_neighboring_type,
      kx_cell_iter,
      ky_cell_iter,
      kx_mapping_copy,
      ky_mapping_copy,
      is_scratch_data_for_kx_uncalculated);

    calc_jacobian_normals_for_sauter_quad(cell_neighboring_type,
                                          bem_values,
                                          scratch_data,
                                          active_quad_rule);

    /**
     * When the bilinear form for the hyper singular operator is evaluated, the
     * covariant transformation is required.
     */
    if (kernel.kernel_type == HyperSingularRegular)
      {
        calc_covariant_transformations(cell_neighboring_type,
                                       bem_values,
                                       scratch_data,
                                       active_quad_rule);
      }

    /**
     *  Clear the local matrix in case that it is reused from another
     *  finished task. N.B. Its memory has already been allocated in the
     *  constructor of @p CellPairWisePerTaskData.
     */
    copy_data.local_pair_cell_matrix.reinit(
      copy_data.kx_local_dof_indices_permuted.size(),
      copy_data.ky_local_dof_indices_permuted.size());

    // Iterate over DoFs for test function space in \f$K_x\f$.
    for (unsigned int i = 0; i < kx_n_dofs; i++)
      {
        // Iterate over DoFs for trial function space in \f$K_y\f$.
        for (unsigned int j = 0; j < ky_n_dofs; j++)
          {
            // Pullback the kernel function to unit cell.
            IdeoBEM::CUDAWrappers::KernelPulledbackToUnitCell<
              dim,
              spacedim,
              KernelFunctionType,
              RangeNumberType>
              kernel_pullback_on_unit(kernel, cell_neighboring_type, i, j);

            // Pullback the kernel function to Sauter parameter space.
            IdeoBEM::CUDAWrappers::KernelPulledbackToSauterSpace<
              dim,
              spacedim,
              KernelFunctionType,
              RangeNumberType>
              kernel_pullback_on_sauter(kernel_pullback_on_unit,
                                        cell_neighboring_type);

            // Apply Sauter numerical quadrature.
            copy_data.local_pair_cell_matrix(i, j) =
              ApplyQuadratureUsingBEMValues(active_quad_rule,
                                            kernel_pullback_on_sauter,
                                            kernel_factor,
                                            bem_values,
                                            scratch_data);
          }
      }
  }


  /**
   * Perform Galerkin-BEM double integral with respect to a given kernel on a
   * pair of DoFs \f$(i, j)\f$ using the Sauter quadrature.
   *
   * Assume \f$\mathcal{K}_i\f$ is the collection of cells sharing the DoF
   * support point \f$i\f$ and \f$\mathcal{K}_j\f$ is the collection of cells
   * sharing the DoF support point \f$j\f$. Then Galerkin-BEM double integral
   * will be over each cell pair which is comprised of an arbitrary cell in
   * \f$\mathcal{K}_i\f$ and an arbitrary cell in \f$\mathcal{K}_j\f$.
   *
   * \mynote{The DoF indices \f$(i, j)\f$ are global, i.e. global in the sense
   * of all DoFs contained in the associated DoF handlers. In mixed boundary
   * value problem, when dealing with the Dirichlet function space, only a
   * subset of these DoFs are selected. Hence, the DoF indices in an \hmat are
   * local, i.e. local in the sense of DoF indices renumbered for the subset.
   * When coming to this function for Sauter quadrature, the global DoF indices
   * should be used.}
   *
   * @param kernel
   * @param factor
   * @param i External DoF numbering
   * @param j External DoF numbering
   * @param kx_dof_to_cell_topo
   * @param ky_dof_to_cell_topo
   * @param bem_values
   * @param kx_dof_handler
   * @param ky_dof_handler
   * @param kx_mapping
   * @param ky_mapping
   * @param map_from_kx_boundary_mesh_to_volume_mesh
   * @param map_from_ky_boundary_mesh_to_volume_mesh
   * @param method_for_cell_neighboring_type
   * @param scratch_data
   * @param copy_data
   * @return
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  RangeNumberType
  sauter_assemble_on_one_pair_of_dofs(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const types::global_dof_index                        i,
    const types::global_dof_index                        j,
    const std::vector<std::vector<unsigned int>>        &kx_dof_to_cell_topo,
    const std::vector<std::vector<unsigned int>>        &ky_dof_to_cell_topo,
    const BEMValues<dim, spacedim, RangeNumberType>     &bem_values,
    const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                            &bem_values_gpu,
    const DoFHandler<dim, spacedim>         &kx_dof_handler,
    const DoFHandler<dim, spacedim>         &ky_dof_handler,
    const MappingQGenericExt<dim, spacedim> &kx_mapping,
    const MappingQGenericExt<dim, spacedim> &ky_mapping,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_kx_boundary_mesh_to_volume_mesh,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_ky_boundary_mesh_to_volume_mesh,
    const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    IdeoBEM::CUDAWrappers::
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                                                            &scratch_data_gpu,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data,
    IdeoBEM::CUDAWrappers::CUDAPairCellWisePerTaskData      &copy_data_gpu)
  {
    /**
     * @internal This result variable accumulates all contributions from pairs
     * of cells related to the global DoF indices @p (i,j).
     */
    RangeNumberType double_integral = 0.0;

    /**
     * Iterate over each cell in the support of the basis function associated
     * with the i-th DoF.
     */
    for (unsigned int kx_cell_index : kx_dof_to_cell_topo[i])
      {
        typename DoFHandler<dim, spacedim>::active_cell_iterator kx_cell_iter =
          kx_dof_handler.begin_active();
        std::advance(kx_cell_iter, kx_cell_index);

        /**
         * Calculate the real support points in the cell \f$K_x\f$ via the
         * mapping object. Since such data will be held and updated in-situ in
         * the mapping object, which has been passed into this working function
         * by const reference, a copy of it should be made.
         */
        MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
        kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
        /**
         * Copy the newly calculated support points into @p ScratchData.
         */
        scratch_data.kx_mapping_support_points_in_default_order =
          kx_mapping_copy.get_support_points();
        /**
         * Update the DoF indices.
         */
        kx_cell_iter->get_dof_indices(
          scratch_data.kx_local_dof_indices_in_default_dof_order);

        /**
         * Iterate over each cell in the support of the basis function
         * associated with the j-th DoF.
         */
        for (unsigned int ky_cell_index : ky_dof_to_cell_topo[j])
          {
            typename DoFHandler<dim, spacedim>::active_cell_iterator
              ky_cell_iter = ky_dof_handler.begin_active();
            std::advance(ky_cell_iter, ky_cell_index);

            /**
             * Detect the cell neighboring type based on cell vertex indices.
             */
            CellNeighboringType cell_neighboring_type =
              detect_cell_neighboring_type<dim, spacedim>(
                method_for_cell_neighboring_type,
                kx_cell_iter,
                ky_cell_iter,
                map_from_kx_boundary_mesh_to_volume_mesh,
                map_from_ky_boundary_mesh_to_volume_mesh,
                scratch_data.common_vertex_pair_local_indices);

            /**
             * Calculate the real support points in the cell \f$K_y\f$ via the
             * mapping object. Since such data will be held and updated in-situ
             * in the mapping object, which has been passed into this working
             * function by const reference, a copy of it should be made.
             */
            MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
            ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
            /**
             * Copy the newly calculated support points into @p ScratchData.
             */
            scratch_data.ky_mapping_support_points_in_default_order =
              ky_mapping_copy.get_support_points();

            /**
             * \mynote{1. Inside this function, whether DoF indices in \f$K_x\f$
             * will be extracted depends on the flag
             * @p is_scratch_data_for_kx_uncalculated. The DoF indices in
             * \f$K_y\f$ will always be extracted.
             * 2. DoF indices and mapping support points will be permuted for
             * both \f$K_x\f$ and \f$K_y\f$.}
             */
            permute_dofs_and_mapping_support_points_for_sauter_quad(
              scratch_data,
              copy_data,
              cell_neighboring_type,
              kx_cell_iter,
              ky_cell_iter,
              kx_mapping_copy,
              ky_mapping_copy,
              false);

            /**
             * @internal Copy the scratch data and copy data from CPU to GPU.
             */
            scratch_data_gpu.assign_from_host(scratch_data);
            copy_data_gpu.assign_from_host(copy_data,
                                           scratch_data.cuda_stream_handle);

            cudaError_t error_code =
              cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            /**
             * @internal Determine thread block dimensions.
             */
            dim3 blocks_rect;
            dim3 threads_rect;
            IdeoBEM::CUDAWrappers::configure_thread_blocks(
              cell_neighboring_type, bem_values, blocks_rect, threads_rect);

            switch (cell_neighboring_type)
              {
                  case CellNeighboringType::SamePanel: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_same_panel<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonEdge: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_edge<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonVertex: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_vertex<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::Regular: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_regular<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());

                    break;
                  }
              }

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            /**
             * When the bilinear form for the hyper singular operator is
             * evaluated, the covariant transformation is required.
             */
            if (kernel.kernel_type == HyperSingularRegular)
              {
                switch (cell_neighboring_type)
                  {
                      case CellNeighboringType::SamePanel: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_same_panel<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonEdge: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_edge<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonVertex: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_vertex<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::Regular: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_regular<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      default: {
                        Assert(false, ExcInternalError());

                        break;
                      }
                  }

                error_code =
                  cudaStreamSynchronize(scratch_data.cuda_stream_handle);
                AssertCuda(error_code);
              }

            /**
             * Find the index of the i-th DoF in the permuted DoF indices of
             * \f$K_x\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              i_iter =
                std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                          copy_data.kx_local_dof_indices_permuted.end(),
                          i);
            Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int i_index =
              i_iter - copy_data.kx_local_dof_indices_permuted.begin();

            /**
             * Find the index of the j-th DoF in the permuted DoF indices of
             * \f$K_y\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              j_iter =
                std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                          copy_data.ky_local_dof_indices_permuted.end(),
                          j);
            Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int j_index =
              j_iter - copy_data.ky_local_dof_indices_permuted.begin();

            // Apply 4d Sauter numerical quadrature on the GPU device.
            switch (cell_neighboring_type)
              {
                  case BEMTools::CellNeighboringType::SamePanel: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesSamePanel<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonEdge: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonEdge<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonVertex: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonVertex<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::Regular: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesRegular<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());
                    break;
                  }
              }

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            /**
             * @internal Extract quadrature results accumulated from all thread
             * blocks.
             */
            error_code =
              cudaMemcpyAsync(scratch_data.quad_values_in_thread_blocks,
                              scratch_data_gpu.quad_values_in_thread_blocks,
                              sizeof(RangeNumberType) * blocks_rect.x,
                              cudaMemcpyDeviceToHost,
                              scratch_data.cuda_stream_handle);
            AssertCuda(error_code);
            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            for (unsigned int t = 0; t < blocks_rect.x; t++)
              {
                double_integral += scratch_data.quad_values_in_thread_blocks[t];
              }
          }
      }

    return double_integral;
  }


  /**
   * Perform Galerkin-BEM double integral with respect to a given kernel on a
   * pair of DoFs \f$(i, j)\f$ using the Sauter quadrature. In addition, entries
   * of the mass matrix are calculated and added into this matrix. Because the
   * evaluation of mass matrix entries involves integrating the product of
   * two shape functions with compact support, there is no long-range
   * interaction between them like the case in BEM. Hence, these mass matrix
   * entries are added into the near field \hmat node.
   *
   * Assume \f$\mathcal{K}_i\f$ is the collection of cells sharing the DoF
   * support point \f$i\f$ and \f$\mathcal{K}_j\f$ is the collection of cells
   * sharing the DoF support point \f$j\f$. Then Galerkin-BEM double integral
   * will be over each cell pair which is comprised of an arbitrary cell in
   * \f$\mathcal{K}_i\f$ and an arbitrary cell in \f$\mathcal{K}_j\f$.
   *
   * \mynote{The DoF indices \f$(i, j)\f$ are global, i.e. global in the sense
   * of all DoFs contained in the associated DoF handlers. In mixed boundary
   * value problem, when dealing with the Dirichlet function space, only a
   * subset of these DoFs are selected. Hence, the DoF indices in an \hmat are
   * local, i.e. local in the sense of DoF indices renumbered for the subset.
   * When coming to this function for Sauter quadrature, the global DoF indices
   * should be used.}
   *
   *
   * @param kernel
   * @param kernel_factor
   * @param mass_matrix_factor
   * @param i
   * @param j
   * @param kx_dof_to_cell_topo
   * @param ky_dof_to_cell_topo
   * @param bem_values
   * @param kx_dof_handler
   * @param ky_dof_handler
   * @param kx_mapping
   * @param ky_mapping
   * @param map_from_kx_boundary_mesh_to_volume_mesh
   * @param map_from_ky_boundary_mesh_to_volume_mesh
   * @param method_for_cell_neighboring_type
   * @param mass_matrix_scratch_data
   * @param scratch_data
   * @param copy_data
   * @return
   */
  template <int dim,
            int spacedim,
            template <int, typename>
            typename KernelFunctionType,
            typename RangeNumberType = double>
  RangeNumberType
  sauter_assemble_on_one_pair_of_dofs(
    const KernelFunctionType<spacedim, RangeNumberType> &kernel,
    const RangeNumberType                                kernel_factor,
    const RangeNumberType                                mass_matrix_factor,
    const types::global_dof_index                        i,
    const types::global_dof_index                        j,
    const std::vector<std::vector<unsigned int>>        &kx_dof_to_cell_topo,
    const std::vector<std::vector<unsigned int>>        &ky_dof_to_cell_topo,
    const BEMValues<dim, spacedim, RangeNumberType>     &bem_values,
    const IdeoBEM::CUDAWrappers::CUDABEMValues<dim, spacedim, RangeNumberType>
                                            &bem_values_gpu,
    const DoFHandler<dim, spacedim>         &kx_dof_handler,
    const DoFHandler<dim, spacedim>         &ky_dof_handler,
    const MappingQGenericExt<dim, spacedim> &kx_mapping,
    const MappingQGenericExt<dim, spacedim> &ky_mapping,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_kx_boundary_mesh_to_volume_mesh,
    const std::map<typename Triangulation<dim, spacedim>::cell_iterator,
                   typename Triangulation<dim + 1, spacedim>::face_iterator>
      &map_from_ky_boundary_mesh_to_volume_mesh,
    const DetectCellNeighboringTypeMethod method_for_cell_neighboring_type,
    CellWiseScratchDataForMassMatrix<dim, spacedim> &mass_matrix_scratch_data,
    PairCellWiseScratchData<dim, spacedim, RangeNumberType> &scratch_data,
    IdeoBEM::CUDAWrappers::
      CUDAPairCellWiseScratchData<dim, spacedim, RangeNumberType>
                                                            &scratch_data_gpu,
    PairCellWisePerTaskData<dim, spacedim, RangeNumberType> &copy_data,
    IdeoBEM::CUDAWrappers::CUDAPairCellWisePerTaskData      &copy_data_gpu)
  {
    /**
     * @internal This result variable accumulates all contributions from pairs
     * of cells related to the global DoF indices @p (i,j).
     */
    RangeNumberType double_integral = 0.0;

    /**
     * Iterate over each cell in the support of the basis function associated
     * with the i-th DoF.
     */
    for (unsigned int kx_cell_index : kx_dof_to_cell_topo[i])
      {
        typename DoFHandler<dim, spacedim>::active_cell_iterator kx_cell_iter =
          kx_dof_handler.begin_active();
        std::advance(kx_cell_iter, kx_cell_index);

        /**
         * Calculate the real support points in the cell \f$K_x\f$ via the
         * mapping object. Since such data will be held and updated in-situ in
         * the mapping object, which has been passed into this working function
         * by const reference, a copy of it should be made.
         */
        MappingQGenericExt<dim, spacedim> kx_mapping_copy(kx_mapping);
        kx_mapping_copy.compute_mapping_support_points(kx_cell_iter);
        /**
         * Copy the newly calculated support points into @p ScratchData.
         */
        scratch_data.kx_mapping_support_points_in_default_order =
          kx_mapping_copy.get_support_points();
        /**
         * Update the DoF indices.
         */
        kx_cell_iter->get_dof_indices(
          scratch_data.kx_local_dof_indices_in_default_dof_order);

        /**
         * Iterate over each cell in the support of the basis function
         * associated with the j-th DoF.
         */
        for (unsigned int ky_cell_index : ky_dof_to_cell_topo[j])
          {
            typename DoFHandler<dim, spacedim>::active_cell_iterator
              ky_cell_iter = ky_dof_handler.begin_active();
            std::advance(ky_cell_iter, ky_cell_index);

            /**
             * Detect the cell neighboring type based on cell vertex indices.
             */
            CellNeighboringType cell_neighboring_type =
              detect_cell_neighboring_type<dim, spacedim>(
                method_for_cell_neighboring_type,
                kx_cell_iter,
                ky_cell_iter,
                map_from_kx_boundary_mesh_to_volume_mesh,
                map_from_ky_boundary_mesh_to_volume_mesh,
                scratch_data.common_vertex_pair_local_indices);

            /**
             * Calculate the real support points in the cell \f$K_y\f$ via the
             * mapping object. Since such data will be held and updated in-situ
             * in the mapping object, which has been passed into this working
             * function by const reference, a copy of it should be made.
             */
            MappingQGenericExt<dim, spacedim> ky_mapping_copy(ky_mapping);
            ky_mapping_copy.compute_mapping_support_points(ky_cell_iter);
            /**
             * Copy the newly calculated support points into @p ScratchData.
             */
            scratch_data.ky_mapping_support_points_in_default_order =
              ky_mapping_copy.get_support_points();

            /**
             * \mynote{Inside this function, whether DoF indices in \f$K_x\f$
             * will be extracted depends on the flag
             * @p is_scratch_data_for_kx_uncalculated. The DoF indices in
             * \f$K_y\f$ will always be extracted.}
             */
            permute_dofs_and_mapping_support_points_for_sauter_quad(
              scratch_data,
              copy_data,
              cell_neighboring_type,
              kx_cell_iter,
              ky_cell_iter,
              kx_mapping_copy,
              ky_mapping_copy,
              false);

            // Copy data from CPU to GPU.
            scratch_data_gpu.assign_from_host(scratch_data);
            copy_data_gpu.assign_from_host(copy_data,
                                           scratch_data.cuda_stream_handle);

            cudaError_t error_code =
              cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            // Determine thread block dimensions.
            dim3 blocks_rect;
            dim3 threads_rect;
            IdeoBEM::CUDAWrappers::configure_thread_blocks(
              cell_neighboring_type, bem_values, blocks_rect, threads_rect);

            switch (cell_neighboring_type)
              {
                  case CellNeighboringType::SamePanel: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_same_panel<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonEdge: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_edge<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::CommonVertex: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_common_vertex<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case CellNeighboringType::Regular: {
                    IdeoBEM::CUDAWrappers::
                      calc_jacobian_normals_for_sauter_quad_regular<<<
                        blocks_rect,
                        threads_rect,
                        0,
                        scratch_data.cuda_stream_handle>>>(bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());

                    break;
                  }
              }

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            /**
             * When the bilinear form for the hyper singular operator is
             * evaluated, the covariant transformation is required.
             */
            if (kernel.kernel_type == HyperSingularRegular)
              {
                switch (cell_neighboring_type)
                  {
                      case CellNeighboringType::SamePanel: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_same_panel<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonEdge: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_edge<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::CommonVertex: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_common_vertex<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      case CellNeighboringType::Regular: {
                        IdeoBEM::CUDAWrappers::
                          calc_covariant_transformations_regular<<<
                            blocks_rect,
                            threads_rect,
                            0,
                            scratch_data.cuda_stream_handle>>>(
                            bem_values_gpu, scratch_data_gpu);

                        break;
                      }
                      default: {
                        Assert(false, ExcInternalError());

                        break;
                      }
                  }

                error_code =
                  cudaStreamSynchronize(scratch_data.cuda_stream_handle);
                AssertCuda(error_code);
              }

            /**
             * Find the index of the i-th DoF in the permuted DoF indices of
             * \f$K_x\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              i_iter =
                std::find(copy_data.kx_local_dof_indices_permuted.begin(),
                          copy_data.kx_local_dof_indices_permuted.end(),
                          i);
            Assert(i_iter != copy_data.kx_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int i_index =
              i_iter - copy_data.kx_local_dof_indices_permuted.begin();

            /**
             * Find the index of the j-th DoF in the permuted DoF indices of
             * \f$K_y\f$.
             */
            typename std::vector<types::global_dof_index>::const_iterator
              j_iter =
                std::find(copy_data.ky_local_dof_indices_permuted.begin(),
                          copy_data.ky_local_dof_indices_permuted.end(),
                          j);
            Assert(j_iter != copy_data.ky_local_dof_indices_permuted.end(),
                   ExcInternalError());
            unsigned int j_index =
              j_iter - copy_data.ky_local_dof_indices_permuted.begin();

            // Apply 4d Sauter numerical quadrature on the GPU device.
            switch (cell_neighboring_type)
              {
                  case BEMTools::CellNeighboringType::SamePanel: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesSamePanel<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonEdge: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonEdge<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::CommonVertex: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesCommonVertex<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  case BEMTools::CellNeighboringType::Regular: {
                    IdeoBEM::CUDAWrappers::
                      ApplyQuadratureUsingBEMValuesRegular<<<
                        blocks_rect.x,
                        threads_rect.x,
                        threads_rect.x * sizeof(RangeNumberType),
                        scratch_data.cuda_stream_handle>>>(kernel,
                                                           kernel_factor,
                                                           i_index,
                                                           j_index,
                                                           bem_values_gpu,
                                                           scratch_data_gpu);

                    break;
                  }
                  default: {
                    Assert(false, ExcInternalError());
                    break;
                  }
              }

            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            /**
             * @internal Extract quadrature results accumulated from all thread
             * blocks.
             */
            error_code =
              cudaMemcpyAsync(scratch_data.quad_values_in_thread_blocks,
                              scratch_data_gpu.quad_values_in_thread_blocks,
                              sizeof(RangeNumberType) * blocks_rect.x,
                              cudaMemcpyDeviceToHost,
                              scratch_data.cuda_stream_handle);
            AssertCuda(error_code);
            error_code = cudaStreamSynchronize(scratch_data.cuda_stream_handle);
            AssertCuda(error_code);

            for (unsigned int t = 0; t < blocks_rect.x; t++)
              {
                double_integral += scratch_data.quad_values_in_thread_blocks[t];
              }

            /**
             * Append the FEM mass matrix contribution.
             *
             * \myalert{N.B. The DoF handlers for the test and ansatz spaces
             * should be constructed on a same triangulation. Then the following
             * comparison @p kx_cell_index == ky_cell_index is meaningful.}
             */
            if ((kx_cell_index == ky_cell_index) && (mass_matrix_factor != 0))
              {
                Assert(cell_neighboring_type == CellNeighboringType::SamePanel,
                       ExcInternalError());

                // Update the finite element values for the test space.
                mass_matrix_scratch_data.fe_values_for_test_space.reinit(
                  kx_cell_iter);

                /**
                 * Update the finite element values for the trial space.
                 *
                 * \mynote{N.B. The @p FEValues related to the trial
                 * space must also be updated, since the trial space may
                 * be different from the test space.}
                 */
                mass_matrix_scratch_data.fe_values_for_trial_space.reinit(
                  ky_cell_iter);

                const unsigned int n_q_points =
                  mass_matrix_scratch_data.fe_values_for_test_space
                    .get_quadrature()
                    .size();
                // The trial space is on a same triangulation as the test space
                // and they share a same quadrature object.
                AssertDimension(n_q_points,
                                mass_matrix_scratch_data
                                  .fe_values_for_trial_space.get_quadrature()
                                  .size());

                /**
                 * Get the index of the global DoF index \f$i\f$ in
                 * the current cell \f$K_x\f$.
                 *
                 * \mynote{N.B. The local DoF index in \f$K_x\f$ is
                 * searched from the list from DoF indices held in the
                 * @p ScratchData for BEM. This is valid because the
                 * test and trial spaces associated with the mass matrix
                 * and the BEM bilinear form are the same.
                 *
                 * Since there is no support point permutation during FEM mass
                 * matrix assembly, the DoF indices here are in the default
                 * order.}
                 */
                auto i_local_dof_iter = std::find(
                  scratch_data.kx_local_dof_indices_in_default_dof_order
                    .begin(),
                  scratch_data.kx_local_dof_indices_in_default_dof_order.end(),
                  i);
                Assert(i_local_dof_iter !=
                         scratch_data.kx_local_dof_indices_in_default_dof_order
                           .end(),
                       ExcMessage(
                         std::string("Cannot find the global DoF index ") +
                         std::to_string(i) +
                         std::string(" in the list of cell DoF indices!")));
                const unsigned int i_local_dof_index =
                  i_local_dof_iter -
                  scratch_data.kx_local_dof_indices_in_default_dof_order
                    .begin();

                /**
                 * Get the index of the global DoF index \f$j\f$ in
                 * the current cell \f$K_y\f$.
                 */
                auto j_local_dof_iter = std::find(
                  scratch_data.ky_local_dof_indices_in_default_dof_order
                    .begin(),
                  scratch_data.ky_local_dof_indices_in_default_dof_order.end(),
                  j);
                Assert(j_local_dof_iter !=
                         scratch_data.ky_local_dof_indices_in_default_dof_order
                           .end(),
                       ExcMessage(
                         std::string("Cannot find the global DoF index ") +
                         std::to_string(j) +
                         std::string(" in the list of cell DoF indices!")));
                const unsigned int j_local_dof_index =
                  j_local_dof_iter -
                  scratch_data.ky_local_dof_indices_in_default_dof_order
                    .begin();

                for (unsigned int q = 0; q < n_q_points; q++)
                  {
                    double_integral +=
                      mass_matrix_factor *
                      mass_matrix_scratch_data.fe_values_for_test_space
                        .shape_value(i_local_dof_index, q) *
                      mass_matrix_scratch_data.fe_values_for_trial_space
                        .shape_value(j_local_dof_index, q) *
                      mass_matrix_scratch_data.fe_values_for_test_space.JxW(q);
                  }
              }
          }
      }

    return double_integral;
  }
} // namespace IdeoBEM

#endif /* INCLUDE_SAUTER_QUADRATURE_HCU_ */
